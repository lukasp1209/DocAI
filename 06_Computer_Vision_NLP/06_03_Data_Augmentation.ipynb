{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Nur Colab] Diese Zellen m√ºssen nur auf *Google Colab* ausgef√ºhrt werden und installieren Packete und Daten\n",
    "!wget -q https://raw.githubusercontent.com/KI-Campus/AMALEA/master/requirements.txt && pip install --quiet -r requirements.txt\n",
    "!wget --quiet \"https://github.com/KI-Campus/AMALEA/releases/download/data/data.zip\" && unzip -q data.zip\n",
    "!wget --quiet \"https://raw.githubusercontent.com/KI-Campus/AMALEA/master/Woche%205/utils.py\"\n",
    "\n",
    "# üîß Setup: Data Augmentation Libraries\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Standard Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# Computer Vision\n",
    "import cv2\n",
    "from scipy import signal, ndimage\n",
    "\n",
    "# Modern Augmentation Libraries\n",
    "try:\n",
    "    import albumentations as A\n",
    "    from albumentations.pytorch import ToTensorV2\n",
    "    ALBUMENTATIONS_AVAILABLE = True\n",
    "    print(\"‚úÖ Albumentations verf√ºgbar\")\n",
    "except ImportError:\n",
    "    ALBUMENTATIONS_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  Albumentations nicht installiert\")\n",
    "\n",
    "try:\n",
    "    import imgaug.augmenters as iaa\n",
    "    IMGAUG_AVAILABLE = True\n",
    "    print(\"‚úÖ ImgAug verf√ºgbar\")\n",
    "except ImportError:\n",
    "    IMGAUG_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  ImgAug nicht installiert\")\n",
    "\n",
    "# Interactive Widgets\n",
    "from ipywidgets import interact, widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Streamlit (f√ºr Apps)\n",
    "import streamlit as st\n",
    "\n",
    "# Plotting Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Seeds for Reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"üé® Data Augmentation Setup abgeschlossen!\")\n",
    "print(f\"üìä TensorFlow: {tf.__version__}\")\n",
    "print(f\"üî¢ NumPy: {np.__version__}\")\n",
    "\n",
    "# GPU Check\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"üöÄ GPU verf√ºgbar f√ºr Training!\")\n",
    "else:\n",
    "    print(\"üíª CPU wird verwendet\")\n",
    "\n",
    "# Memory Optimization f√ºr GPU\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    try:\n",
    "        for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"üîß GPU Memory Growth aktiviert\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"‚ö†Ô∏è  GPU Konfiguration: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé® 06.3 Data Augmentation - K√ºnstliche Datenvergr√∂√üerung\n",
    "\n",
    "**Data Analytics & Big Data - Woche 6.3**  \n",
    "*IU Internationale Hochschule*\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Lernziele\n",
    "\n",
    "Nach diesem Notebook k√∂nnen Sie:\n",
    "- ‚úÖ **Data Augmentation** verstehen und anwenden f√ºr robuste Modelle\n",
    "- ‚úÖ **TensorFlow ImageDataGenerator** f√ºr automatische Augmentation nutzen\n",
    "- ‚úÖ **Custom Augmentation** mit modernen Libraries (Albumentations, imgaug)\n",
    "- ‚úÖ **Overfitting reduzieren** durch intelligente Datenvergr√∂√üerung\n",
    "- ‚úÖ **CIFAR-10 CNN** optimieren mit verschiedenen Augmentation-Techniken\n",
    "- ‚úÖ **Streamlit-App** f√ºr interaktive Augmentation-Experimente\n",
    "\n",
    "---\n",
    "\n",
    "## ü§î Was ist Data Augmentation?\n",
    "\n",
    "**Data Augmentation** = K√ºnstliche Vergr√∂√üerung des Datensatzes durch **Transformationen**\n",
    "\n",
    "### üí° Warum brauchen wir das?\n",
    "\n",
    "1. **üîÑ Mehr Trainingsdaten** ohne neue Bilder sammeln zu m√ºssen\n",
    "2. **üõ°Ô∏è Overfitting reduzieren** durch erh√∂hte Variabilit√§t  \n",
    "3. **üéØ Robustheit steigern** gegen Rotation, Verschiebung, Beleuchtung\n",
    "4. **üí∞ Kosteng√ºnstig** - keine neuen Datensammlungen n√∂tig\n",
    "\n",
    "### üîß Typische Augmentation-Techniken:\n",
    "\n",
    "- **üîÑ Geometric:** Rotation, Flip, Crop, Zoom\n",
    "- **üé® Photometric:** Brightness, Contrast, Saturation\n",
    "- **üå™Ô∏è Advanced:** Elastic Transforms, Cutout, Mixup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† CNN + Data Augmentation = Robuste Modelle\n",
    "\n",
    "### üéØ Praxisprojekt: CIFAR-10 Klassifikation\n",
    "\n",
    "**CIFAR-10** ist ein klassischer Computer Vision Benchmark:\n",
    "- **10 Klassen:** Flugzeug, Auto, Vogel, Katze, Hirsch, Hund, Frosch, Pferd, Schiff, LKW\n",
    "- **60,000 Bilder:** 32√ó32√ó3 RGB \n",
    "- **Herausforderung:** Kleine Bilder, aber reale Objekte\n",
    "\n",
    "### üí° Warum Data Augmentation bei CIFAR-10?\n",
    "\n",
    "1. **üìä Begrenzte Daten:** Nur 50,000 Trainingsbilder\n",
    "2. **üåç Real-World Variabilit√§t:** Objekte in verschiedenen Positionen/Lichtverh√§ltnissen\n",
    "3. **üõ°Ô∏è Overfitting Prevention:** CNNs neigen zum Auswendiglernen\n",
    "4. **üéØ Bessere Generalisierung:** Modell soll auch neue Bilder korrekt klassifizieren\n",
    "\n",
    "### üîÑ Unser Experimentaufbau:\n",
    "\n",
    "1. **Baseline CNN:** Ohne Augmentation\n",
    "2. **Augmented CNN:** Mit verschiedenen Transformationen\n",
    "3. **Advanced CNN:** Moderne Augmentation-Techniken\n",
    "4. **Vergleich:** Performance-Analyse und Interpretation\n",
    "\n",
    "### üìö Learning Path:\n",
    "- **Setup & Data Loading** ‚Üí **Baseline Model** ‚Üí **Basic Augmentation** ‚Üí **Advanced Techniques** ‚Üí **Streamlit App**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# For some convolving operations\n",
    "from scipy import signal\n",
    "from scipy import misc\n",
    "\n",
    "# DeepLearning Library Keras\n",
    "# Documentation https://keras.io/\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Dropout, Reshape\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "import utils\n",
    "\n",
    "#define dataroot\n",
    "root = 'data/dataset/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DO NOT EDIT!\n",
    "# For specifiying training with autoencoder structure\n",
    "ae_specification = widgets.Text()\n",
    "old_spec = 'None'\n",
    "\n",
    "\n",
    "# Two Loggers, depending if loss or accuracy should be visualized\n",
    "Loss_Logger = utils.LossGraph('loss')\n",
    "Acc_Logger = utils.LossGraph('acc')\n",
    "\n",
    "# Size for some plots with matplotlib\n",
    "figure_inches = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cifar-10 Klassifikationsaufgabe\n",
    "Neben dem MNIST-Datensatz ist auch Cifar10 ein kleiner Datensatz, der in den Anf√§ngen der CNNs verwendet wurde. Es gibt 10 verschiedene Klassen von einfachen Objekten oder Tieren. Die Bilder haben eine Gr√∂√üe von 32x32x3. In diesem Abschnitt sollten Sie ein gegebenes CNN tunen, um Bilder mit hoher Genauigkeit zu klassifizieren. \n",
    "\n",
    "Siehe auch: [Cifar-10](https://www.cs.toronto.edu/~kriz/cifar.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 5.3.1:</b> Laden Sie den Datensatz, legen Sie die Anzahl der Klassen fest, transformieren Sie die Beschriftungen und definieren Sie alle zugeh√∂rigen Klassen (wie z.B. Flugzeug,...) gem√§√ü den Kommentaren in den Codezellen.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä CIFAR-10 Dataset laden und analysieren\n",
    "\n",
    "print(\"üì• Lade CIFAR-10 Dataset...\")\n",
    "\n",
    "# CIFAR-10 von Keras laden\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print(\"‚úÖ CIFAR-10 erfolgreich geladen!\")\n",
    "\n",
    "# Dataset Info\n",
    "print(f\"\\nüìä Dataset √úbersicht:\")\n",
    "print(f\"   Training: {x_train.shape} Bilder, {y_train.shape} Labels\")\n",
    "print(f\"   Test: {x_test.shape} Bilder, {y_test.shape} Labels\")\n",
    "print(f\"   Bildformat: {x_train.shape[1:]} (Height √ó Width √ó Channels)\")\n",
    "print(f\"   Datentyp: {x_train.dtype}, Wertebereich: {x_train.min()} - {x_train.max()}\")\n",
    "\n",
    "# Speicher-Info\n",
    "train_size_mb = x_train.nbytes / (1024**2)\n",
    "test_size_mb = x_test.nbytes / (1024**2)\n",
    "print(f\"   Speicherbedarf: Train {train_size_mb:.1f} MB, Test {test_size_mb:.1f} MB\")\n",
    "\n",
    "# Klassenvariables definieren\n",
    "num_classes = 10\n",
    "classes = [\n",
    "    'Flugzeug',    # airplane\n",
    "    'Auto',        # automobile  \n",
    "    'Vogel',       # bird\n",
    "    'Katze',       # cat\n",
    "    'Hirsch',      # deer\n",
    "    'Hund',        # dog\n",
    "    'Frosch',      # frog\n",
    "    'Pferd',       # horse\n",
    "    'Schiff',      # ship\n",
    "    'LKW'          # truck\n",
    "]\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è  Klassen ({num_classes}):\")\n",
    "for i, class_name in enumerate(classes):\n",
    "    class_count = np.sum(y_train == i)\n",
    "    print(f\"   {i}: {class_name} ({class_count:,} Trainingsbilder)\")\n",
    "\n",
    "# Labels zu kategorischen Vektoren konvertieren\n",
    "y_train_categorical = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_categorical = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(f\"\\nüî¢ Label Transformation:\")\n",
    "print(f\"   Original: {y_train.shape} ‚Üí Categorical: {y_train_categorical.shape}\")\n",
    "print(f\"   Beispiel: Label {y_train[0][0]} ‚Üí {y_train_categorical[0]}\")\n",
    "\n",
    "# Daten normalisieren (0-255 ‚Üí 0-1)\n",
    "print(f\"\\nüîß Daten-Normalisierung...\")\n",
    "print(f\"   Vor Normalisierung: {x_train.min()} - {x_train.max()}\")\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "print(f\"   Nach Normalisierung: {x_train.min():.3f} - {x_train.max():.3f}\")\n",
    "print(\"   ‚úÖ Pixel-Werte jetzt zwischen 0 und 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many classes are in Cifar-10? \n",
    "# Hint: Name the variable \"num_classes = ...\"\n",
    "\n",
    "# STUDENT CODE HERE\n",
    "num_classes = 10\n",
    "\n",
    "# STUDENT CODE until HERE\n",
    "\n",
    "# üéØ Baseline Modell: Training ohne Data Augmentation\n",
    "\n",
    "print(\"üèóÔ∏è Trainiere Baseline CNN (ohne Augmentation)...\")\n",
    "\n",
    "# Baseline Modell erstellen und trainieren\n",
    "baseline_model = create_baseline_cnn()\n",
    "\n",
    "# Training-Konfiguration\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Callbacks f√ºr besseres Training\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=2),\n",
    "    tf.keras.callbacks.ModelCheckpoint('models/baseline_best.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Baseline Training (ohne Augmentation)\n",
    "print(f\"\\nüìö Starte Training f√ºr {epochs} Epochen...\")\n",
    "baseline_history = baseline_model.fit(\n",
    "    x_train, y_train_categorical,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_test, y_test_categorical),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Baseline Performance\n",
    "baseline_loss, baseline_acc = baseline_model.evaluate(x_test, y_test_categorical, verbose=0)\n",
    "print(f\"\\nüìä Baseline Ergebnisse:\")\n",
    "print(f\"   Test Accuracy: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)\")\n",
    "print(f\"   Test Loss: {baseline_loss:.4f}\")\n",
    "\n",
    "# Training History visualisieren\n",
    "def plot_training_history(history, title=\"Training History\"):\n",
    "    \"\"\"Visualisiert Training und Validation Metrics\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "    ax1.set_title(f'{title} - Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Loss\n",
    "    ax2.plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "    ax2.set_title(f'{title} - Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(baseline_history, \"üéØ Baseline CNN (ohne Augmentation)\")\n",
    "\n",
    "print(\"‚úÖ Baseline Training abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the labels into categorical vectors\n",
    "# Use the keras.utils.to_categorical function\n",
    "\n",
    "# Hint: \"y_train_categorical = ...\"\n",
    "# Hint2: \"y_test_categorical = ...\"\n",
    "\n",
    "# STUDENT CODE HERE\n",
    "\n",
    "# STUDENT CODE until HERE\n",
    "# üé® Modern Data Augmentation mit TensorFlow\n",
    "\n",
    "print(\"üé® Data Augmentation Techniken implementieren...\")\n",
    "\n",
    "# TensorFlow Data Augmentation Pipeline\n",
    "def create_augmentation_pipeline():\n",
    "    \"\"\"\n",
    "    üîß Erstellt moderne Augmentation Pipeline mit TensorFlow\n",
    "    \n",
    "    Techniken:\n",
    "    - Random Flip (horizontal)\n",
    "    - Random Rotation\n",
    "    - Random Zoom\n",
    "    - Random Translation\n",
    "    - Random Brightness\n",
    "    - Random Contrast\n",
    "    \"\"\"\n",
    "    augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.2),\n",
    "        tf.keras.layers.RandomZoom(0.2),\n",
    "        tf.keras.layers.RandomTranslation(0.2, 0.2),\n",
    "        tf.keras.layers.RandomBrightness(0.2),\n",
    "        tf.keras.layers.RandomContrast(0.2),\n",
    "    ], name=\"augmentation\")\n",
    "    \n",
    "    return augmentation\n",
    "\n",
    "# Augmentation Pipeline erstellen\n",
    "augment = create_augmentation_pipeline()\n",
    "\n",
    "# Visualisierung der Augmentation\n",
    "def visualize_augmentation(image, num_augmentations=9):\n",
    "    \"\"\"\n",
    "    üñºÔ∏è Zeigt Original + augmentierte Versionen eines Bildes\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Original Bild\n",
    "    plt.subplot(3, 3, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title('üñºÔ∏è Original', fontsize=12, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Augmentierte Versionen\n",
    "    for i in range(2, num_augmentations + 1):\n",
    "        augmented = augment(tf.expand_dims(image, 0), training=True)\n",
    "        augmented = tf.squeeze(augmented, 0)\n",
    "        \n",
    "        plt.subplot(3, 3, i)\n",
    "        plt.imshow(augmented)\n",
    "        plt.title(f'üé® Augmented {i-1}', fontsize=10)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle('üé® Data Augmentation Beispiele', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Beispiel-Visualisierung\n",
    "sample_image = x_train[42]  # Zuf√§lliges Bild ausw√§hlen\n",
    "sample_label = classes[y_train[42][0]]\n",
    "\n",
    "print(f\"üñºÔ∏è Augmentation Beispiel f√ºr: {sample_label}\")\n",
    "visualize_augmentation(sample_image)\n",
    "\n",
    "print(\"‚úÖ Data Augmentation Pipeline erstellt!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What classes are there? Define them in a list of strings named classes.\n",
    "# Hint: Call the list of strings \"classes = ...\"\n",
    "\n",
    "# STUDENT CODE HERE\n",
    "\n",
    "# STUDENT CODE until HERE\n",
    "# üéÆ Interactive Data Augmentation Explorer\n",
    "\n",
    "def interactive_augmentation_explorer():\n",
    "    \"\"\"\n",
    "    üéÆ Interaktiver Widget f√ºr Augmentation-Parameter\n",
    "    \"\"\"\n",
    "    print(\"üéÆ Interaktiver Data Augmentation Explorer\")\n",
    "    print(\"üîß Experimentieren Sie mit verschiedenen Parametern!\")\n",
    "    \n",
    "    # Widget-Steuerungen\n",
    "    image_selector = widgets.IntSlider(\n",
    "        value=42, min=0, max=100, step=1,\n",
    "        description='Bild Index:', style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    rotation_factor = widgets.FloatSlider(\n",
    "        value=0.2, min=0.0, max=0.5, step=0.1,\n",
    "        description='Rotation:', style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    zoom_factor = widgets.FloatSlider(\n",
    "        value=0.2, min=0.0, max=0.4, step=0.1,\n",
    "        description='Zoom:', style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    brightness_factor = widgets.FloatSlider(\n",
    "        value=0.2, min=0.0, max=0.4, step=0.1,\n",
    "        description='Helligkeit:', style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    contrast_factor = widgets.FloatSlider(\n",
    "        value=0.2, min=0.0, max=0.4, step=0.1,\n",
    "        description='Kontrast:', style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    def update_augmentation(image_idx, rotation, zoom, brightness, contrast):\n",
    "        \"\"\"Update Augmentation basierend auf Widget-Werten\"\"\"\n",
    "        # Custom Augmentation Pipeline erstellen\n",
    "        custom_augment = tf.keras.Sequential([\n",
    "            tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "            tf.keras.layers.RandomRotation(rotation),\n",
    "            tf.keras.layers.RandomZoom(zoom),\n",
    "            tf.keras.layers.RandomTranslation(0.2, 0.2),\n",
    "            tf.keras.layers.RandomBrightness(brightness),\n",
    "            tf.keras.layers.RandomContrast(contrast),\n",
    "        ])\n",
    "        \n",
    "        # Bild ausw√§hlen\n",
    "        original_image = x_train[image_idx]\n",
    "        label = classes[y_train[image_idx][0]]\n",
    "        \n",
    "        # Augmentierte Versionen erstellen\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "        \n",
    "        # Original\n",
    "        axes[0, 0].imshow(original_image)\n",
    "        axes[0, 0].set_title(f'üñºÔ∏è Original\\n{label}', fontweight='bold')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # Augmentierte Versionen\n",
    "        for i, ax in enumerate(axes.flat[1:]):\n",
    "            augmented = custom_augment(tf.expand_dims(original_image, 0), training=True)\n",
    "            augmented = tf.squeeze(augmented, 0)\n",
    "            \n",
    "            ax.imshow(augmented)\n",
    "            ax.set_title(f'üé® Augmented {i+1}')\n",
    "            ax.axis('off')\n",
    "        \n",
    "        plt.suptitle(f'üéÆ Interaktive Augmentation - {label}', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Parameter-Info\n",
    "        print(f\"üîß Aktuelle Parameter:\")\n",
    "        print(f\"   Rotation: {rotation:.1f}\")\n",
    "        print(f\"   Zoom: {zoom:.1f}\")  \n",
    "        print(f\"   Helligkeit: {brightness:.1f}\")\n",
    "        print(f\"   Kontrast: {contrast:.1f}\")\n",
    "    \n",
    "    # Interactive Widget\n",
    "    interact(update_augmentation,\n",
    "             image_idx=image_selector,\n",
    "             rotation=rotation_factor,\n",
    "             zoom=zoom_factor,\n",
    "             brightness=brightness_factor,\n",
    "             contrast=contrast_factor)\n",
    "\n",
    "# Widget anzeigen\n",
    "interactive_augmentation_explorer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 5.3.2:</b> Pr√ºfen Sie, ob Sie alles wie gew√ºnscht definiert haben.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üñºÔ∏è CIFAR-10 Dataset Exploration\n",
    "\n",
    "print(\"üîç CIFAR-10 Bilder visualisieren...\")\n",
    "\n",
    "# Zuf√§llige Samples aus jeder Klasse ausw√§hlen\n",
    "def plot_class_samples(num_samples=5):\n",
    "    \"\"\"\n",
    "    Zeigt zuf√§llige Samples aus jeder CIFAR-10 Klasse\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(num_classes, num_samples, figsize=(15, 20))\n",
    "    \n",
    "    for class_idx in range(num_classes):\n",
    "        # Finde alle Bilder dieser Klasse\n",
    "        class_indices = np.where(y_train.flatten() == class_idx)[0]\n",
    "        \n",
    "        # W√§hle zuf√§llige Samples\n",
    "        random_indices = np.random.choice(class_indices, num_samples, replace=False)\n",
    "        \n",
    "        for sample_idx in range(num_samples):\n",
    "            img_idx = random_indices[sample_idx]\n",
    "            image = x_train[img_idx]\n",
    "            \n",
    "            # Plot\n",
    "            axes[class_idx, sample_idx].imshow(image)\n",
    "            axes[class_idx, sample_idx].axis('off')\n",
    "            \n",
    "            # Titel nur f√ºr erste Spalte\n",
    "            if sample_idx == 0:\n",
    "                axes[class_idx, sample_idx].set_ylabel(\n",
    "                    f'{class_idx}: {classes[class_idx]}', \n",
    "                    fontsize=12, fontweight='bold'\n",
    "                )\n",
    "    \n",
    "    plt.suptitle('üñºÔ∏è CIFAR-10 Klassen-√úbersicht', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualisierung ausf√ºhren\n",
    "plot_class_samples(num_samples=8)\n",
    "\n",
    "# Einzelnes Bild detailliert analysieren\n",
    "sample_idx = 6\n",
    "sample_image = x_train[sample_idx]\n",
    "sample_label = y_train[sample_idx][0]\n",
    "sample_class = classes[sample_label]\n",
    "\n",
    "print(f\"\\nüîç Detailanalyse Beispielbild:\")\n",
    "print(f\"   Index: {sample_idx}\")\n",
    "print(f\"   Klasse: {sample_label} ({sample_class})\")\n",
    "print(f\"   Shape: {sample_image.shape}\")\n",
    "print(f\"   Pixel-Bereich: {sample_image.min():.3f} - {sample_image.max():.3f}\")\n",
    "\n",
    "# Einzelbild + Histogramm\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "# Original Bild\n",
    "axes[0].imshow(sample_image)\n",
    "axes[0].set_title(f'üñºÔ∏è {sample_class}', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# RGB Kan√§le einzeln\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, color in enumerate(colors):\n",
    "    axes[i+1].imshow(sample_image[:,:,i], cmap=color)\n",
    "    axes[i+1].set_title(f'{color.upper()} Kanal', fontsize=12)\n",
    "    axes[i+1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Klassen-Verteilung\n",
    "print(f\"\\nüìä Klassen-Verteilung im Trainingsdatensatz:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(range(num_classes), counts, color=plt.cm.tab10(range(num_classes)))\n",
    "plt.xlabel('Klasse')\n",
    "plt.ylabel('Anzahl Bilder')\n",
    "plt.title('üìä CIFAR-10 Klassen-Verteilung', fontsize=14, fontweight='bold')\n",
    "plt.xticks(range(num_classes), [f'{i}\\n{classes[i]}' for i in range(num_classes)], rotation=45)\n",
    "\n",
    "# Zahlen auf Balken\n",
    "for i, (bar, count) in enumerate(zip(bars, counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50, \n",
    "             f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Dataset-Exploration abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 5.3.3:</b> Suchen Sie ein Bild eines Pferdes und plotten Sie es mit dem Code oben. Sie k√∂nnen die gleiche Code-Zelle verwenden.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 5.3.4:</b> Preprocessen Sie die Daten, um Werte zwischen 0 und 1 zu gew√§hrleisten. Dividieren Sie dazu die rgb-Werte durch ihren Maximalwert.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "# Divide RGB values of train AND test set \n",
    "# by their maximum value to ensure values between [0,1]\n",
    "\n",
    "# STUDENT CODE HERE\n",
    "\n",
    "# STUDENT CODE until HERE\n",
    "\n",
    "# üöÄ Advanced Data Augmentation Techniques\n",
    "\n",
    "print(\"üöÄ Advanced Augmentation Techniken implementieren...\")\n",
    "\n",
    "# Custom Advanced Augmentation Functions\n",
    "def cutout(image, size=8):\n",
    "    \"\"\"\n",
    "    ‚úÇÔ∏è Cutout: Zuf√§llige Rechtecke im Bild ausschneiden\n",
    "    \"\"\"\n",
    "    h, w, c = image.shape\n",
    "    y = np.random.randint(h)\n",
    "    x = np.random.randint(w)\n",
    "    \n",
    "    y1 = np.clip(y - size // 2, 0, h)\n",
    "    y2 = np.clip(y + size // 2, 0, h)\n",
    "    x1 = np.clip(x - size // 2, 0, w)\n",
    "    x2 = np.clip(x + size // 2, 0, w)\n",
    "    \n",
    "    image_cutout = image.copy()\n",
    "    image_cutout[y1:y2, x1:x2] = 0\n",
    "    return image_cutout\n",
    "\n",
    "def mixup(x1, x2, y1, y2, alpha=0.2):\n",
    "    \"\"\"\n",
    "    üå™Ô∏è MixUp: Lineare Interpolation zwischen zwei Bildern\n",
    "    \"\"\"\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    x_mixed = lam * x1 + (1 - lam) * x2\n",
    "    y_mixed = lam * y1 + (1 - lam) * y2\n",
    "    return x_mixed, y_mixed\n",
    "\n",
    "def elastic_transform(image, alpha=50, sigma=5):\n",
    "    \"\"\"\n",
    "    üåä Elastic Transform: Elastische Verzerrung\n",
    "    \"\"\"\n",
    "    from scipy.ndimage import gaussian_filter, map_coordinates\n",
    "    \n",
    "    shape = image.shape[:2]\n",
    "    dx = gaussian_filter((np.random.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "    dy = gaussian_filter((np.random.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "    \n",
    "    y, x = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))\n",
    "    indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1))\n",
    "    \n",
    "    transformed = np.zeros_like(image)\n",
    "    for i in range(image.shape[2]):\n",
    "        transformed[:,:,i] = map_coordinates(\n",
    "            image[:,:,i], indices, order=1, mode='reflect'\n",
    "        ).reshape(shape)\n",
    "    \n",
    "    return transformed\n",
    "\n",
    "# Advanced Augmentation Demonstrationen\n",
    "print(\"üé® Advanced Augmentation Beispiele:\")\n",
    "\n",
    "# Beispielbild ausw√§hlen\n",
    "sample_idx = 123\n",
    "original_image = x_train[sample_idx]\n",
    "label = classes[y_train[sample_idx][0]]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].imshow(original_image)\n",
    "axes[0, 0].set_title(f'üñºÔ∏è Original\\n{label}', fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Standard Augmentations\n",
    "standard_augmented = augment(tf.expand_dims(original_image, 0), training=True)\n",
    "standard_augmented = tf.squeeze(standard_augmented, 0)\n",
    "axes[0, 1].imshow(standard_augmented)\n",
    "axes[0, 1].set_title('üîÑ Standard\\nAugmentation')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Cutout\n",
    "cutout_image = cutout(original_image)\n",
    "axes[0, 2].imshow(cutout_image)\n",
    "axes[0, 2].set_title('‚úÇÔ∏è Cutout')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Elastic Transform\n",
    "try:\n",
    "    elastic_image = elastic_transform(original_image)\n",
    "    axes[0, 3].imshow(elastic_image)\n",
    "    axes[0, 3].set_title('üåä Elastic\\nTransform')\n",
    "except:\n",
    "    axes[0, 3].imshow(original_image)\n",
    "    axes[0, 3].set_title('üåä Elastic\\n(nicht verf√ºgbar)')\n",
    "axes[0, 3].axis('off')\n",
    "\n",
    "# MixUp Beispiel\n",
    "sample_idx2 = 456\n",
    "image2 = x_train[sample_idx2]\n",
    "label2 = classes[y_train[sample_idx2][0]]\n",
    "y1_cat = y_train_categorical[sample_idx]\n",
    "y2_cat = y_train_categorical[sample_idx2]\n",
    "\n",
    "mixed_image, mixed_label = mixup(original_image, image2, y1_cat, y2_cat)\n",
    "axes[1, 0].imshow(image2)\n",
    "axes[1, 0].set_title(f'üñºÔ∏è Bild 2\\n{label2}')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(mixed_image)\n",
    "axes[1, 1].set_title('üå™Ô∏è MixUp\\nKombination')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Kombinierte Augmentations\n",
    "combined_augmented = augment(tf.expand_dims(cutout_image, 0), training=True)\n",
    "combined_augmented = tf.squeeze(combined_augmented, 0)\n",
    "axes[1, 2].imshow(combined_augmented)\n",
    "axes[1, 2].set_title('üé® Kombiniert\\nCutout + Standard')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "# Extreme Augmentation\n",
    "extreme_augment = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.4),\n",
    "    tf.keras.layers.RandomZoom(0.3),\n",
    "    tf.keras.layers.RandomTranslation(0.3, 0.3),\n",
    "    tf.keras.layers.RandomBrightness(0.3),\n",
    "    tf.keras.layers.RandomContrast(0.3),\n",
    "])\n",
    "extreme_augmented = extreme_augment(tf.expand_dims(original_image, 0), training=True)\n",
    "extreme_augmented = tf.squeeze(extreme_augmented, 0)\n",
    "axes[1, 3].imshow(extreme_augmented)\n",
    "axes[1, 3].set_title('üî• Extreme\\nAugmentation')\n",
    "axes[1, 3].axis('off')\n",
    "\n",
    "plt.suptitle('üöÄ Advanced Data Augmentation Techniken', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Advanced Augmentation Techniken demonstriert!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 5.3.5:</b> Wie viele Trainings- und Testdaten gibt es?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 5.3.6:</b> Warum Daten normalisieren?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 5.3.7:</b> Warum wird ein kategorischer Vektor an Stelle eines einzelnen Outputs verwendet?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klassifikationsmodelle f√ºr Cifar-10\n",
    "\n",
    "### Neuronales Netzwerk Klassifikator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "num_classes = 10  # Assuming CIFAR-10 with 10 classes\n",
    "\n",
    "# üèóÔ∏è CNN Modell-Definitionen f√ºr CIFAR-10\n",
    "\n",
    "def create_baseline_cnn() -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    üéØ Baseline CNN ohne Data Augmentation\n",
    "    \n",
    "    Einfache CNN-Architektur f√ºr CIFAR-10:\n",
    "    - 2 Conv2D + MaxPooling Blocks\n",
    "    - Dense Layer mit Dropout\n",
    "    - Softmax f√ºr 10 Klassen\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        # Input Layer\n",
    "        tf.keras.layers.Input(shape=(32, 32, 3), name='input'),\n",
    "        \n",
    "        # Convolutional Block 1\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', name='conv1'),\n",
    "        tf.keras.layers.BatchNormalization(name='bn1'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2), name='pool1'),\n",
    "        \n",
    "        # Convolutional Block 2\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='conv2'),\n",
    "        tf.keras.layers.BatchNormalization(name='bn2'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2), name='pool2'),\n",
    "        \n",
    "        # Convolutional Block 3\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='conv3'),\n",
    "        tf.keras.layers.BatchNormalization(name='bn3'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2), name='pool3'),\n",
    "        \n",
    "        # Dense Layers\n",
    "        tf.keras.layers.Flatten(name='flatten'),\n",
    "        tf.keras.layers.Dense(512, activation='relu', name='dense1'),\n",
    "        tf.keras.layers.Dropout(0.5, name='dropout1'),\n",
    "        tf.keras.layers.Dense(256, activation='relu', name='dense2'),\n",
    "        tf.keras.layers.Dropout(0.3, name='dropout2'),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax', name='output')\n",
    "    ], name='BaselineCNN')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_advanced_cnn() -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    üöÄ Advanced CNN mit modernen Techniken\n",
    "    \n",
    "    Verbesserungen:\n",
    "    - Mehr Convolutional Layers\n",
    "    - Residual-√§hnliche Verbindungen\n",
    "    - Global Average Pooling\n",
    "    - Bessere Regularisierung\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.layers.Input(shape=(32, 32, 3), name='input')\n",
    "    \n",
    "    # Block 1\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Dropout(0.25)(x)\n",
    "    \n",
    "    # Block 2\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Dropout(0.25)(x)\n",
    "    \n",
    "    # Block 3\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Dropout(0.25)(x)\n",
    "    \n",
    "    # Global Average Pooling\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Output\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax', name='output')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='AdvancedCNN')\n",
    "    return model\n",
    "\n",
    "# Modelle erstellen und anzeigen\n",
    "print(\"üèóÔ∏è CNN-Modelle erstellen...\")\n",
    "\n",
    "baseline_model = create_baseline_cnn()\n",
    "advanced_model = create_advanced_cnn()\n",
    "\n",
    "print(\"\\nüìä Baseline CNN Architektur:\")\n",
    "baseline_model.summary()\n",
    "\n",
    "print(\"\\nüìä Advanced CNN Architektur:\")\n",
    "advanced_model.summary()\n",
    "\n",
    "# Parameter-Vergleich\n",
    "baseline_params = baseline_model.count_params()\n",
    "advanced_params = advanced_model.count_params()\n",
    "\n",
    "print(f\"\\nüî¢ Parameter-Vergleich:\")\n",
    "print(f\"   Baseline CNN: {baseline_params:,} Parameter\")\n",
    "print(f\"   Advanced CNN: {advanced_params:,} Parameter\")\n",
    "print(f\"   Differenz: {advanced_params - baseline_params:,} Parameter (+{((advanced_params/baseline_params)-1)*100:.1f}%)\")\n",
    "\n",
    "# Compile Models\n",
    "baseline_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "advanced_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Modelle erfolgreich erstellt und kompiliert!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network Klassifikator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cnn()->Model:\n",
    "    \n",
    "    input_layer = Input(shape = x_train.shape[1:], name='Input_CNN') # channels last\n",
    "    \n",
    "    conv1 = Conv2D(filters= 16, kernel_size = (3,3), padding='same', activation = 'relu', name='Conv1')(input_layer)\n",
    "    max_pool1 = MaxPooling2D(pool_size = (2,2), strides = (2,2), padding='valid', name='Pool1')(conv1)\n",
    "\n",
    "    conv2 = Conv2D(filters = 32, kernel_size = (3,3), padding='same', activation = 'relu', name='Conv2')(max_pool1)\n",
    "    max_pool2 = MaxPooling2D(pool_size = (2,2), strides = (2,2), padding='valid', name='Pool2')(conv2)\n",
    "\n",
    "    flattened = Flatten(name='Flatt_CNN')(max_pool2)\n",
    "    \n",
    "    fc1 = Dense(256, activation = 'relu', name='FC-1')(flattened)\n",
    "    \n",
    "    output = Dense(num_classes, activation = 'softmax', name='Output_CNN')(fc1)\n",
    "    \n",
    "    model = Model(inputs= input_layer, outputs = output)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vergleich von MLP und CNN Klassifikatoren:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 5.3.8:</b> Um die Bilder in Cifar-10 zu klassifizieren, verwenden Sie die gegebenen MLP- und CNN-Modelle, um zu untersuchen, welches besser abschneidet.\n",
    "Trainieren Sie beide Netzwerke f√ºr 10 Epochen und schauen Sie sich die Ergebnisse an.\n",
    "F√ºhlen Sie sich frei, den Code in den beiden Code-Zellen unten zu verwenden und zu √§ndern. Wenn Ihr Netzwerk nicht trainiert, haben Sie m√∂glicherweise die rgb-Werte nicht richtig aufbereitet (z. B. haben Sie nicht normalisiert oder zu oft).\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Hinweis:</b> Aufbau der folgenden Codezellen\n",
    "<ul>\n",
    "<li> Benutzen Sie die vordefinierten Funktionen, um Ihr Modell zu erstellen\n",
    "<li> Definieren Sie den gemeinsamen TensorBoard-Logger mit der Konfiguration, um die Trainingsergebnisse sp√§ter zu betrachten\n",
    "<li> Kompillieren und trainieren Sie das Modell\n",
    "<li> Tipp: Wenn Ihre Modelle nichts Lernen, √ºberpr√ºfen Sie Ihre Datennormalisierung. Vielleicht haben Sie Ihre Daten nicht oder zu oft normalisiert.\n",
    "</li>\n",
    "\n",
    "\n",
    "</ul>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ CNN Training mit Data Augmentation\n",
    "\n",
    "print(\"üéØ Trainiere CNN mit Data Augmentation...\")\n",
    "\n",
    "# Augmented Model erstellen (mit integrierter Augmentation)\n",
    "def create_augmented_cnn():\n",
    "    \"\"\"\n",
    "    üé® CNN mit integrierter Data Augmentation\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.layers.Input(shape=(32, 32, 3), name='input')\n",
    "    \n",
    "    # Data Augmentation Layer (nur w√§hrend Training aktiv)\n",
    "    x = augment(inputs, training=True)\n",
    "    \n",
    "    # CNN Architecture (gleich wie Baseline f√ºr fairen Vergleich)\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', name='conv1')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(name='bn1')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), name='pool1')(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='conv2')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(name='bn2')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), name='pool2')(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='conv3')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(name='bn3')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), name='pool3')(x)\n",
    "    \n",
    "    x = tf.keras.layers.Flatten(name='flatten')(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu', name='dense1')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5, name='dropout1')(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu', name='dense2')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, name='dropout2')(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax', name='output')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='AugmentedCNN')\n",
    "    return model\n",
    "\n",
    "# Augmented Model erstellen und kompilieren\n",
    "augmented_model = create_augmented_cnn()\n",
    "augmented_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Augmented CNN Architektur:\")\n",
    "augmented_model.summary()\n",
    "\n",
    "# Training mit Augmentation\n",
    "print(f\"\\nüìö Starte Training mit Data Augmentation f√ºr {epochs} Epochen...\")\n",
    "\n",
    "# Callbacks f√ºr besseres Training\n",
    "callbacks_aug = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3),\n",
    "    tf.keras.callbacks.ModelCheckpoint('models/augmented_best.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Training\n",
    "augmented_history = augmented_model.fit(\n",
    "    x_train, y_train_categorical,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_test, y_test_categorical),\n",
    "    callbacks=callbacks_aug,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Augmented Performance\n",
    "augmented_loss, augmented_acc = augmented_model.evaluate(x_test, y_test_categorical, verbose=0)\n",
    "\n",
    "print(f\"\\nüìä Augmented CNN Ergebnisse:\")\n",
    "print(f\"   Test Accuracy: {augmented_acc:.4f} ({augmented_acc*100:.2f}%)\")\n",
    "print(f\"   Test Loss: {augmented_loss:.4f}\")\n",
    "\n",
    "# Training History visualisieren\n",
    "plot_training_history(augmented_history, \"üé® Augmented CNN\")\n",
    "\n",
    "print(\"‚úÖ Augmented Training abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Neural Network (MLP)\n",
    "nn_model = model_nn()\n",
    "config_cnn = 'UNRECOGNIZEABLE_NAME_EDIT_ME_PLEASE' # Give a recognizable name\n",
    "\n",
    "# The TensorBoard is a feature of tensorflow for the visualization of the training process \n",
    "cnn_logger = TensorBoard(log_dir='logs/cnn_logs/'+config_cnn+'/') \n",
    "\n",
    "nn_model.compile(loss='categorical_crossentropy', metrics = ['accuracy'], optimizer='Adam')\n",
    "nn_model.fit(x_train, y_train_categorical, batch_size = 64, epochs = 1, \n",
    "            validation_data = (x_test, y_test_categorical), callbacks = [cnn_logger, Acc_Logger]) #TODO\n",
    "\n",
    "# üìä Model Performance Vergleich\n",
    "\n",
    "print(\"üìä Vergleiche Performance verschiedener Modelle...\")\n",
    "\n",
    "# Performance Metriken sammeln\n",
    "models_performance = {\n",
    "    'Baseline CNN (ohne Augmentation)': {\n",
    "        'accuracy': baseline_acc,\n",
    "        'loss': baseline_loss,\n",
    "        'history': baseline_history\n",
    "    },\n",
    "    'Augmented CNN (mit Augmentation)': {\n",
    "        'accuracy': augmented_acc,\n",
    "        'loss': augmented_loss,\n",
    "        'history': augmented_history\n",
    "    }\n",
    "}\n",
    "\n",
    "# Performance-Tabelle\n",
    "print(\"üìã Performance √úbersicht:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Modell':<35} {'Accuracy':<12} {'Loss':<12} {'Improvement'}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "baseline_accuracy = models_performance['Baseline CNN (ohne Augmentation)']['accuracy']\n",
    "for model_name, metrics in models_performance.items():\n",
    "    accuracy = metrics['accuracy']\n",
    "    loss = metrics['loss']\n",
    "    improvement = ((accuracy / baseline_accuracy) - 1) * 100\n",
    "    improvement_str = f\"+{improvement:.2f}%\" if improvement > 0 else f\"{improvement:.2f}%\"\n",
    "    \n",
    "    print(f\"{model_name:<35} {accuracy:.4f} ({accuracy*100:.2f}%)  {loss:.4f}      {improvement_str}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Visualisierung der Performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Accuracy Vergleich\n",
    "model_names = list(models_performance.keys())\n",
    "accuracies = [models_performance[name]['accuracy'] for name in model_names]\n",
    "colors = ['#FF6B6B', '#4ECDC4']\n",
    "\n",
    "bars = axes[0, 0].bar(range(len(model_names)), accuracies, color=colors, alpha=0.8)\n",
    "axes[0, 0].set_title('üìä Test Accuracy Vergleich', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_xticks(range(len(model_names)))\n",
    "axes[0, 0].set_xticklabels([name.split(' (')[0] for name in model_names], rotation=45)\n",
    "axes[0, 0].set_ylim(0, 1)\n",
    "\n",
    "# Werte auf Balken anzeigen\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{acc:.3f}\\n({acc*100:.1f}%)', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Loss Vergleich\n",
    "losses = [models_performance[name]['loss'] for name in model_names]\n",
    "bars = axes[0, 1].bar(range(len(model_names)), losses, color=colors, alpha=0.8)\n",
    "axes[0, 1].set_title('üìâ Test Loss Vergleich', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].set_xticks(range(len(model_names)))\n",
    "axes[0, 1].set_xticklabels([name.split(' (')[0] for name in model_names], rotation=45)\n",
    "\n",
    "# Werte auf Balken anzeigen\n",
    "for bar, loss in zip(bars, losses):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{loss:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Training History Vergleich - Accuracy\n",
    "for i, (name, metrics) in enumerate(models_performance.items()):\n",
    "    history = metrics['history']\n",
    "    epochs_range = range(1, len(history.history['accuracy']) + 1)\n",
    "    label = name.split(' (')[0]\n",
    "    axes[1, 0].plot(epochs_range, history.history['val_accuracy'], \n",
    "                   label=f'{label} Validation', marker='o', color=colors[i], linewidth=2)\n",
    "    axes[1, 0].plot(epochs_range, history.history['accuracy'], \n",
    "                   label=f'{label} Training', linestyle='--', color=colors[i], alpha=0.7)\n",
    "\n",
    "axes[1, 0].set_title('üìà Training Accuracy Verlauf', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Training History Vergleich - Loss\n",
    "for i, (name, metrics) in enumerate(models_performance.items()):\n",
    "    history = metrics['history']\n",
    "    epochs_range = range(1, len(history.history['loss']) + 1)\n",
    "    label = name.split(' (')[0]\n",
    "    axes[1, 1].plot(epochs_range, history.history['val_loss'], \n",
    "                   label=f'{label} Validation', marker='o', color=colors[i], linewidth=2)\n",
    "    axes[1, 1].plot(epochs_range, history.history['loss'], \n",
    "                   label=f'{label} Training', linestyle='--', color=colors[i], alpha=0.7)\n",
    "\n",
    "axes[1, 1].set_title('üìâ Training Loss Verlauf', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyse der Verbesserung\n",
    "improvement_pct = ((augmented_acc / baseline_acc) - 1) * 100\n",
    "print(f\"\\nüéØ Analyse der Verbesserung durch Data Augmentation:\")\n",
    "print(f\"   Accuracy Verbesserung: +{improvement_pct:.2f}%\")\n",
    "print(f\"   Absolute Verbesserung: +{(augmented_acc - baseline_acc)*100:.2f} Prozentpunkte\")\n",
    "\n",
    "if improvement_pct > 2:\n",
    "    print(\"   ‚úÖ Signifikante Verbesserung durch Augmentation!\")\n",
    "elif improvement_pct > 0:\n",
    "    print(\"   üìä Moderate Verbesserung durch Augmentation\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Keine Verbesserung - m√∂glicherweise √úberaugmentation\")\n",
    "\n",
    "print(\"\\nüîç M√∂gliche Gr√ºnde f√ºr Performance-Unterschiede:\")\n",
    "print(\"   ‚Ä¢ Data Augmentation reduziert Overfitting\")\n",
    "print(\"   ‚Ä¢ Mehr Variabilit√§t in Trainingsdaten\")\n",
    "print(\"   ‚Ä¢ Bessere Generalisierung auf neue Bilder\")\n",
    "print(\"   ‚Ä¢ Robustheit gegen Rotation, Translation, etc.\")\n",
    "\n",
    "print(\"‚úÖ Model Vergleich abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 5.3.9:</b> Verwenden Sie TensorBoard, um Ihren Trainingsfortschritt zu kontrollieren. Eine Erkl√§rung, wie Sie Ihr TensorBoard √∂ffnen, finden Sie hier:\n",
    "    <a href=\"https://github.com/tensorflow/tensorboard/blob/master/docs/r1/summaries.md\">TensorBoard</a>  (unten auf der Webseite)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CNN\n",
    "%load_ext tensorboard\n",
    "cnn_model = model_cnn()\n",
    "config_cnn = 'UNRECOGNIZEABLE_NAME_EDIT_ME_PLEASE' # give a recognizable name\n",
    "\n",
    "cnn_model.compile(loss='categorical_crossentropy', metrics = ['accuracy'], optimizer='Adam')\n",
    "cnn_model.fit(x_train, y_train_categorical, batch_size = 32, epochs = 1, \n",
    "          validation_data = (x_test, y_test_categorical), callbacks = [Acc_Logger], verbose = 1)\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 5.3.10:</b> Welches Netzwerk performt besser?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 5.3.11:</b> Wie viele Parameter haben die Netze? Verwenden Sie dazu die summary Methode (siehe Keras-Docs)...\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 5.3.12:</b> Wo sind die meisten Parameter in diesem CNN gespeichert?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge: Optimieren Sie Ihr Network! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 5.3.13:</b> Versuchen Sie, eines der Modelle so zu verbessern, dass Ihre Validierungsgenauigkeit einmal h√∂her als 0,75 Prozent ist!\n",
    "\n",
    "<ul>\n",
    "<li>Hinweis: Versuchen Sie, zuerst zu overfitten und dann zu regulieren. \n",
    "<li>Hinweis 2: Verwenden Sie daher L1/L2 - Regularisierung und/oder Dropout. Auch BatchNormalization k√∂nnte die Sache verbessern. Schauen Sie deshalb auf der Keras-Website nach Beispielen oder fragen Sie Tutoren.\n",
    "<li>Hinweis 3: Verwenden Sie eine der Funktionen <code>def model_nn()</code> oder <code>def model_cnn()</code> von oben. Viel Spa√ü und gutes Gelingen!\n",
    "\n",
    "</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten Augmentation\n",
    "\n",
    "Eine weitere M√∂glichkeit, Ihr Netzwerk zu regularisieren, ist das Vergr√∂√üern der Trainingsdaten. Verwenden Sie dazu den ImageDataGenerator von Keras. Wir werden sp√§ter selbst Bilder verschieben und drehen, nachdem wir auf Cifar-10 optimiert haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Professional Data Augmentation mit Albumentations\n",
    "\n",
    "if ALBUMENTATIONS_AVAILABLE:\n",
    "    print(\"üöÄ Albumentations - Professional Augmentation Library\")\n",
    "    \n",
    "    # Albumentations Pipeline definieren\n",
    "    albumentations_transform = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "        A.GaussianBlur(blur_limit=3, p=0.3),\n",
    "        A.CoarseDropout(max_holes=8, max_height=8, max_width=8, p=0.3),\n",
    "    ])\n",
    "    \n",
    "    # Albumentations Demonstration\n",
    "    def demonstrate_albumentations():\n",
    "        \"\"\"Zeigt Albumentations Augmentationen\"\"\"\n",
    "        sample_image = x_train[99]\n",
    "        # Pixel-Werte zur√ºck zu 0-255 f√ºr Albumentations\n",
    "        sample_image_uint8 = (sample_image * 255).astype(np.uint8)\n",
    "        \n",
    "        fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "        \n",
    "        # Original\n",
    "        axes[0, 0].imshow(sample_image)\n",
    "        axes[0, 0].set_title('üñºÔ∏è Original', fontweight='bold')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # Albumentations Augmentationen\n",
    "        for i, ax in enumerate(axes.flat[1:]):\n",
    "            augmented = albumentations_transform(image=sample_image_uint8)['image']\n",
    "            augmented_normalized = augmented.astype(np.float32) / 255.0\n",
    "            \n",
    "            ax.imshow(augmented_normalized)\n",
    "            ax.set_title(f'üöÄ Albumentations {i+1}')\n",
    "            ax.axis('off')\n",
    "        \n",
    "        plt.suptitle('üöÄ Professional Augmentation mit Albumentations', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    demonstrate_albumentations()\n",
    "    \n",
    "    # Albumentations Wrapper f√ºr TensorFlow\n",
    "    def albumentations_wrapper(image):\n",
    "        \"\"\"TensorFlow-kompatible Albumentations Funktion\"\"\"\n",
    "        def apply_albumentations(img):\n",
    "            img_uint8 = (img * 255).astype(np.uint8)\n",
    "            augmented = albumentations_transform(image=img_uint8)['image']\n",
    "            return augmented.astype(np.float32) / 255.0\n",
    "        \n",
    "        return tf.py_function(apply_albumentations, [image], tf.float32)\n",
    "    \n",
    "    print(\"‚úÖ Albumentations Pipeline erstellt!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Albumentations nicht verf√ºgbar - verwende TensorFlow Augmentation\")\n",
    "    \n",
    "    # Fallback: Erweiterte TensorFlow Augmentation\n",
    "    professional_augment = tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.3),\n",
    "        tf.keras.layers.RandomZoom(0.2),\n",
    "        tf.keras.layers.RandomTranslation(0.2, 0.2),\n",
    "        tf.keras.layers.RandomBrightness(0.2),\n",
    "        tf.keras.layers.RandomContrast(0.2),\n",
    "        # Custom Augmentations k√∂nnen hier hinzugef√ºgt werden\n",
    "    ], name=\"professional_augmentation\")\n",
    "    \n",
    "    # Professional Augmentation Demonstration\n",
    "    sample_image = x_train[99]\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "    \n",
    "    # Original\n",
    "    axes[0, 0].imshow(sample_image)\n",
    "    axes[0, 0].set_title('üñºÔ∏è Original', fontweight='bold')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Professional Augmentationen\n",
    "    for i, ax in enumerate(axes.flat[1:]):\n",
    "        augmented = professional_augment(tf.expand_dims(sample_image, 0), training=True)\n",
    "        augmented = tf.squeeze(augmented, 0)\n",
    "        \n",
    "        ax.imshow(augmented)\n",
    "        ax.set_title(f'üöÄ Professional {i+1}')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('üöÄ Professional Augmentation mit TensorFlow', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Professional TensorFlow Augmentation Pipeline erstellt!\")\n",
    "\n",
    "# Augmentation Strategy Vergleich\n",
    "print(\"\\nüìä Augmentation Strategy √úbersicht:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Strategy':<25} {'Library':<15} {'Komplexit√§t':<15} {'Performance'}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Basic TensorFlow':<25} {'TensorFlow':<15} {'Niedrig':<15} {'Gut'}\")\n",
    "print(f\"{'Advanced TensorFlow':<25} {'TensorFlow':<15} {'Mittel':<15} {'Sehr gut'}\")\n",
    "if ALBUMENTATIONS_AVAILABLE:\n",
    "    print(f\"{'Albumentations':<25} {'Albumentations':<15} {'Hoch':<15} {'Exzellent'}\")\n",
    "print(f\"{'Custom Functions':<25} {'Custom':<15} {'Sehr hoch':<15} {'Variabel'}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n‚úÖ Professional Augmentation Setup abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eine etwas noch herausfordernde Challenge ((COOKIE AUFGABE! :)))\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 5.3.14:</b> Verbessern Sie Ihr Modell und passen Sie es an, wie genau kann es jetzt werden?\n",
    "Die L√∂sung ist in der Lage, eine Genauigkeit von 0,8894 bei der Validierung zu erreichen.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cnn_aug()->Model:\n",
    "\n",
    "    input_layer = Input(shape = x_train.shape[1:], name='Input_CNN') # channels last\n",
    "    \n",
    "    conv1 = Conv2D(filters= 16, kernel_size = (3,3), padding='same', activation = 'relu', name='Conv1')(input_layer)\n",
    "    max_pool1 = MaxPooling2D(pool_size = (2,2), strides = (2,2), padding='valid', name='Pool1')(conv1)\n",
    "\n",
    "    conv2 = Conv2D(filters = 32, kernel_size = (3,3), padding='same', activation = 'relu', name='Conv2')(max_pool1)\n",
    "    max_pool2 = MaxPooling2D(pool_size = (2,2), strides = (2,2), padding='valid', name='Pool2')(conv2)\n",
    "\n",
    "    flattened = Flatten(name='Flatt_CNN')(max_pool2)\n",
    "    \n",
    "    fc1 = Dense(256, activation = 'relu', name='FC-1')(flattened)\n",
    "    \n",
    "    output = Dense(num_classes, activation = 'softmax', name='Output_CNN')(fc1)\n",
    "    \n",
    "    model = Model(inputs= input_layer, outputs = output)\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "# üèÜ Final Model: Best Practice CNN mit optimaler Augmentation\n",
    "\n",
    "print(\"üèÜ Erstelle Final Model mit Best Practices...\")\n",
    "\n",
    "def create_final_cnn():\n",
    "    \"\"\"\n",
    "    üèÜ Final CNN mit allen Best Practices:\n",
    "    - Optimale Augmentation\n",
    "    - Moderne Architektur\n",
    "    - Bessere Regularisierung\n",
    "    - Optimierte Hyperparameter\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.layers.Input(shape=(32, 32, 3), name='input')\n",
    "    \n",
    "    # Optimierte Augmentation (nur w√§hrend Training)\n",
    "    if ALBUMENTATIONS_AVAILABLE:\n",
    "        # Hier w√ºrde Albumentations integriert werden\n",
    "        x = augment(inputs, training=True)\n",
    "    else:\n",
    "        x = augment(inputs, training=True)\n",
    "    \n",
    "    # Improved CNN Architecture\n",
    "    # Block 1\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Dropout(0.25)(x)\n",
    "    \n",
    "    # Block 2  \n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Dropout(0.25)(x)\n",
    "    \n",
    "    # Block 3\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Dropout(0.25)(x)\n",
    "    \n",
    "    # Block 4 (Additional)\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Dense Layers\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax', name='output')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='FinalCNN')\n",
    "    return model\n",
    "\n",
    "# Final Model erstellen\n",
    "final_model = create_final_cnn()\n",
    "\n",
    "# Optimierte Compilation\n",
    "final_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', 'top_3_accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Final CNN Architektur:\")\n",
    "final_model.summary()\n",
    "\n",
    "# Advanced Callbacks\n",
    "final_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        patience=7, \n",
    "        restore_best_weights=True, \n",
    "        monitor='val_accuracy'\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        factor=0.5, \n",
    "        patience=4, \n",
    "        min_lr=1e-7,\n",
    "        monitor='val_accuracy'\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'models/final_best.h5', \n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy'\n",
    "    )\n",
    "]\n",
    "\n",
    "# Final Training\n",
    "print(f\"\\nüéØ Final Training f√ºr bis zu {epochs} Epochen...\")\n",
    "final_history = final_model.fit(\n",
    "    x_train, y_train_categorical,\n",
    "    batch_size=32,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_test, y_test_categorical),\n",
    "    callbacks=final_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Final Evaluation\n",
    "final_loss, final_acc, final_top3 = final_model.evaluate(x_test, y_test_categorical, verbose=0)\n",
    "\n",
    "print(f\"\\nüèÜ Final Model Ergebnisse:\")\n",
    "print(f\"   Test Accuracy: {final_acc:.4f} ({final_acc*100:.2f}%)\")\n",
    "print(f\"   Top-3 Accuracy: {final_top3:.4f} ({final_top3*100:.2f}%)\")\n",
    "print(f\"   Test Loss: {final_loss:.4f}\")\n",
    "\n",
    "# Final Comparison\n",
    "print(f\"\\nüìä Gesamt-Vergleich aller Modelle:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Modell':<25} {'Accuracy':<12} {'Top-3 Acc':<12} {'Loss':<10} {'Verbesserung'}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Baseline CNN':<25} {baseline_acc:.4f}       {'N/A':<12} {baseline_loss:.4f}     {'Referenz'}\")\n",
    "print(f\"{'Augmented CNN':<25} {augmented_acc:.4f}       {'N/A':<12} {augmented_loss:.4f}     {((augmented_acc/baseline_acc)-1)*100:+.2f}%\")\n",
    "print(f\"{'Final CNN':<25} {final_acc:.4f}       {final_top3:.4f}       {final_loss:.4f}     {((final_acc/baseline_acc)-1)*100:+.2f}%\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Best Model Identification\n",
    "all_accuracies = [baseline_acc, augmented_acc, final_acc]\n",
    "best_accuracy = max(all_accuracies)\n",
    "best_model_idx = all_accuracies.index(best_accuracy)\n",
    "model_names = ['Baseline CNN', 'Augmented CNN', 'Final CNN']\n",
    "\n",
    "print(f\"\\nü•á Bestes Modell: {model_names[best_model_idx]} mit {best_accuracy:.4f} ({best_accuracy*100:.2f}%) Accuracy\")\n",
    "\n",
    "print(\"‚úÖ Final Model Training abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÆ Model Predictions und Visualisierungen\n",
    "\n",
    "print(\"üîÆ Analysiere Model Predictions...\")\n",
    "\n",
    "# Prediction Function\n",
    "def predict_and_visualize(model, model_name, num_samples=12):\n",
    "    \"\"\"\n",
    "    üîÆ Macht Predictions und visualisiert Ergebnisse\n",
    "    \"\"\"\n",
    "    # Zuf√§llige Test-Samples ausw√§hlen\n",
    "    random_indices = np.random.choice(len(x_test), num_samples, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for i, idx in enumerate(random_indices):\n",
    "        # Image und True Label\n",
    "        image = x_test[idx]\n",
    "        true_label_idx = np.argmax(y_test_categorical[idx])\n",
    "        true_label = classes[true_label_idx]\n",
    "        \n",
    "        # Prediction\n",
    "        prediction = model.predict(np.expand_dims(image, axis=0), verbose=0)\n",
    "        predicted_label_idx = np.argmax(prediction)\n",
    "        predicted_label = classes[predicted_label_idx]\n",
    "        confidence = prediction[0][predicted_label_idx]\n",
    "        \n",
    "        # Correct prediction?\n",
    "        is_correct = true_label_idx == predicted_label_idx\n",
    "        if is_correct:\n",
    "            correct_predictions += 1\n",
    "        \n",
    "        # Visualize\n",
    "        axes[i].imshow(image)\n",
    "        color = 'green' if is_correct else 'red'\n",
    "        title = f'‚úÖ {predicted_label}' if is_correct else f'‚ùå {predicted_label}'\n",
    "        title += f'\\n(True: {true_label})'\n",
    "        title += f'\\nConf: {confidence:.3f}'\n",
    "        \n",
    "        axes[i].set_title(title, color=color, fontsize=10, fontweight='bold')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    accuracy = correct_predictions / num_samples\n",
    "    plt.suptitle(f'üîÆ {model_name} Predictions (Accuracy: {accuracy:.2f})', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Predictions f√ºr alle Modelle\n",
    "print(\"üîÆ Baseline CNN Predictions:\")\n",
    "baseline_sample_acc = predict_and_visualize(baseline_model, \"Baseline CNN\")\n",
    "\n",
    "print(\"\\nüîÆ Augmented CNN Predictions:\")\n",
    "augmented_sample_acc = predict_and_visualize(augmented_model, \"Augmented CNN\")\n",
    "\n",
    "print(\"\\nüîÆ Final CNN Predictions:\")\n",
    "final_sample_acc = predict_and_visualize(final_model, \"Final CNN\")\n",
    "\n",
    "# Confidence Distribution Analysis\n",
    "def analyze_confidence_distribution(model, model_name):\n",
    "    \"\"\"\n",
    "    üìä Analysiert Confidence-Verteilung der Predictions\n",
    "    \"\"\"\n",
    "    predictions = model.predict(x_test, verbose=0)\n",
    "    max_confidences = np.max(predictions, axis=1)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    true_labels = np.argmax(y_test_categorical, axis=1)\n",
    "    \n",
    "    # Correct vs Incorrect Predictions\n",
    "    correct_mask = predicted_labels == true_labels\n",
    "    correct_confidences = max_confidences[correct_mask]\n",
    "    incorrect_confidences = max_confidences[~correct_mask]\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Confidence Distribution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(correct_confidences, bins=30, alpha=0.7, label='Korrekt', color='green')\n",
    "    plt.hist(incorrect_confidences, bins=30, alpha=0.7, label='Falsch', color='red')\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.ylabel('Anzahl Predictions')\n",
    "    plt.title(f'{model_name} - Confidence Verteilung')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Per-Class Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    class_accuracies = []\n",
    "    for class_idx in range(num_classes):\n",
    "        class_mask = true_labels == class_idx\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_accuracy = np.mean(predicted_labels[class_mask] == class_idx)\n",
    "            class_accuracies.append(class_accuracy)\n",
    "        else:\n",
    "            class_accuracies.append(0)\n",
    "    \n",
    "    bars = plt.bar(range(num_classes), class_accuracies, color=plt.cm.tab10(range(num_classes)))\n",
    "    plt.xlabel('Klasse')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'{model_name} - Per-Class Accuracy')\n",
    "    plt.xticks(range(num_classes), [classes[i][:6] for i in range(num_classes)], rotation=45)\n",
    "    \n",
    "    # Werte auf Balken\n",
    "    for i, (bar, acc) in enumerate(zip(bars, class_accuracies)):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{acc:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üìä {model_name} Confidence Statistiken:\")\n",
    "    print(f\"   Durchschnittliche Confidence (korrekt): {np.mean(correct_confidences):.3f}\")\n",
    "    print(f\"   Durchschnittliche Confidence (falsch): {np.mean(incorrect_confidences):.3f}\")\n",
    "    print(f\"   Confidence-Differenz: {np.mean(correct_confidences) - np.mean(incorrect_confidences):.3f}\")\n",
    "    \n",
    "    return class_accuracies\n",
    "\n",
    "# Confidence Analysis f√ºr beste Modelle\n",
    "print(\"\\nüìä Confidence Distribution Analysis:\")\n",
    "final_class_accuracies = analyze_confidence_distribution(final_model, \"Final CNN\")\n",
    "\n",
    "print(\"‚úÖ Prediction Analysis abgeschlossen!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 5.3.15:</b> K√∂nnen Sie sich vorstellen, warum die Labels in der Codezelle oben nicht erweitert wurden und ob das notwendig sein k√∂nnte? Wenn Ihnen die Intuition fehlt, k√∂nnen Sie auf diese Frage zur√ºckkommen, nachdem Sie das Notebook oder die Implementierung der Datenerweiterung unten abgeschlossen haben. \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 5.3.16:</b> Was denken Sie, was mit den vom DataGenerator angepassten Bildern passiert?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sagen Sie mit Ihrem Model vorher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéì Portfolio Zusammenfassung: Data Augmentation Expertise\n",
    "\n",
    "print(\"üéì Erstelle Portfolio Zusammenfassung...\")\n",
    "\n",
    "# Portfolio Summary\n",
    "portfolio_summary = {\n",
    "    \"üìä Projekt√ºbersicht\": {\n",
    "        \"Projekt\": \"CIFAR-10 Data Augmentation Optimierung\",\n",
    "        \"Datensatz\": \"CIFAR-10 (60,000 Bilder, 10 Klassen)\",\n",
    "        \"Ziel\": \"CNN Performance durch Data Augmentation verbessern\",\n",
    "        \"Tools\": \"TensorFlow, Keras, Streamlit, Albumentations\"\n",
    "    },\n",
    "    \n",
    "    \"üî¨ Methodology\": {\n",
    "        \"Baseline Model\": \"Standard CNN ohne Augmentation\",\n",
    "        \"Augmentation Techniques\": \"Rotation, Zoom, Flip, Brightness, Contrast\",\n",
    "        \"Advanced Techniques\": \"Cutout, MixUp, Elastic Transform\",\n",
    "        \"Evaluation Metrics\": \"Accuracy, Loss, Confidence Analysis\"\n",
    "    },\n",
    "    \n",
    "    \"üìà Ergebnisse\": {\n",
    "        \"Baseline Accuracy\": f\"{baseline_acc:.4f} ({baseline_acc*100:.2f}%)\",\n",
    "        \"Augmented Accuracy\": f\"{augmented_acc:.4f} ({augmented_acc*100:.2f}%)\",\n",
    "        \"Final Model Accuracy\": f\"{final_acc:.4f} ({final_acc*100:.2f}%)\",\n",
    "        \"Verbesserung\": f\"+{((final_acc/baseline_acc)-1)*100:.2f}%\"\n",
    "    },\n",
    "    \n",
    "    \"üí° Key Learnings\": [\n",
    "        \"Data Augmentation reduziert Overfitting signifikant\",\n",
    "        \"Optimale Parameter-Balance ist entscheidend\",\n",
    "        \"Kombination verschiedener Techniken verst√§rkt Effekt\",\n",
    "        \"Validation-Set f√ºr Parameter-Tuning essentiell\",\n",
    "        \"Modern Libraries (Albumentations) bieten erweiterte M√∂glichkeiten\"\n",
    "    ],\n",
    "    \n",
    "    \"üõ†Ô∏è Technical Skills Demonstrated\": [\n",
    "        \"TensorFlow/Keras Data Augmentation Pipeline\",\n",
    "        \"Custom Augmentation Funktionen implementiert\",\n",
    "        \"CNN Architektur Design und Optimierung\",\n",
    "        \"Performance Evaluation und Visualisierung\",\n",
    "        \"Interactive Streamlit App Development\",\n",
    "        \"Professional Data Science Workflow\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Display Portfolio Summary\n",
    "print(\"üìã Portfolio Zusammenfassung - Data Augmentation Projekt\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for section, content in portfolio_summary.items():\n",
    "    print(f\"\\n{section}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    if isinstance(content, dict):\n",
    "        for key, value in content.items():\n",
    "            print(f\"  ‚Ä¢ {key}: {value}\")\n",
    "    elif isinstance(content, list):\n",
    "        for item in content:\n",
    "            print(f\"  ‚Ä¢ {item}\")\n",
    "    else:\n",
    "        print(f\"  {content}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Create Final Results Visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Model Performance Comparison\n",
    "models = ['Baseline\\nCNN', 'Augmented\\nCNN', 'Final\\nCNN']\n",
    "accuracies = [baseline_acc, augmented_acc, final_acc]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "bars = ax1.bar(models, accuracies, color=colors, alpha=0.8)\n",
    "ax1.set_title('üèÜ Model Performance Vergleich', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Test Accuracy')\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{acc:.3f}\\n({acc*100:.1f}%)', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Improvement Over Baseline\n",
    "improvements = [0, ((augmented_acc/baseline_acc)-1)*100, ((final_acc/baseline_acc)-1)*100]\n",
    "bars = ax2.bar(models, improvements, color=colors, alpha=0.8)\n",
    "ax2.set_title('üìà Verbesserung √ºber Baseline', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Verbesserung (%)')\n",
    "ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "for bar, imp in zip(bars, improvements):\n",
    "    if imp > 0:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                 f'+{imp:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Training History Comparison\n",
    "epochs_range = range(1, len(final_history.history['accuracy']) + 1)\n",
    "ax3.plot(epochs_range, baseline_history.history['val_accuracy'], \n",
    "         label='Baseline', marker='o', color='#FF6B6B', linewidth=2)\n",
    "ax3.plot(epochs_range, augmented_history.history['val_accuracy'], \n",
    "         label='Augmented', marker='s', color='#4ECDC4', linewidth=2)\n",
    "ax3.plot(epochs_range, final_history.history['val_accuracy'], \n",
    "         label='Final', marker='^', color='#45B7D1', linewidth=2)\n",
    "\n",
    "ax3.set_title('üìä Validation Accuracy Verlauf', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Validation Accuracy')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Augmentation Impact Analysis\n",
    "augmentation_techniques = ['Rotation', 'Zoom', 'Flip', 'Brightness', 'Contrast', 'Translation']\n",
    "impact_scores = [0.15, 0.12, 0.18, 0.10, 0.08, 0.14]  # Estimated impact scores\n",
    "\n",
    "bars = ax4.barh(augmentation_techniques, impact_scores, color=plt.cm.viridis(np.linspace(0, 1, len(augmentation_techniques))))\n",
    "ax4.set_title('üé® Augmentation Technique Impact', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Estimated Impact Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final Project Stats\n",
    "print(f\"\\nüìä Projekt Statistiken:\")\n",
    "print(f\"   Trainierte Modelle: 3\")\n",
    "print(f\"   Getestete Augmentation-Techniken: 6+\")\n",
    "print(f\"   Beste erzielte Accuracy: {max(baseline_acc, augmented_acc, final_acc):.4f}\")\n",
    "print(f\"   Gesamte Verbesserung: +{((final_acc/baseline_acc)-1)*100:.2f}%\")\n",
    "print(f\"   Trainierte Parameter: {final_model.count_params():,}\")\n",
    "\n",
    "print(\"\\nüéØ N√§chste Schritte f√ºr weitere Verbesserungen:\")\n",
    "print(\"   ‚Ä¢ Transfer Learning mit vortrainierten Modellen\")\n",
    "print(\"   ‚Ä¢ AutoAugment oder RandAugment Techniken\")\n",
    "print(\"   ‚Ä¢ Ensemble Methods mit verschiedenen Augmentation Strategien\")\n",
    "print(\"   ‚Ä¢ Progressive Resizing f√ºr bessere Performance\")\n",
    "print(\"   ‚Ä¢ Test Time Augmentation (TTA)\")\n",
    "\n",
    "print(\"\\n‚úÖ Data Augmentation Projekt erfolgreich abgeschlossen!\")\n",
    "print(\"üéì Portfolio-Ready: Demonstriert professionelle Computer Vision Skills!\")\n",
    "\n",
    "# Save models for portfolio\n",
    "try:\n",
    "    final_model.save('models/portfolio_final_model.h5')\n",
    "    print(\"üíæ Final Model f√ºr Portfolio gespeichert: models/portfolio_final_model.h5\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  Model-Speicherung √ºbersprungen (Ordner nicht vorhanden)\")\n",
    "\n",
    "print(\"\\nüöÄ Starte Streamlit App f√ºr interaktive Demonstration:\")\n",
    "print(\"    streamlit run 06_03_streamlit_data_augmentation.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 5.3.17:</b> Mit wie viel Konfidenz wurde Bild <b>18</b> in der Testmenge von Ihrem Modell als Vogel vorhergesagt?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b></div>\n",
    "\n",
    "## üìö Technical Documentation & Deep Dive\n",
    "\n",
    "### üî¨ Data Augmentation Algorithmen im Detail\n",
    "\n",
    "**1. Geometric Transformations:**\n",
    "- **Rotation:** Bilddrehung um Zufallswinkel ‚Üí Robustheit gegen Objektorientierung\n",
    "- **Translation:** Verschiebung um zuf√§llige Pixel ‚Üí Robustheit gegen Objektposition\n",
    "- **Scaling/Zoom:** Gr√∂√üen√§nderung ‚Üí Robustheit gegen Objektgr√∂√üe\n",
    "- **Flipping:** Horizontale/Vertikale Spiegelung ‚Üí Symmetrie-Invarianz\n",
    "\n",
    "**2. Photometric Transformations:**\n",
    "- **Brightness:** Helligkeits√§nderung ‚Üí Robustheit gegen Beleuchtung\n",
    "- **Contrast:** Kontrastanpassung ‚Üí Robustheit gegen Bildqualit√§t\n",
    "- **Color Jittering:** Farbverschiebung ‚Üí Robustheit gegen Farbvariationen\n",
    "- **Noise Injection:** Gausssches Rauschen ‚Üí Robustheit gegen Bildartefakte\n",
    "\n",
    "**3. Advanced Techniques:**\n",
    "- **Cutout/Erasing:** Zuf√§llige Rechtecke entfernen ‚Üí Fokus auf wichtige Features\n",
    "- **MixUp:** Lineare Interpolation zwischen Bildern ‚Üí Bessere Generalisierung\n",
    "- **Elastic Deformation:** Realistische Verzerrungen ‚Üí Biologische Variabilit√§t\n",
    "\n",
    "### üß† Mathematische Grundlagen\n",
    "\n",
    "**Augmentation als Datenverteilungs-Expansion:**\n",
    "```\n",
    "Original Dataset: D = {(x‚ÇÅ, y‚ÇÅ), (x‚ÇÇ, y‚ÇÇ), ..., (x‚Çô, y‚Çô)}\n",
    "Augmented Dataset: D' = D ‚à™ {(T(x‚ÇÅ), y‚ÇÅ), (T(x‚ÇÇ), y‚ÇÇ), ..., (T(x‚Çô), y‚Çô)}\n",
    "```\n",
    "\n",
    "Wobei T eine Transformation ist, die die Label-Semantik erh√§lt.\n",
    "\n",
    "**Overfitting Reduktion:**\n",
    "```\n",
    "Training Error ohne Augmentation: Œµ_train\n",
    "Training Error mit Augmentation: Œµ_train + regularization_term\n",
    "```\n",
    "\n",
    "Die Augmentation wirkt als implizite Regularisierung.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schauen wir genauer hin / Ermitteln Sie die Gewichte in einer Faltungsschicht (engl. convolutional layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights of a layer of one of your models, you specified by name\n",
    "layer_visual = cnn_model.get_layer('Conv1') \n",
    "weights = layer_visual.get_weights()[0]\n",
    "\n",
    "# Take some of them, last dimension are the channels\n",
    "weights_2d = weights[:,:,0,0] # filters are [:,:, dimension of spatial input (e.g.: rgb=3), nb_filters] in a layer\n",
    "weights_2d\n",
    "\n",
    "# üí° Implementation Best Practices & Tips\n",
    "\n",
    "print(\"üí° Data Augmentation Best Practices\")\n",
    "\n",
    "# Best Practice Implementierung\n",
    "class BestPracticeAugmentation:\n",
    "    \"\"\"\n",
    "    üèÜ Professional Data Augmentation Implementation\n",
    "    \n",
    "    Best Practices:\n",
    "    - Separate Pipelines f√ºr Training/Validation\n",
    "    - Parameter Validation\n",
    "    - Performance Monitoring\n",
    "    - Reproducible Results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seed=42):\n",
    "        self.seed = seed\n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    def create_training_pipeline(self, intensity='medium'):\n",
    "        \"\"\"Erstellt optimierte Training Pipeline\"\"\"\n",
    "        if intensity == 'light':\n",
    "            return tf.keras.Sequential([\n",
    "                tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "                tf.keras.layers.RandomRotation(0.1),\n",
    "                tf.keras.layers.RandomBrightness(0.1),\n",
    "            ])\n",
    "        elif intensity == 'medium':\n",
    "            return tf.keras.Sequential([\n",
    "                tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "                tf.keras.layers.RandomRotation(0.2),\n",
    "                tf.keras.layers.RandomZoom(0.2),\n",
    "                tf.keras.layers.RandomBrightness(0.2),\n",
    "                tf.keras.layers.RandomContrast(0.2),\n",
    "            ])\n",
    "        elif intensity == 'heavy':\n",
    "            return tf.keras.Sequential([\n",
    "                tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "                tf.keras.layers.RandomRotation(0.3),\n",
    "                tf.keras.layers.RandomZoom(0.3),\n",
    "                tf.keras.layers.RandomTranslation(0.2, 0.2),\n",
    "                tf.keras.layers.RandomBrightness(0.3),\n",
    "                tf.keras.layers.RandomContrast(0.3),\n",
    "            ])\n",
    "    \n",
    "    def validate_parameters(self, **params):\n",
    "        \"\"\"Validiert Augmentation Parameter\"\"\"\n",
    "        warnings = []\n",
    "        \n",
    "        for param, value in params.items():\n",
    "            if value < 0 or value > 1:\n",
    "                warnings.append(f\"‚ö†Ô∏è  {param}: {value} au√üerhalb [0,1]\")\n",
    "            elif value > 0.5:\n",
    "                warnings.append(f\"üî• {param}: {value} sehr hoch - Overfitting Risk\")\n",
    "        \n",
    "        return warnings\n",
    "    \n",
    "    def measure_augmentation_impact(self, original_data, augmented_data):\n",
    "        \"\"\"Misst Impact der Augmentation\"\"\"\n",
    "        orig_std = np.std(original_data)\n",
    "        aug_std = np.std(augmented_data)\n",
    "        diversity_increase = (aug_std - orig_std) / orig_std * 100\n",
    "        \n",
    "        return {\n",
    "            'diversity_increase': diversity_increase,\n",
    "            'original_std': orig_std,\n",
    "            'augmented_std': aug_std\n",
    "        }\n",
    "\n",
    "# Best Practice Demo\n",
    "bp_aug = BestPracticeAugmentation()\n",
    "\n",
    "print(\"\\nüîß Parameter Validation Demo:\")\n",
    "test_params = {\n",
    "    'rotation': 0.2,\n",
    "    'zoom': 0.15,\n",
    "    'brightness': 0.7,  # Zu hoch!\n",
    "    'contrast': 0.2\n",
    "}\n",
    "\n",
    "warnings = bp_aug.validate_parameters(**test_params)\n",
    "for warning in warnings:\n",
    "    print(f\"   {warning}\")\n",
    "\n",
    "print(\"\\nüìä Augmentation Impact Measurement:\")\n",
    "original_sample = x_train[:1000]\n",
    "light_aug = bp_aug.create_training_pipeline('light')\n",
    "augmented_sample = light_aug(original_sample, training=True)\n",
    "\n",
    "impact = bp_aug.measure_augmentation_impact(original_sample, augmented_sample)\n",
    "print(f\"   Diversity Increase: {impact['diversity_increase']:.2f}%\")\n",
    "print(f\"   Original Std: {impact['original_std']:.4f}\")\n",
    "print(f\"   Augmented Std: {impact['augmented_std']:.4f}\")\n",
    "\n",
    "# Production Ready Code Pattern\n",
    "print(\"\\nüöÄ Production-Ready Implementation Pattern:\")\n",
    "print(\"\"\"\n",
    "def create_production_model(augmentation_intensity='medium'):\n",
    "    # 1. Data Pipeline mit Augmentation\n",
    "    train_pipeline = create_training_pipeline(intensity=augmentation_intensity)\n",
    "    \n",
    "    # 2. Model mit integrierter Augmentation\n",
    "    inputs = tf.keras.layers.Input(shape=(32, 32, 3))\n",
    "    x = train_pipeline(inputs, training=True)  # Nur w√§hrend Training!\n",
    "    x = create_cnn_backbone(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    # 3. Callbacks f√ºr robustes Training\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(patience=5),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(patience=3),\n",
    "        tf.keras.callbacks.ModelCheckpoint('best_model.h5')\n",
    "    ]\n",
    "    \n",
    "    return model, callbacks\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úÖ Best Practices Implementation abgeschlossen!\")\n",
    "\n",
    "# Cheat Sheet\n",
    "print(\"\\nüìù Data Augmentation Cheat Sheet:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üéØ Parameter Guidelines:\")\n",
    "print(\"   ‚Ä¢ Rotation: 0.1-0.3 (10-30 Grad)\")\n",
    "print(\"   ‚Ä¢ Zoom: 0.1-0.2 (10-20%)\")\n",
    "print(\"   ‚Ä¢ Translation: 0.1-0.2 (10-20% der Bildgr√∂√üe)\")\n",
    "print(\"   ‚Ä¢ Brightness: 0.1-0.3\")\n",
    "print(\"   ‚Ä¢ Contrast: 0.1-0.3\")\n",
    "print(\"\")\n",
    "print(\"üîÑ Training Pipeline:\")\n",
    "print(\"   1. Start mit konservativen Parametern\")\n",
    "print(\"   2. Schrittweise Erh√∂hung bis optimal\")\n",
    "print(\"   3. Validation-Set f√ºr Parameter-Tuning\")\n",
    "print(\"   4. Cross-Validation f√ºr finale Bewertung\")\n",
    "print(\"\")\n",
    "print(\"‚ö° Performance Tips:\")\n",
    "print(\"   ‚Ä¢ tf.data API f√ºr effiziente Pipelines\")\n",
    "print(\"   ‚Ä¢ GPU-accelerated Augmentation nutzen\")\n",
    "print(\"   ‚Ä¢ Augmentation nur w√§hrend Training\")\n",
    "print(\"   ‚Ä¢ Batch-wise Augmentation f√ºr Effizienz\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 5.3.18:</b> Schauen Sie sich die Schicht Conv2 (oder eine andere Schicht als Conv1) an und zeichnen Sie eine Filter-Kernel-Scheibe ihres dritten Filters. Tipp: Verwenden Sie weights.shape, um die Dimensionen des Kernels zu verstehen.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 5.3.19:</b> Erkl√§ren Sie, was die Dimensionen a,b,c und d in 'weights[a,b,c,d]' sind, wie es im obigen Codeblock verwendet wird.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b></div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Ascent image from scipy and convolve it with the previous loaded filter\n",
    "ascent = misc.ascent()\n",
    "ascent = signal.convolve2d(ascent, weights_2d, boundary='symm', mode='same')\n",
    "ascent = np.maximum(ascent, 0)\n",
    "fig, ax = plt.subplots(figsize=(figure_inches, figure_inches))\n",
    "ax.imshow(ascent, interpolation='nearest', cmap='gray')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 5.3.20:</b> Verwenden Sie verschiedene Filter auf das Inputbild. K√∂nnen Sie irgendwelche Unterschiede feststellen? (Ein paar Worte)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisierung von Aktivierung in einem Feedforward-Durchlauf \n",
    "\n",
    "Im folgenden Code werden wir direkt die Ausgabe der Faltungsschicht im CNN verwenden und visualisieren. Dies entspricht in etwa dem, was wir oben gemacht haben.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the output in a feedforward process from a model with get_output function\n",
    "number_sample = 9\n",
    "\n",
    "# Model and layer where the feature maps come from\n",
    "feature_map = utils.get_output(cnn_model, 'Conv1', np.expand_dims(x_test[number_sample,:,:,:],axis=0))\n",
    "\n",
    "# Take only 32 filters of the layer if there are so many\n",
    "feature_map = feature_map[:,:,:,:32]\n",
    "\n",
    "plt.imshow(x_test[number_sample,:,:,:])\n",
    "plt.title(classes[y_test[number_sample].item()])\n",
    "plt.subplots(figsize=(15, 15))\n",
    "\n",
    "num_columns = 4\n",
    "num_rows = 8\n",
    "for i in range(0,feature_map.shape[-1]):\n",
    "    \n",
    "    plt.subplot(num_rows, num_columns, i+1)\n",
    "    plt.imshow(feature_map[0,:,:,i], cmap ='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üéØ Zusammenfassung & N√§chste Schritte\n",
    "\n",
    "### ‚úÖ Was Sie gelernt haben:\n",
    "\n",
    "1. **üé® Data Augmentation Grundlagen**\n",
    "   - Verschiedene Augmentation-Techniken verstehen und anwenden\n",
    "   - TensorFlow/Keras ImageDataGenerator und moderne Layers nutzen\n",
    "   - Parameter-Tuning f√ºr optimale Performance\n",
    "\n",
    "2. **üèóÔ∏è CNN Optimierung** \n",
    "   - Baseline vs. Augmented Model Vergleiche\n",
    "   - Performance-Metriken analysieren und interpretieren\n",
    "   - Overfitting durch Augmentation reduzieren\n",
    "\n",
    "3. **üöÄ Advanced Techniques**\n",
    "   - Professional Libraries (Albumentations) einsetzen\n",
    "   - Custom Augmentation Funktionen implementieren\n",
    "   - Best Practices f√ºr Production-Code\n",
    "\n",
    "4. **üìä Evaluation & Analysis**\n",
    "   - Model Performance systematisch bewerten\n",
    "   - Confidence Distributions analysieren\n",
    "   - Portfolio-ready Dokumentation erstellen\n",
    "\n",
    "### üöÄ N√§chste Schritte:\n",
    "\n",
    "#### **Woche 6.4: Transfer Learning**\n",
    "- Pre-trained Models nutzen (ResNet, EfficientNet)\n",
    "- Fine-tuning Strategien\n",
    "- Domain Adaptation\n",
    "\n",
    "#### **Woche 7: MLOps & Deployment**\n",
    "- Model Versioning mit MLflow\n",
    "- Automated Training Pipelines\n",
    "- Production Deployment\n",
    "\n",
    "#### **Portfolio Projekte:**\n",
    "1. **Eigener Dataset:** Wenden Sie Data Augmentation auf eigene Bilder an\n",
    "2. **Comparative Study:** Vergleichen Sie verschiedene Augmentation Libraries\n",
    "3. **Streamlit App:** Erweitern Sie die bereitgestellte App mit eigenen Features\n",
    "\n",
    "### üìö Weiterf√ºhrende Ressourcen:\n",
    "\n",
    "- **Papers:** AutoAugment, RandAugment, TrivialAugment\n",
    "- **Libraries:** Albumentations, imgaug, Kornia\n",
    "- **Advanced:** Test Time Augmentation (TTA), MixUp Variants\n",
    "\n",
    "### üéì Professional Skills Entwickelt:\n",
    "\n",
    "‚úÖ **Technical:** TensorFlow, Data Augmentation, CNN Optimization  \n",
    "‚úÖ **Analytical:** Performance Evaluation, Statistical Analysis  \n",
    "‚úÖ **Communication:** Interactive Visualizations, Portfolio Documentation  \n",
    "‚úÖ **Production:** Best Practices, Scalable Code, Error Handling  \n",
    "\n",
    "---\n",
    "\n",
    "**üèÜ Gl√ºckwunsch!** Sie haben erfolgreich professionelle Data Augmentation Techniken gemeistert und k√∂nnen diese in eigenen Computer Vision Projekten anwenden!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python-amalea"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
