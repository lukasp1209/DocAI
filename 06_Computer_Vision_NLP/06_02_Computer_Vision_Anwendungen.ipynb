{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Nur Colab] Diese Zellen mÃ¼ssen nur auf *Google Colab* ausgefÃ¼hrt werden und installieren Packete und Daten\n",
    "!wget -q https://raw.githubusercontent.com/KI-Campus/AMALEA/master/requirements.txt && pip install --quiet -r requirements.txt\n",
    "!wget --quiet \"https://github.com/KI-Campus/AMALEA/releases/download/data/data.zip\" && unzip -q data.zip\n",
    "\n",
    "# ğŸ”§ Setup: Computer Vision Libraries\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Standard Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "# Computer Vision  \n",
    "import cv2\n",
    "from scipy import signal, ndimage\n",
    "import skimage\n",
    "from skimage import feature, filters, segmentation, measure\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import applications, layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Interactive Widgets\n",
    "from ipywidgets import interact, interactive, fixed, widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Streamlit (fÃ¼r Apps)\n",
    "import streamlit as st\n",
    "\n",
    "# Konfiguration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "figure_inches = 10\n",
    "\n",
    "print(\"âœ… Computer Vision Libraries geladen!\")\n",
    "print(f\"ğŸ“Š TensorFlow: {tf.__version__}\")\n",
    "print(f\"ğŸ–¼ï¸ OpenCV: {cv2.__version__}\")\n",
    "print(f\"ğŸ”¬ Scikit-Image: {skimage.__version__}\")\n",
    "\n",
    "# GPU Check\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"ğŸš€ GPU verfÃ¼gbar fÃ¼r Deep Learning!\")\n",
    "else:\n",
    "    print(\"ğŸ’» CPU wird verwendet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ–¼ï¸ 06.2 Computer Vision Anwendungen - Von Theorie zur Praxis\n",
    "\n",
    "**Data Analytics & Big Data - Woche 6.2**  \n",
    "*IU Internationale Hochschule*\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Lernziele\n",
    "\n",
    "Nach diesem Notebook kÃ¶nnen Sie:\n",
    "- âœ… **Faltungsoperationen** mathematisch verstehen und implementieren\n",
    "- âœ… **Verschiedene Filter** (Roberts, Sobel, etc.) anwenden  \n",
    "- âœ… **Praktische CV-Anwendungen** entwickeln (Objekterkennung, Segmentierung)\n",
    "- âœ… **OpenCV** fÃ¼r professionelle Computer Vision nutzen\n",
    "- âœ… **Streamlit-App** fÃ¼r Bildklassifikation erstellen\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§® Faltungsoperationen verstehen\n",
    "\n",
    "**Convolution** ist das HerzstÃ¼ck von Computer Vision! Lassen Sie uns verstehen, wie es funktioniert:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§® Faltungsoperation (Convolution) verstehen\n",
    "\n",
    "### ğŸ¤” Was ist Convolution?\n",
    "\n",
    "**Convolution** ist eine mathematische Operation, die zwei Funktionen miteinander \"faltet\". In Computer Vision:\n",
    "- **Bild** (Input) âŠ› **Filter** (Kernel) = **Feature Map** (Output)\n",
    "\n",
    "### ğŸ“ Mathematische Definition\n",
    "\n",
    "**Kontinuierlich:** \n",
    "$$x(t) \\ast y(t) = \\int_{-\\infty}^{+\\infty} x(t -\\tau) y(\\tau) d\\tau$$\n",
    "\n",
    "**Diskret (fÃ¼r Pixel):**\n",
    "$$x_n \\ast y_n = \\sum_{i = -\\infty}^{\\infty} x_i \\cdot y_{n-i}$$\n",
    "\n",
    "### ğŸ¯ Warum ist das wichtig?\n",
    "\n",
    "1. **Feature Detection:** Filter erkennen Muster (Kanten, Texturen, etc.)\n",
    "2. **Parameter Sharing:** Ein Filter wird Ã¼ber das ganze Bild angewendet  \n",
    "3. **Translation Invariance:** Objekte werden Ã¼berall im Bild erkannt\n",
    "\n",
    "### ğŸ® Interaktive Convolution Demonstration\n",
    "\n",
    "Verstehen Sie Convolution durch interaktive Rechteck-Funktionen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "figure_inches = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_demo(tau: float, width1: float, width2: float):\n",
    "    \"\"\"\n",
    "    ğŸ“Š Interaktive Convolution-Demonstration\n",
    "    \n",
    "    Parameter:\n",
    "    - tau: Verschiebung der zweiten Funktion\n",
    "    - width1: Breite der ersten Rechteck-Funktion\n",
    "    - width2: Breite der zweiten Rechteck-Funktion\n",
    "    \"\"\"\n",
    "    # Definiere x-Achse\n",
    "    x1 = np.linspace(-3.5, 3.5, num=1000)\n",
    "    dX = x1[1] - x1[0]    \n",
    "    \n",
    "    # Erstelle Rechteck-Funktionen\n",
    "    rect1 = np.where(abs(x1) <= width1/2, 1, 0)  # rectâ‚(t)\n",
    "    rect2 = np.where(abs(x1 - tau) <= width2/2, 1, 0)  # rectâ‚‚(t-Ï„)\n",
    "    \n",
    "    # Convolution berechnen\n",
    "    conv = np.convolve(rect1, rect2, 'same') * dX \n",
    "    \n",
    "    # Visualisierung\n",
    "    plt.figure(figsize=(16.5, 4))\n",
    "    \n",
    "    # Plots\n",
    "    plt.plot(x1, rect1, 'b', linewidth=2, label='ğŸ“Š rectâ‚(t)')\n",
    "    plt.plot(x1, rect2, 'r', linewidth=2, label='ğŸ“Š rectâ‚‚(t-Ï„)')\n",
    "    \n",
    "    # Convolution bis zu aktueller Position\n",
    "    x_gr = x1 - tau\n",
    "    if tau <= 0:\n",
    "        index = np.where((np.absolute(x_gr) - np.absolute(tau)) <= 0.004)\n",
    "        index = index[0][0] if len(index[0]) > 0 else 0\n",
    "    else:\n",
    "        index = np.where(np.absolute(x_gr - tau) <= 0.004)\n",
    "        index = index[0][0] if len(index[0]) > 0 else 999\n",
    "        \n",
    "    plt.plot(x_gr[:index], conv[:index], 'g', linewidth=3, \n",
    "             label='ğŸ¯ rectâ‚ âŠ› rectâ‚‚ (Convolution)')\n",
    "    \n",
    "    # Vertikale Linie bei aktueller Position\n",
    "    plt.axvline(x=tau, color='r', linestyle='--', alpha=0.7, linewidth=2)\n",
    "    \n",
    "    # Styling\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., prop={'size': 13})\n",
    "    plt.ylim(0, np.maximum(np.max(conv), np.max(rect1)) + 0.1)\n",
    "    plt.xlim(-2.5, 2.5)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.title(f'ğŸ® Convolution Demo: Ï„ = {tau:.2f}', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Zeit t', fontsize=12)\n",
    "    plt.ylabel('Amplitude', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Info-Text\n",
    "    overlap_area = np.trapz(conv[:index], x_gr[:index]) if index > 0 else 0\n",
    "    print(f\"ğŸ“Š Convolution Wert bei Ï„={tau:.2f}: {overlap_area:.3f}\")\n",
    "    print(f\"ğŸ’¡ Interpretation: Ãœberlappungsbereich der beiden Rechtecke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ® Interaktive Convolution Demo\n",
    "\n",
    "print(\"ğŸ¯ Convolution verstehen durch interaktive Rechteck-Funktionen:\")\n",
    "print(\"â€¢ Bewegen Sie Ï„ (tau) um die zweite Funktion zu verschieben\")\n",
    "print(\"â€¢ Ã„ndern Sie width1/width2 um die Rechteck-Breiten anzupassen\")\n",
    "print(\"â€¢ Beobachten Sie, wie sich die Convolution (grÃ¼n) entwickelt\")\n",
    "\n",
    "# Widget fÃ¼r interaktive Convolution\n",
    "interactive_plot = interactive(\n",
    "    convolution_demo, \n",
    "    tau=widgets.FloatSlider(\n",
    "        value=0, min=-2.0, max=2.0, step=0.1,\n",
    "        description='Ï„ (Verschiebung):',\n",
    "        style={'description_width': 'initial'}\n",
    "    ),\n",
    "    width1=widgets.FloatSlider(\n",
    "        value=1.0, min=0.25, max=1.75, step=0.25,\n",
    "        description='Width 1:',\n",
    "        style={'description_width': 'initial'}  \n",
    "    ),\n",
    "    width2=widgets.FloatSlider(\n",
    "        value=1.0, min=0.25, max=1.75, step=0.25,\n",
    "        description='Width 2:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    ")\n",
    "\n",
    "# Widget anzeigen\n",
    "display(interactive_plot)\n",
    "\n",
    "print(\"\\nğŸ’¡ Key Insight: Convolution misst die 'Ã„hnlichkeit' zwischen zwei Funktionen!\")\n",
    "print(\"ğŸ”— In CNNs: Bild âŠ› Filter = Feature Map (zeigt wo Features gefunden wurden)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NatÃ¼rlich erfolgt die Berechnung der Faltung nicht im kontinuierlichen Bereich. Daher verwendet numpy die folgende Formel fÃ¼r diskrete Faltung im 1-dimensionalen Raum:\n",
    "\n",
    "\\begin{equation*}\n",
    "x_n \\ast y_n = \\sum_{i = -\\infty}^{\\infty} x_i  \\  y_{n-i} = \\sum_{i = -\\infty}^{\\infty} y_{i}  \\ x_{n-i}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ–¼ï¸ 2D-Convolution fÃ¼r Bilder\n",
    "\n",
    "### ğŸ“ Von 1D zu 2D\n",
    "\n",
    "**2D-Convolution** fÃ¼r Bilder:\n",
    "$$x_{mn} \\star y_{mn} = \\sum_{i} \\sum_{j} x_{ij} \\cdot y_{m-i, n-j}$$\n",
    "\n",
    "### ğŸ”§ Praktische Parameter\n",
    "\n",
    "**Bildformat:** Height Ã— Width Ã— Channels (H Ã— W Ã— C)\n",
    "- **Graustufenbild:** 28 Ã— 28 Ã— 1  \n",
    "- **RGB-Bild:** 224 Ã— 224 Ã— 3\n",
    "\n",
    "**Filter-Parameter:**\n",
    "- **Kernel Size (K):** z.B. 3Ã—3, 5Ã—5  \n",
    "- **Stride (S):** Schrittweite (meist 1 oder 2)\n",
    "- **Padding (P):** Randbehandlung (SAME, VALID)\n",
    "\n",
    "### ğŸ“ AusgabegrÃ¶ÃŸe berechnen\n",
    "\n",
    "$$W_{out} = \\frac{W_{in} - K + 2P}{S} + 1$$\n",
    "$$H_{out} = \\frac{H_{in} - K + 2P}{S} + 1$$\n",
    "\n",
    "### ğŸ¯ Beispiel-Berechnung\n",
    "\n",
    "```python\n",
    "# Input: 28Ã—28Ã—1 (MNIST)\n",
    "# Filter: 3Ã—3, Stride=1, Padding=0\n",
    "W_out = (28 - 3 + 0) / 1 + 1 = 26\n",
    "H_out = (28 - 3 + 0) / 1 + 1 = 26  \n",
    "# Output: 26Ã—26Ã—1\n",
    "```\n",
    "\n",
    "### ğŸ” Convolution Visualisierung\n",
    "\n",
    "Die Animation zeigt, wie ein 3Ã—3 Filter Ã¼ber ein Bild \"gleitet\":\n",
    "\n",
    "![Convolution Animation](images/Faltung1.png)\n",
    "\n",
    "**Schritt-fÃ¼r-Schritt:**\n",
    "1. **Filter positionieren** auf Bildbereich\n",
    "2. **Element-wise Multiplikation** zwischen Filter und Pixeln  \n",
    "3. **Summe bilden** â†’ ein Output-Pixel\n",
    "4. **Filter verschieben** (Stride) und wiederholen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 5.2.1:</b> \n",
    "Implementieren Sie die Funktion <code>conv</code> welche ein gegebenes Bild <code>image_data</code> mit einem gegebenenen Filter <code>filter_kern</code> filtert. Nehmen Sie an:\n",
    "\n",
    "* Das Bild liegt entsprechend dem Beispiel (in der folgenden Zelle) als eine Liste von Listen vor\n",
    "* Die Tiefe des Bildes ist 1\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# ğŸ”§ Convolution-Funktion implementieren\n",
    "\n",
    "def conv(image_data: List[List[int]], filter_kern: List[List[int]]) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    ğŸ¯ 2D-Convolution Implementation (Educational)\n",
    "    \n",
    "    Parameter:\n",
    "    - image_data: Bild als 2D-Liste [[pixel, pixel, ...], [row2, ...], ...]\n",
    "    - filter_kern: Filter als 2D-Liste [[w1, w2], [w3, w4], ...]\n",
    "    \n",
    "    Returns:\n",
    "    - Gefilterte Bilddata als 2D-Liste\n",
    "    \n",
    "    ğŸ’¡ Hinweis: Das ist eine vereinfachte Educational-Version.\n",
    "       In der Praxis nutzen wir OpenCV oder TensorFlow!\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dimensionen ermitteln\n",
    "    image_height = len(image_data)\n",
    "    image_width = len(image_data[0])\n",
    "    filter_height = len(filter_kern)\n",
    "    filter_width = len(filter_kern[0])\n",
    "    \n",
    "    # Output-Dimensionen berechnen (ohne Padding)\n",
    "    output_height = image_height - filter_height + 1\n",
    "    output_width = image_width - filter_width + 1\n",
    "    \n",
    "    # Initialisiere Output\n",
    "    result = []\n",
    "    \n",
    "    # Convolution durchfÃ¼hren\n",
    "    for i in range(output_height):\n",
    "        row = []\n",
    "        for j in range(output_width):\n",
    "            # Berechne Convolution fÃ¼r aktuelle Position\n",
    "            conv_sum = 0\n",
    "            for fi in range(filter_height):\n",
    "                for fj in range(filter_width):\n",
    "                    # Element-wise Multiplikation und Summation\n",
    "                    pixel_value = image_data[i + fi][j + fj]\n",
    "                    filter_value = filter_kern[fi][fj]\n",
    "                    conv_sum += pixel_value * filter_value\n",
    "            \n",
    "            row.append(conv_sum)\n",
    "        result.append(row)\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"âœ… Convolution-Funktion implementiert!\")\n",
    "print(\"\\nğŸ“š Lernziel: Verstehen Sie, wie Convolution 'unter der Haube' funktioniert\")\n",
    "print(\"ğŸš€ In der Praxis: Nutzen Sie cv2.filter2D() oder TensorFlow fÃ¼r Performance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª Test der Convolution-Implementierung\n",
    "\n",
    "print(\"ğŸ§ª Test-Case: Convolution mit Edge-Detection Filter\")\n",
    "\n",
    "# Test-Daten\n",
    "test_input_data = [\n",
    "    [0, 0, 0, 0, 0], \n",
    "    [0, 1, 1, 1, 0], \n",
    "    [0, 0, 2, 0, 0], \n",
    "    [0, 3, 3, 3, 0], \n",
    "    [0, 0, 0, 0, 0], \n",
    "    [0, 0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "test_filter = [\n",
    "    [0, 0], \n",
    "    [-1, 1]\n",
    "]\n",
    "\n",
    "expected_result = [\n",
    "    [1, 0, 0, -1],\n",
    "    [0, 2, -2, 0],\n",
    "    [3, 0, 0, -3], \n",
    "    [0, 0, 0, 0], \n",
    "    [0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "print(\"ğŸ“Š Input Bild:\")\n",
    "for i, row in enumerate(test_input_data):\n",
    "    print(f\"Row {i}: {row}\")\n",
    "\n",
    "print(f\"\\nğŸ”§ Filter (Edge Detection):\")\n",
    "for i, row in enumerate(test_filter):\n",
    "    print(f\"Filter Row {i}: {row}\")\n",
    "\n",
    "# Test ausfÃ¼hren\n",
    "found = conv(test_input_data, test_filter)\n",
    "\n",
    "print(f\"\\nğŸ¯ Erwartetes Ergebnis:\")\n",
    "for i, row in enumerate(expected_result):\n",
    "    print(f\"Expected Row {i}: {row}\")\n",
    "\n",
    "print(f\"\\nâœ… Gefundenes Ergebnis:\")\n",
    "for i, row in enumerate(found):\n",
    "    print(f\"Found Row {i}: {row}\")\n",
    "\n",
    "# Verification\n",
    "try:\n",
    "    assert found == expected_result\n",
    "    print(\"\\nğŸ‰ Test erfolgreich! Ihre Convolution-Implementation ist korrekt!\")\n",
    "except AssertionError:\n",
    "    print(\"\\nâŒ Test fehlgeschlagen. ÃœberprÃ¼fen Sie Ihre Implementation.\")\n",
    "    print(\"ğŸ’¡ Tipp: Stellen Sie sicher, dass Sie Element-wise Multiplikation und Summation korrekt implementiert haben.\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Was passiert hier?\")\n",
    "print(\"â€¢ Filter [-1, 1] erkennt horizontale ÃœbergÃ¤nge (Links-Rechts Unterschiede)\")\n",
    "print(\"â€¢ Positive Werte: Ãœbergang von dunkel zu hell\")  \n",
    "print(\"â€¢ Negative Werte: Ãœbergang von hell zu dunkel\")\n",
    "print(\"â€¢ Zero Werte: Keine Ã„nderung\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtertypen\n",
    "\n",
    "Bevor wir nun in Richtung praktische Anwendung gehen, schauen wir uns grundlegende Filter an. AuÃŸerdem werden wir uns die Effekte der Filter anschauen - hierzu verwenden wir das folgende Bild:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_as_array(image_path: str, new_size: Tuple[int, int] = (500, 500)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    ğŸ“¸ Bild laden und als NumPy Array zurÃ¼ckgeben\n",
    "    \n",
    "    Parameter:\n",
    "    - image_path: Pfad zur Bilddatei\n",
    "    - new_size: Neue BildgrÃ¶ÃŸe (width, height)\n",
    "    \n",
    "    Returns:\n",
    "    - NumPy Array mit Graustufenwerten\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('L')  # Graustufen\n",
    "        img = img.resize(new_size, Image.Resampling.LANCZOS)  # Moderne Resampling-Methode\n",
    "        return np.array(img)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Bild nicht gefunden: {image_path}\")\n",
    "        # Fallback: Erstelle Beispielbild\n",
    "        return create_sample_image(new_size)\n",
    "\n",
    "def create_sample_image(size: Tuple[int, int] = (500, 500)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    ğŸ¨ Erstellt ein Beispielbild mit geometrischen Formen\n",
    "    \"\"\"\n",
    "    img = np.zeros(size)\n",
    "    h, w = size\n",
    "    \n",
    "    # Rechteck\n",
    "    img[h//4:3*h//4, w//4:w//2] = 255\n",
    "    \n",
    "    # Kreis  \n",
    "    center = (3*w//4, h//2)\n",
    "    y, x = np.ogrid[:h, :w]\n",
    "    mask = (x - center[0])**2 + (y - center[1])**2 <= (w//8)**2\n",
    "    img[mask] = 128\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Lade Testbild\n",
    "print(\"ğŸ“¸ Lade Testbild...\")\n",
    "\n",
    "try:\n",
    "    # Versuche Lama-Bild zu laden\n",
    "    if os.path.exists('images/lama.png'):\n",
    "        lama_array = read_image_as_array('images/lama.png', (500, 500))\n",
    "        print(\"âœ… Lama-Bild geladen!\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Lama-Bild nicht gefunden, erstelle Beispielbild...\")\n",
    "        lama_array = create_sample_image((500, 500))\n",
    "        \n",
    "    # Visualisierung\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Original Bild\n",
    "    axes[0].imshow(lama_array, cmap='gray', interpolation='nearest')\n",
    "    axes[0].set_title('ğŸ–¼ï¸ Testbild fÃ¼r Filter-Experimente', fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Histogram\n",
    "    axes[1].hist(lama_array.flatten(), bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1].set_title('ğŸ“Š Pixel-IntensitÃ¤ts-Verteilung', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Grauwert (0-255)')\n",
    "    axes[1].set_ylabel('Anzahl Pixel')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"ğŸ“ BildgrÃ¶ÃŸe: {lama_array.shape}\")\n",
    "    print(f\"ğŸ“Š Grauwert-Bereich: {lama_array.min()} - {lama_array.max()}\")\n",
    "    print(f\"ğŸ¯ Durchschnitt: {lama_array.mean():.1f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Fehler beim Bildladen: {e}\")\n",
    "    lama_array = create_sample_image()\n",
    "    print(\"ğŸ¨ Verwende generiertes Beispielbild\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IdentitÃ¤tsfilter\n",
    "\n",
    "Der erste Filter entspricht der IdentitÃ¤t, d.h. der Wert eines Pixel wird auf genau diesen abgebildet. Um dies zu erreichen wird ein quadratischer Filterkernel benÃ¶tigt, dessen GrÃ¶ÃŸe ungerade ist. AuÃŸerdem ist der mittlere Eintrag 1 und alle anderen 0. Ein $3\\times 3$-Filterkernel hat somit die Form:\n",
    "\n",
    "$\\left\\lbrack\\begin{array}{ccc} 0&0&0\\\\ 0&1&0\\\\ 0&0&0\\end{array}\\right\\rbrack$\n",
    "\n",
    "Und nun die angekÃ¼ndigte Anwendung auf das Bild:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(figure_inches, figure_inches))\n",
    "# IdentitÃ¤tsfilter definieren\n",
    "identity_filter = [\n",
    "    [0, 0, 0],\n",
    "    [0, 1, 0], \n",
    "    [0, 0, 0]\n",
    "]\n",
    "\n",
    "# Filter anwenden (mit unserer Implementation)\n",
    "filtered_identity = conv(data, identity_filter)\n",
    "\n",
    "# Visualisierung\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "images_data = [\n",
    "    (data, \"ğŸ–¼ï¸ Original\", \"Original Bild\"),\n",
    "    (np.array(filtered_identity), \"ğŸ”„ IdentitÃ¤tsfilter\", \"Sollte identisch sein\"),\n",
    "    (np.abs(data[1:-1, 1:-1] - np.array(filtered_identity)), \"ğŸ“Š Differenz\", \"Unterschied (sollte â‰ˆ0 sein)\")\n",
    "]\n",
    "\n",
    "for i, (img, title, subtitle) in enumerate(images_data):\n",
    "    axes[i].imshow(img, cmap='gray', interpolation='nearest')\n",
    "    axes[i].set_title(title, fontsize=14, fontweight='bold')\n",
    "    axes[i].text(0.02, 0.98, subtitle, transform=axes[i].transAxes, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8),\n",
    "                verticalalignment='top', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verifikation\n",
    "original_roi = data[1:-1, 1:-1]  # Region of Interest (ohne Rand)\n",
    "filtered_array = np.array(filtered_identity)\n",
    "difference = np.abs(original_roi - filtered_array)\n",
    "max_diff = np.max(difference)\n",
    "\n",
    "print(f\"\\nğŸ“Š Verifikation:\")\n",
    "print(f\"   Original ROI Shape: {original_roi.shape}\")\n",
    "print(f\"   Filtered Shape: {filtered_array.shape}\")\n",
    "print(f\"   Max Differenz: {max_diff}\")\n",
    "\n",
    "if max_diff < 1e-10:\n",
    "    print(\"   âœ… Perfect! IdentitÃ¤tsfilter verÃ¤ndert nichts\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  Kleine Differenz gefunden (erwartet bei Integer-Implementierung)\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Key Insight:\")\n",
    "print(\"â€¢ IdentitÃ¤tsfilter = Kernel mit 1 in der Mitte, 0 Ã¼berall sonst\")\n",
    "print(\"â€¢ Wichtig fÃ¼r CNN-Architekturen (Skip Connections, ResNet)\")\n",
    "print(\"â€¢ Baseline um andere Filter zu verstehen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eckendetektoren\n",
    "\n",
    "Die nÃ¤chsten drei Filter ziehlen darauf ab, Ecken im Bild zu finden. Ziel hierbei ist es flÃ¤chige Bereiche voneinander zu trennen. Die Filter sind oft nach deren Erfinder bzw. Entdecker benannt. In diesem Fall stellt der Sobel2 eine Verbesserung des Sobel1 dar - dieser kann zusÃ¤tzlich zum horizontalen sowie vertikalen auch im $45^\\circ$ Bereich messen.\n",
    "\n",
    "Roberts: $\\left\\lbrack\\begin{array}{ccc} 1&0&-1\\\\ 0&0&0\\\\ -1&0&1 \\end{array}\\right\\rbrack$\n",
    "\n",
    "Sobel1: $\\left\\lbrack\\begin{array}{ccc} 0&-1&0\\\\ -1&4&-1\\\\ 0&-1&0\\end{array}\\right\\rbrack$\n",
    "\n",
    "Sobel2: $\\left\\lbrack\\begin{array}{ccc} -1&-1&-1\\\\-1&8&-1\\\\ -1&-1&-1\\end{array}\\right\\rbrack$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš¡ Edge Detection Filter - Finde Kanten und Konturen!\n",
    "\n",
    "print(\"ğŸ¯ Edge Detection: Findet ÃœbergÃ¤nge zwischen verschiedenen Bereichen\")\n",
    "print(\"ğŸ’¡ Anwendung: Objekterkennung, Contouring, Feature Extraction\")\n",
    "\n",
    "# Edge Detection Filter definieren\n",
    "filters_edge = {\n",
    "    \"Roberts\": {\n",
    "        \"kernel\": [[1, 0, -1], [0, 0, 0], [-1, 0, 1]],\n",
    "        \"description\": \"Einfache Diagonal-Edge Detection\"\n",
    "    },\n",
    "    \"Sobel (Light)\": {\n",
    "        \"kernel\": [[0, -1, 0], [-1, 4, -1], [0, -1, 0]], \n",
    "        \"description\": \"KantenverstÃ¤rkung in 4 Richtungen\"\n",
    "    },\n",
    "    \"Sobel (Strong)\": {\n",
    "        \"kernel\": [[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]],\n",
    "        \"description\": \"Starke KantenverstÃ¤rkung in 8 Richtungen\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Moderne OpenCV Vergleiche\n",
    "sobel_x_cv = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])\n",
    "sobel_y_cv = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])\n",
    "\n",
    "print(\"\\nğŸ”§ Filter-Ãœbersicht:\")\n",
    "fig, axes = plt.subplots(1, len(filters_edge), figsize=(15, 4))\n",
    "\n",
    "for i, (name, filter_info) in enumerate(filters_edge.items()):\n",
    "    kernel = np.array(filter_info[\"kernel\"])\n",
    "    \n",
    "    # Filter visualisieren\n",
    "    im = axes[i].imshow(kernel, cmap='RdBu', vmin=-8, vmax=8)\n",
    "    axes[i].set_title(f'{name}', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Werte in Zellen schreiben\n",
    "    for (j, k), val in np.ndenumerate(kernel):\n",
    "        axes[i].text(k, j, f'{val}', ha='center', va='center', \n",
    "                    color='white' if abs(val) > 2 else 'black', fontweight='bold')\n",
    "    \n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])\n",
    "    \n",
    "    # Beschreibung\n",
    "    axes[i].text(0.5, -0.15, filter_info[\"description\"], \n",
    "                transform=axes[i].transAxes, ha='center', \n",
    "                fontsize=10, style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Filter anwenden und vergleichen\n",
    "print(\"\\nâš™ï¸  Anwenden der Edge-Detection Filter...\")\n",
    "\n",
    "# Roberts Filter anwenden\n",
    "roberts_result = conv(lama_array.tolist(), filters_edge[\"Roberts\"][\"kernel\"])\n",
    "\n",
    "# Visualisierung der Ergebnisse\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].imshow(lama_array, cmap='gray')\n",
    "axes[0, 0].set_title('ğŸ–¼ï¸ Original Bild', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Roberts Result\n",
    "roberts_array = np.array(roberts_result)\n",
    "axes[0, 1].imshow(np.abs(roberts_array), cmap='gray')  # Abs fÃ¼r bessere Sichtbarkeit\n",
    "axes[0, 1].set_title('âš¡ Roberts Edge Detection', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# OpenCV Sobel X\n",
    "sobel_x_result = cv2.filter2D(lama_array.astype(np.float32), -1, sobel_x_cv)\n",
    "axes[1, 0].imshow(np.abs(sobel_x_result), cmap='gray')\n",
    "axes[1, 0].set_title('ğŸ” Sobel X (OpenCV)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# OpenCV Sobel Y  \n",
    "sobel_y_result = cv2.filter2D(lama_array.astype(np.float32), -1, sobel_y_cv)\n",
    "axes[1, 1].imshow(np.abs(sobel_y_result), cmap='gray')\n",
    "axes[1, 1].set_title('ğŸ” Sobel Y (OpenCV)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiken\n",
    "print(f\"\\nğŸ“Š Edge Detection Statistiken:\")\n",
    "print(f\"   Roberts - Min: {roberts_array.min():.1f}, Max: {roberts_array.max():.1f}\")\n",
    "print(f\"   Sobel X - Min: {sobel_x_result.min():.1f}, Max: {sobel_x_result.max():.1f}\")\n",
    "print(f\"   Sobel Y - Min: {sobel_y_result.min():.1f}, Max: {sobel_y_result.max():.1f}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Edge Detection Insights:\")\n",
    "print(\"â€¢ Negative Werte = Ãœbergang von hell zu dunkel\")\n",
    "print(\"â€¢ Positive Werte = Ãœbergang von dunkel zu hell\") \n",
    "print(\"â€¢ Absolutwerte = KantenintensitÃ¤t (unabhÃ¤ngig von Richtung)\")\n",
    "print(\"â€¢ Sobel X = Vertikale Kanten, Sobel Y = Horizontale Kanten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_kern_sobel1 = [[0,-1,0], [-1,4,-1], [0,-1,0]]\n",
    "\n",
    "plt.figure(figsize=(figure_inches, figure_inches))\n",
    "data = read_image_as_array('images/lama.png', (500,500))\n",
    "filtered_data = conv(data, filter_kern_sobel1)\n",
    "\n",
    "plt.imshow(filtered_data, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ Moderne Computer Vision mit OpenCV\n",
    "\n",
    "### ğŸ’¡ Von manuellen Filtern zu intelligenten Algorithmen\n",
    "\n",
    "Bisher haben wir **manuelle Filter** kennengelernt. Jetzt schauen wir uns **intelligente CV-Algorithmen** an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ Moderne Computer Vision Anwendungen mit OpenCV\n",
    "\n",
    "print(\"ğŸš€ Von manuellen Filtern zu intelligenten Algorithmen\")\n",
    "\n",
    "# 1. ğŸ“Š CANNY EDGE DETECTION - State-of-the-Art\n",
    "def canny_edge_detection(image, low_threshold=50, high_threshold=150):\n",
    "    \"\"\"\n",
    "    ğŸ” Canny Edge Detection - Beste Edge Detection Methode\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n",
    "    edges = cv2.Canny(gray, low_threshold, high_threshold)\n",
    "    return edges\n",
    "\n",
    "# 2. ğŸ¯ CONTOUR DETECTION - Objektumrisse finden\n",
    "def detect_contours(image):\n",
    "    \"\"\"\n",
    "    ğŸ¯ Findet Objekt-Konturen im Bild\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n",
    "    \n",
    "    # Canny edges\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    \n",
    "    # Contours finden\n",
    "    contours, hierarchy = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Konturen auf Original zeichnen\n",
    "    result = image.copy() if len(image.shape) == 3 else cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    cv2.drawContours(result, contours, -1, (0, 255, 0), 2)\n",
    "    \n",
    "    return result, contours\n",
    "\n",
    "# 3. ğŸ§© IMAGE SEGMENTATION - K-Means Clustering\n",
    "def kmeans_segmentation(image, k=4):\n",
    "    \"\"\"\n",
    "    ğŸ§© K-Means Bildsegmentierung\n",
    "    \"\"\"\n",
    "    # Reshape fÃ¼r K-Means\n",
    "    data = image.reshape((-1, 3))\n",
    "    data = np.float32(data)\n",
    "    \n",
    "    # K-Means Parameter\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 1.0)\n",
    "    \n",
    "    # K-Means ausfÃ¼hren\n",
    "    _, labels, centers = cv2.kmeans(data, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "    \n",
    "    # Ergebnis rekonstruieren\n",
    "    centers = np.uint8(centers)\n",
    "    segmented_data = centers[labels.flatten()]\n",
    "    segmented_image = segmented_data.reshape(image.shape)\n",
    "    \n",
    "    return segmented_image\n",
    "\n",
    "# 4. ğŸ” FEATURE DETECTION - SIFT Features\n",
    "def detect_sift_features(image):\n",
    "    \"\"\"\n",
    "    ğŸ” SIFT (Scale-Invariant Feature Transform) Features\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n",
    "    \n",
    "    # SIFT Detector\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "    \n",
    "    # Keypoints zeichnen\n",
    "    result = cv2.drawKeypoints(image, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    \n",
    "    return result, keypoints, descriptors\n",
    "\n",
    "print(\"âœ… Computer Vision Funktionen definiert!\")\n",
    "\n",
    "# Teste mit unserem Bild\n",
    "if 'lama_array' in locals():\n",
    "    # Konvertiere zu RGB falls nÃ¶tig\n",
    "    if len(lama_array.shape) == 2:\n",
    "        test_image_rgb = cv2.cvtColor(lama_array, cv2.COLOR_GRAY2RGB)\n",
    "    else:\n",
    "        test_image_rgb = lama_array\n",
    "        \n",
    "    print(f\"ğŸ“¸ Test-Bild bereit: {test_image_rgb.shape}\")\n",
    "else:\n",
    "    print(\"âš ï¸  Laden Sie zuerst das Test-Bild!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ® Computer Vision Applications Demo\n",
    "\n",
    "print(\"ğŸ¯ Anwendung aller CV-Algorithmen auf unser Test-Bild:\")\n",
    "\n",
    "# Stelle sicher, dass wir ein RGB-Bild haben\n",
    "if len(lama_array.shape) == 2:\n",
    "    demo_image = cv2.cvtColor(lama_array, cv2.COLOR_GRAY2RGB)\n",
    "else:\n",
    "    demo_image = lama_array.copy()\n",
    "\n",
    "# 1. Canny Edge Detection\n",
    "edges = canny_edge_detection(demo_image)\n",
    "\n",
    "# 2. Contour Detection\n",
    "contour_result, contours = detect_contours(demo_image)\n",
    "\n",
    "# 3. K-Means Segmentation\n",
    "segmented = kmeans_segmentation(demo_image, k=4)\n",
    "\n",
    "# 4. SIFT Feature Detection\n",
    "sift_result, keypoints, descriptors = detect_sift_features(demo_image)\n",
    "\n",
    "# GroÃŸe Visualisierung aller Ergebnisse\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].imshow(demo_image)\n",
    "axes[0, 0].set_title('ğŸ–¼ï¸ Original Bild', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Canny Edges\n",
    "axes[0, 1].imshow(edges, cmap='gray')\n",
    "axes[0, 1].set_title('âš¡ Canny Edge Detection', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Contours\n",
    "axes[0, 2].imshow(contour_result)\n",
    "axes[0, 2].set_title(f'ğŸ¯ Contours ({len(contours)} gefunden)', fontsize=14, fontweight='bold')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# K-Means Segmentation\n",
    "axes[1, 0].imshow(segmented)\n",
    "axes[1, 0].set_title('ğŸ§© K-Means Segmentation', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# SIFT Features\n",
    "axes[1, 1].imshow(sift_result)\n",
    "axes[1, 1].set_title(f'ğŸ” SIFT Features ({len(keypoints)} gefunden)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Kombiniertes Ergebnis\n",
    "combined = demo_image.copy()\n",
    "# Canny Edges als rote Overlay\n",
    "combined[edges > 0] = [255, 0, 0]\n",
    "axes[1, 2].imshow(combined)\n",
    "axes[1, 2].set_title('ğŸ¨ Kombiniert: Original + Edges', fontsize=14, fontweight='bold')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiken ausgeben\n",
    "print(f\"\\nğŸ“Š Computer Vision Analyse-Ergebnisse:\")\n",
    "print(f\"   ğŸ–¼ï¸  BildgrÃ¶ÃŸe: {demo_image.shape}\")\n",
    "print(f\"   âš¡ Canny Edges: {np.sum(edges > 0):,} Pixel ({np.sum(edges > 0)/edges.size*100:.1f}%)\")\n",
    "print(f\"   ğŸ¯ Konturen: {len(contours)} Objekte erkannt\")\n",
    "print(f\"   ğŸ” SIFT Features: {len(keypoints)} charakteristische Punkte\")\n",
    "print(f\"   ğŸ§© Segmentierung: 4 Bereiche unterschieden\")\n",
    "\n",
    "# GrÃ¶ÃŸte Kontur analysieren\n",
    "if contours:\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    contour_area = cv2.contourArea(largest_contour)\n",
    "    image_area = demo_image.shape[0] * demo_image.shape[1]\n",
    "    print(f\"   ğŸ“ GrÃ¶ÃŸtes Objekt: {contour_area:.0f} Pixel ({contour_area/image_area*100:.1f}% des Bildes)\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ SIFT Descriptor Info:\")\n",
    "if descriptors is not None:\n",
    "    print(f\"   ğŸ“Š Descriptor Matrix: {descriptors.shape}\")\n",
    "    print(f\"   ğŸ”¢ Pro Feature: {descriptors.shape[1]} Dimensionen\")\n",
    "    print(f\"   ğŸ’¾ Speicher: {descriptors.nbytes:,} Bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ® Interaktive Computer Vision Experimente\n",
    "\n",
    "### ğŸ’» Jupyter Widgets fÃ¼r Live-Demos\n",
    "\n",
    "Testen Sie verschiedene CV-Parameter interaktiv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ® Interaktive Canny Edge Detection\n",
    "\n",
    "def interactive_canny(low_threshold=50, high_threshold=150, blur_kernel=1):\n",
    "    \"\"\"\n",
    "    ğŸ¯ Interaktive Canny Edge Detection mit Parameter-Tuning\n",
    "    \"\"\"\n",
    "    # Bild vorbereiten\n",
    "    img = demo_image.copy()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Optional: Blur anwenden\n",
    "    if blur_kernel > 1:\n",
    "        gray = cv2.GaussianBlur(gray, (blur_kernel, blur_kernel), 0)\n",
    "    \n",
    "    # Canny Edge Detection\n",
    "    edges = cv2.Canny(gray, low_threshold, high_threshold)\n",
    "    \n",
    "    # Visualisierung\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Original\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title('ğŸ–¼ï¸ Original', fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Graustufen (mit Blur)\n",
    "    axes[1].imshow(gray, cmap='gray')\n",
    "    title = f'ğŸ“Š Graustufen' + (f' + Blur({blur_kernel}x{blur_kernel})' if blur_kernel > 1 else '')\n",
    "    axes[1].set_title(title, fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Canny Edges\n",
    "    axes[2].imshow(edges, cmap='gray')\n",
    "    axes[2].set_title(f'âš¡ Canny (L:{low_threshold}, H:{high_threshold})', fontsize=14, fontweight='bold')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistiken\n",
    "    edge_count = np.sum(edges > 0)\n",
    "    edge_percentage = (edge_count / edges.size) * 100\n",
    "    \n",
    "    print(f\"ğŸ“Š Canny Statistiken:\")\n",
    "    print(f\"   Threshold: Low={low_threshold}, High={high_threshold}\")\n",
    "    print(f\"   Edge Pixel: {edge_count:,} ({edge_percentage:.2f}%)\")\n",
    "    print(f\"   Blur Kernel: {blur_kernel}x{blur_kernel}\")\n",
    "\n",
    "# Widget erstellen\n",
    "print(\"ğŸ® Interaktive Canny Edge Detection:\")\n",
    "print(\"â€¢ Low Threshold: Minimaler Gradient fÃ¼r Edge-Kandidaten\")\n",
    "print(\"â€¢ High Threshold: Minimaler Gradient fÃ¼r sichere Edges\")  \n",
    "print(\"â€¢ Blur Kernel: RauschunterdrÃ¼ckung (ungerade Zahlen)\")\n",
    "\n",
    "interactive_canny_widget = interact(\n",
    "    interactive_canny,\n",
    "    low_threshold=widgets.IntSlider(\n",
    "        value=50, min=10, max=200, step=10,\n",
    "        description='Low Threshold:',\n",
    "        style={'description_width': 'initial'}\n",
    "    ),\n",
    "    high_threshold=widgets.IntSlider(\n",
    "        value=150, min=50, max=300, step=10,\n",
    "        description='High Threshold:',\n",
    "        style={'description_width': 'initial'}\n",
    "    ),\n",
    "    blur_kernel=widgets.IntSlider(\n",
    "        value=1, min=1, max=15, step=2,\n",
    "        description='Blur Kernel:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ’¡ Tuning-Tipps:\")\n",
    "print(\"â€¢ Niedrige Thresholds = mehr Edges (mehr Rauschen)\")\n",
    "print(\"â€¢ Hohe Thresholds = weniger Edges (nur starke Kanten)\")\n",
    "print(\"â€¢ Blur reduziert Rauschen, kann aber Details verwischen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ® Streamlit Computer Vision App\n",
    "\n",
    "### ğŸ’» Professionelle CV-Anwendung erstellen\n",
    "\n",
    "Sie kÃ¶nnen eine **vollstÃ¤ndige Computer Vision App** mit Streamlit erstellen:\n",
    "\n",
    "```bash\n",
    "# Im Terminal ausfÃ¼hren:\n",
    "cd 06_Computer_Vision_NLP\n",
    "streamlit run 06_02_streamlit_cv_apps.py\n",
    "```\n",
    "\n",
    "### ğŸŒŸ Features der CV-App:\n",
    "- âœ… **Multiple CV-Algorithmen:** Edge Detection, Object Detection, Segmentation\n",
    "- âœ… **Eigene Bilder hochladen** und analysieren\n",
    "- âœ… **Parameter-Tuning** in Echtzeit\n",
    "- âœ… **Feature Detection** (SIFT, ORB, Harris Corners)\n",
    "- âœ… **Filter-Vergleiche** side-by-side\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Zusammenfassung & Key Takeaways\n",
    "\n",
    "### âœ… Was Sie gelernt haben:\n",
    "\n",
    "#### 1. ğŸ§® **Mathematische Grundlagen**\n",
    "- **Convolution Operation:** Wie Filter mathematisch funktionieren\n",
    "- **2D-Convolution:** Anwendung auf Bilder (HÃ—WÃ—C)\n",
    "- **Parameter-Berechnung:** Kernel Size, Stride, Padding\n",
    "- **Eigene Implementation:** Convolution von Grund auf verstehen\n",
    "\n",
    "#### 2. ğŸ”§ **Klassische Filter**\n",
    "- **IdentitÃ¤tsfilter:** Baseline und VerstÃ¤ndnis\n",
    "- **Edge Detection:** Roberts, Sobel (Light/Strong)\n",
    "- **Filter-Comparison:** Manuelle vs. OpenCV Implementation\n",
    "- **Parameter-Tuning:** Optimale Einstellungen finden\n",
    "\n",
    "#### 3. ğŸš€ **Moderne Computer Vision**\n",
    "- **Canny Edge Detection:** State-of-the-Art Kantendetection\n",
    "- **Contour Detection:** Objektumrisse automatisch finden\n",
    "- **Image Segmentation:** K-Means Clustering fÃ¼r Bereiche\n",
    "- **Feature Detection:** SIFT fÃ¼r charakteristische Punkte\n",
    "\n",
    "#### 4. ğŸ® **Praktische Anwendungen**\n",
    "- **OpenCV Integration:** Professionelle CV-Library nutzen\n",
    "- **Interactive Widgets:** Parameter live anpassen\n",
    "- **Streamlit Apps:** Portfolio-ready CV-Anwendungen\n",
    "- **Real-world Workflows:** Von Preprocessing bis Analyse\n",
    "\n",
    "### ğŸ’¡ **Von Theorie zur Praxis:**\n",
    "\n",
    "```python\n",
    "# Klassische Implementierung (Educational)\n",
    "def conv(image, filter_kernel):\n",
    "    # Manual convolution for understanding\n",
    "    \n",
    "# Moderne Implementierung (Production)\n",
    "edges = cv2.Canny(image, 50, 150)\n",
    "contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "```\n",
    "\n",
    "### ğŸ¯ **NÃ¤chste Schritte:**\n",
    "\n",
    "In den kommenden Notebooks vertiefen wir:\n",
    "- **06.3:** Data Augmentation fÃ¼r robuste Modelle\n",
    "- **06.4:** Transfer Learning mit vortrainierten CNNs\n",
    "- **Portfolio:** Eigene CV-Projekte fÃ¼r Bewerbungen\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ† **Portfolio-Projekt Challenge:**\n",
    "\n",
    "1. **ğŸ“¸ Eigenes Bild-Dataset** sammeln (Selfies, Objekte, etc.)\n",
    "2. **ğŸ”§ CV-Pipeline** entwickeln (Preprocessing â†’ Detection â†’ Analysis)\n",
    "3. **ğŸ® Streamlit-App** erstellen mit Upload-FunktionalitÃ¤t\n",
    "4. **ğŸ“Š Analyse-Dashboard** mit Statistiken und Visualisierungen\n",
    "5. **ğŸš€ GitHub Repository** fÃ¼r Ihr Portfolio\n",
    "\n",
    "**ğŸ’ª Advanced Challenge:** Kombinieren Sie Edge Detection + Contour Detection + Feature Detection fÃ¼r ein intelligentes Objekt-Analyse-Tool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_kern_sobel2 = [[-1,-1,-1], [-1,8,-1], [-1,-1,-1]]\n",
    "\n",
    "plt.figure(figsize=(figure_inches, figure_inches))\n",
    "data = read_image_as_array('images/lama.png', (500,500))\n",
    "filtered_data = conv(data, filter_kern_sobel2)\n",
    "\n",
    "plt.imshow(filtered_data, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BildschÃ¤rfen\n",
    "\n",
    "Der nÃ¤chste Filter dient, wie der Name bereits vermuten lÃ¤sst, dazu, dass Konturen im Bild schÃ¤rfer werden.\n",
    "\n",
    "$\\left\\lbrack\\begin{array}{ccc} 0&-1&0\\\\ -1&5&-1\\\\ 0&-1&0 \\end{array}\\right\\rbrack$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filter_kern_sharp = [[0,-1,0], [-1,5,-1], [0,-1,0]]\n",
    "\n",
    "plt.figure(figsize=(figure_inches, figure_inches))\n",
    "data = read_image_as_array('images/lama.png', (500,500))\n",
    "filtered_data = conv(data, filter_kern_sharp)\n",
    "\n",
    "plt.imshow(filtered_data, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blur / UnschÃ¤rfe\n",
    "\n",
    "Die letzen beiden Filter dienen dazu, das Bild zu glÃ¤tten. Der erste Filter wird auch als Box-Linear-Filter bezeichnet und ist verhÃ¤tlinismÃ¤ÃŸig relativ simple aufgebaut. Der zweite Filter basiert auf einer GauÃŸverteilung und wird daher als GauÃŸ-Filter bezeichnet.\n",
    "\n",
    "Box-Linear-Filter: $\\frac{1}{9} \\left\\lbrack\\begin{array}{ccc}1&1&1\\\\ 1&1&1\\\\ 1&1&1\\end{array}\\right\\rbrack$\n",
    "\n",
    "GauÃŸ-Filter: $\\frac{1}{16} \\left\\lbrack\\begin{array}{ccc}1&2&1\\\\ 2&4&2\\\\ 1&2&1\\end{array}\\right\\rbrack$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_kern_blf = [[1/9, 1/9, 1/9], [1/9, 1/9, 1/9], [1/9, 1/9, 1/9]]\n",
    "\n",
    "plt.figure(figsize=(figure_inches, figure_inches))\n",
    "data = read_image_as_array('images/lama.png', (500,500))\n",
    "filtered_data = conv(data, filter_kern_blf)\n",
    "\n",
    "plt.imshow(filtered_data, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_kern_gauss = [[1/16, 2/16, 1/16], [2/16, 4/16, 2/16], [1/16, 2/16, 1/16]]\n",
    "\n",
    "plt.figure(figsize=(figure_inches, figure_inches))\n",
    "data = read_image_as_array('images/lama.png', (500,500))\n",
    "filtered_data = conv(data, filter_kern_gauss)\n",
    "\n",
    "plt.imshow(filtered_data, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB-Bilder\n",
    "\n",
    "Farbige Bilder kÃ¶nnen in der Regel durch RGB-Bilder dargestellt werden, wobei $d$ gleich 3 ist und enthÃ¤lt:\n",
    "\n",
    "- R (rot), \n",
    "- G (grÃ¼n),\n",
    "- B (blau)\n",
    "\n",
    "Werte fÃ¼r alle Pixel in einem Bild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lama = Image.open('images/lama.png')\n",
    "lama = np.array(lama)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(figure_inches, figure_inches))\n",
    "ax.set_title('Lama image 768x1024', fontsize = 15)\n",
    "ax.imshow(lama, interpolation='nearest')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In general deep learning (and in tensorflow) Conv-layers will \n",
    "# regard all channels and therefore use \"cubic\" filter\n",
    "\n",
    "# The filter used here in the example down below is only using d=1 (two - dimensional) of the \n",
    "# rgb image (therefore red), you can change [:,:,0] to [:,:,1] (green) and [:,:,2] (blue)!\n",
    "# Try it! :)\n",
    "\n",
    "prewitt_x =  np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]])\n",
    "\n",
    "lama_x_prew = signal.convolve2d(lama[:,:,0], prewitt_x, boundary='symm', mode='same')\n",
    "lama_x_prew = np.absolute(lama_x_prew)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(figure_inches, figure_inches))\n",
    "ax.set_title('Horizontale Ableitung des Lama Bildes', fontsize = 15)\n",
    "ax.imshow(lama_x_prew, interpolation='nearest', cmap='gray')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Die Faltungsschicht (engl. Convolutional Layer)\n",
    "\n",
    "<img src=\"images/featuremaps.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "Eine Faltungsschicht, welche die erste Schicht im Netzwerk sein kÃ¶nnte, ist im Bild oben dargestellt. Ihr Kernel oder Filter mit den Dimensionen $K_x \\times K_y \\times d$ enthÃ¤lt Gewichte, die wÃ¤hrend des Trainings aktualisiert werden und auch die Darstellung der Bilder verÃ¤ndern. Eine Aktivierungskarte (engl. activation map) entspricht einer Faltungsoperation mit einem bestimmten Filter und dem zugehÃ¶rigen Eingangsbild oder den rÃ¤umlichen Daten der vorherigen Schicht. In den meisten FÃ¤llen werden nicht nur ein, sondern mehrere Filter in einer Faltungsschicht gelernt, so dass es mehrere Aktivierungskarten gibt. In diesem speziellen Fall scheint die AusgabegrÃ¶ÃŸe dieser Faltungsschicht im Vergleich zur EingabegrÃ¶ÃŸe grÃ¶ÃŸer geworden zu sein. Infolgedessen werden hÃ¤ufig Pooling-Operationen angehÃ¤ngt, um die Daten innerhalb des Netzwerks zu reduzieren. Die nÃ¤chste Schicht erhÃ¤lt dann wieder rÃ¤umliche Informationen und verwendet Filter, um die rÃ¤umlichen Informationen zu extrahieren und zu verÃ¤ndern.\n",
    "\n",
    "\n",
    "**Idea**: _`SpÃ¤rliche Verbindungen (engl. Sparse Connections)` (nicht vollstÃ¤ndig verbundene Schichten wie bei einem MLP) sind als Kernel fÃ¼r groÃŸe Datenstrukturen gegeben. Die Anzahl der lernbaren Gewichte sinkt!_\n",
    "\n",
    "Vergleichen wir eine standardmÃ¤ÃŸige voll verbundene Schicht (engl. fully connected layer) eines MLP mit einer Faltungsschicht fÃ¼r ein regulÃ¤res farbiges Bild der GrÃ¶ÃŸe $256\\times256\\times3$:\n",
    "- Erstes Hidden Layer in einer voll verbundenen Schicht:\n",
    "    - Input Neuronen $\\rightarrow$ $256*256*3$\n",
    "    - Beginnen Sie z. B. mit der HÃ¤lfte der Neuronen im ersten Hidden Layer $\\rightarrow$ $128*256*3$\n",
    "    - Ergebnisse in Gewichte und Biases $\\rightarrow$ $256*256*3*128*256*3 + 128*256*3 = 19.327.451.136$ Parameters\n",
    "\n",
    "        \n",
    "- Erste Faltungsschicht in einem faltigen neuronalen Netz: Standard 256 Filter (vernÃ¼nftige GrÃ¶ÃŸe) der GrÃ¶ÃŸe $3\\times3\\times3$ \n",
    "    - Gewichte und Biases $\\rightarrow$ $256 * 3 * 3 *3 + 256 = 7.168 $ Parameters\n",
    "    \n",
    "Trotzdem brauchen Faltungen mit rÃ¤umlichen BlÃ¶cken wie in der obigen Abbildung noch Zeit, um verarbeitet zu werden.\n",
    "Lokale Informationen werden nur nicht wie globale AbhÃ¤ngigkeiten in Hidden Layers verwendet!\n",
    "\n",
    "Die **Vorteile** einer Faltungsschicht (`CONV`) gegenÃ¼ber einer vollverknÃ¼pften Schicht sind die folgenden\n",
    " - Weniger Parameter fÃ¼r das Training\n",
    " - Nutzung der lokalen Strukturen des Bildes\n",
    " - UnabhÃ¤ngig von der Position des Merkmals im Bild\n",
    " \n",
    "**Nachteile** von Faltungsschichten (`CONV`):\n",
    " - Informationen mÃ¼ssen rÃ¤umliche AbhÃ¤ngigkeiten haben (wie bei einem menschlich erkennbaren Bild)\n",
    "\n",
    "Beim Stapeln mehrerer Faltungsschichten hat ein Kernel der folgenden Faltungsschicht die Form $K_x \\times K_y \\times d$, wobei $d$ die Anzahl der KanÃ¤le der vorherigen Schicht ist. Die Anzahl der KanÃ¤le ist gegeben durch die Anzahl der verschiedenen Filter, die in der Faltungsschicht verwendet werden. Definiert man also eine Faltungsschicht mit z. B. $nb\\_filters=64$, so legt man die dritte Dimension eines Filters in der nÃ¤chsten Schicht fest. Denn im zweidimensionalen Fall expandiert der Filter immer auf die vorherige Kanaldimension. Betrachtet man CNNs fÃ¼r die Videoanalyse oder fÃ¼r Zeitreihen, so stÃ¶ÃŸt man auf 3-dimensionale Faltungsschichten, die sich nicht nur in den Bilddimensionen bewegen, sondern in einer dritten Dimension (in diesem Fall: Zeit). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Die Poolingsschicht (engl. pooling layer)\n",
    "\n",
    "<img src=\"images/maxpool.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "                                     Quelle: http://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "\n",
    "Die Pooling Schicht ist ein Filter wie alle anderen Filter im neuronalen Faltungsnetzwerk. Allerdings mit der Ausnahme, dass sie ihre Gewichte nicht aktualisiert und eine feste Funktionsoperation durchfÃ¼hrt. Die hÃ¤ufigste Pooling-Operation ist das Max-Pooling. Wie der Name schon sagt, wird im Bereich des Kerns nur der Maximalwert weitergegeben. Normalerweise entspricht der Stride den Dimensionen des Kernels. Das Max-Pooling wird nur auf die HÃ¶he und Breite des Bildes angewendet, so dass die Kanaldimensionen nicht betroffen sind. Es wird verwendet, um rÃ¤umliche Informationen zu reduzieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 5.2.2:</b> Implementieren Sie die Funktion <code>max_pool</code> die Maxpooling durchfÃ¼hrt. Gegeben ist wieder ein Grauwertbild <code>image_data</code>, d.h. es besitzt nur einen Kanal und Sie kÃ¶nnen annehmen, dass das Bilder wieder als eine Liste von Listen Ã¼bergeben wird. AuÃŸerdem ist die GrÃ¶ÃŸe des Filters <code>filter_size</code> als Tupel und die <code>stride</code> als <code>int</code> gegeben.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool(image_data:list, filter_size:tuple, stride:int)->list:\n",
    "    # STUDENT CODE HERE\n",
    "\n",
    "    # STUDENT CODE until HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_data = [[0,0,0,0,0], [0,1,1,1,0], [0,0,2,0,0], [0,3,3,3,0], [0,0,0,0,0], [0,0,0,0,0]]\n",
    "test_filter_size = (2,2)\n",
    "stride = 2\n",
    "test_result = [[1,1],[3,3], [0,0]]\n",
    "\n",
    "# The folgende Zeile erzeugt einen Fehler, wenn die Ausgabe der Methode nicht mit der erwarteten Ã¼bereinstimmt \n",
    "found = max_pool(test_input_data, test_filter_size, stride)\n",
    "assert found == test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU - Schicht oder Aktivierung\n",
    "Die \"RELU\"-Schicht oder Aktivierung verwendet eine elementweise Aktivierungsfunktion auf das Raumvolumen an, wie auf jeden Knoten in einer Hidden Layer. Die Funktion kann als $max(0,x)$ angegeben werden und ist unten dargestellt. Betrachten Sie $\\sigma(x)$ als die Aktivierungsfunktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def relu(x:float)->float:\n",
    "    return np.maximum(0,x)\n",
    "x = np.linspace(-10, 10, num = 1000)\n",
    "\n",
    "plt.figure(2, figsize=(10,3.5))\n",
    "plt.plot(x, relu(x), label='ReLU')\n",
    "plt.title('The ReLU activation')\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('$\\sigma(x)$')\n",
    "plt.tight_layout()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zusammenfassung\n",
    "\n",
    "Die folgende Animation zeigt recht gut, wie ein Faltungsnetzwerk (engl. convolutional network) anhand des `MNIST`-Datensatzes funktioniert.\n",
    "Nachdem die Faltungsschichten die ReprÃ¤sentation der Bilder verÃ¤ndert haben, werden die endgÃ¼ltigen mehrdimensionalen BlÃ¶cke in ein langes Array gelegt (die Operation wird \"Flattening\" genannt) und an voll verbundene Schichten eines neuronalen Netzes weitergeleitet.\n",
    "\n",
    "[MNIST-CLassification](http://scs.ryerson.ca/~aharley/vis/conv/flat.html)\n",
    "\n",
    "#### Receptive Field\n",
    "\n",
    "In der Animation bzw. Simulation von MNIST werden AbhÃ¤ngigkeiten, die als Linien zwischen mehr als zwei Schichten dargestellt werden, nicht abgebildet.\n",
    "Dennoch ist es mÃ¶glich, Beziehungen zwischen beliebigen Schichten innerhalb des Netzes darzustellen. Dadurch ist es mÃ¶glich, ein gewisses Wissen oder eine Idee Ã¼ber die Anzahl der Faltungsschichten zu erhalten, die fÃ¼r eine Anwendung oder Aufgabe verwendet werden sollten. Betrachten Sie drei Ã¼bereinander gestapelte Faltungsschichten wie im Bild unten. Ein Wert in der grÃ¼nen Schicht bezieht sich auf 9 Eingangswerte. Folglich summiert sich ein Wert in der gelben Schicht auf 9 in der grÃ¼nen Schicht. Ein Eintrag in der gelben Schicht wird also von mehr Werten beeinflusst als die grÃ¼nen AktivierungseintrÃ¤ge in Bezug auf das Eingangsbild. Dieser Bereich ist gelb dargestellt und deckt 49 Werte des Eingangsbildes ab. Um die Dimensionen wÃ¤hrend der Faltungen wie in Ã¼blichen CNNs beizubehalten, wurde ein Padding verwendet, um die Dimensionen der Matrix gleich zu halten. Die `Initialmatrix` ist dann von der GrÃ¶ÃŸe $7 \\times 7$.\n",
    "\n",
    "<img src=\"images/ReceptiveField.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "    Quelle:https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 5.2.3:</b> Was ist der Hauptunterschied zwischen einer Faltungsschicht (engl. convolutional layer) und einer vollverknÃ¼pften Schicht (engl. fully-connected layer) und warum werden Ã¼berhaupt Filter verwendet?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python-amalea"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
