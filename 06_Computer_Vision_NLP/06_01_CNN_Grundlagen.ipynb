{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ Setup: Installation und Importe\n",
    "# FÃ¼hren Sie diese Zelle aus, um alle notwendigen Libraries zu installieren\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Standard Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Computer Vision\n",
    "import cv2\n",
    "from scipy import signal\n",
    "import skimage\n",
    "from skimage import filters, feature\n",
    "\n",
    "# Streamlit (fÃ¼r interaktive Apps)\n",
    "import streamlit as st\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "# Konfiguration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"âœ… Alle Libraries erfolgreich importiert!\")\n",
    "print(f\"ğŸ“Š TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"ğŸ–¼ï¸ OpenCV Version: {cv2.__version__}\")\n",
    "\n",
    "# GPU Check\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"ğŸš€ GPU verfÃ¼gbar!\")\n",
    "else:\n",
    "    print(\"ğŸ’» CPU wird verwendet\")\n",
    "\n",
    "# [Nur Colab] Diese Zellen mÃ¼ssen nur auf *Google Colab* ausgefÃ¼hrt werden und installieren Packete und Daten\n",
    "!wget -q https://raw.githubusercontent.com/KI-Campus/AMALEA/master/requirements.txt && pip install --quiet -r requirements.txt\n",
    "!wget --quiet \"https://github.com/KI-Campus/AMALEA/releases/download/data/data.zip\" && unzip -q data.zip\n",
    "!wget --quiet \"https://github.com/KI-Campus/AMALEA/releases/download/images/images.zip\" && unzip -q images.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ‘ï¸ 06.1 CNN Grundlagen - Convolutional Neural Networks verstehen\n",
    "\n",
    "**Data Analytics & Big Data - Woche 6.1**  \n",
    "*IU Internationale Hochschule*\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Lernziele\n",
    "\n",
    "Nach diesem Notebook kÃ¶nnen Sie:\n",
    "- âœ… **CNN-Architektur** verstehen (Convolutional, Pooling, Dense Layers)\n",
    "- âœ… **Bildfilter** anwenden und deren Wirkung verstehen  \n",
    "- âœ… **Feature Maps** visualisieren und interpretieren\n",
    "- âœ… **Streamlit-App** fÃ¼r Computer Vision entwickeln\n",
    "- âœ… **TensorFlow/Keras** fÃ¼r CNN-Implementierung nutzen\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Was sind CNNs?\n",
    "\n",
    "**Convolutional Neural Networks (CNNs)** sind spezialisierte neuronale Netze fÃ¼r die **Bildverarbeitung**. \n",
    "\n",
    "### ğŸ”‘ Kernkonzepte:\n",
    "1. **Convolution (Faltung)** - Erkennt lokale Features wie Kanten\n",
    "2. **Pooling** - Reduziert BildgrÃ¶ÃŸe, behÃ¤lt wichtige Information  \n",
    "3. **Feature Maps** - Zeigen welche Merkmale das Netz erkannt hat\n",
    "4. **Hierarchical Learning** - Von einfachen Kanten zu komplexen Objekten\n",
    "\n",
    "### ğŸ—ï¸ CNN-Architektur:\n",
    "```\n",
    "Input Image â†’ Conv2D â†’ ReLU â†’ MaxPool â†’ Conv2D â†’ ReLU â†’ MaxPool â†’ Flatten â†’ Dense â†’ Output\n",
    "```\n",
    "\n",
    "![CNN feedforward](images/stanford_cnn.jpg \"CNN_feedforward\")\n",
    "\n",
    "*Quelle: Stanford CS231n*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  Warum CNNs fÃ¼r Computer Vision?\n",
    "\n",
    "### ğŸ¤” Das Problem mit normalen Neural Networks\n",
    "\n",
    "**Normale Fully-Connected Networks** fÃ¼r Bilder haben massive Probleme:\n",
    "\n",
    "```python\n",
    "# Beispiel: 28x28 Pixel Bild (MNIST)\n",
    "input_size = 28 * 28 * 1  # = 784 Parameter pro Bild\n",
    "hidden_layer = 128        # = 784 * 128 = 100,352 Gewichte nur fÃ¼r erste Schicht!\n",
    "\n",
    "# Bei 224x224 RGB Bildern (ImageNet):\n",
    "input_size = 224 * 224 * 3  # = 150,528 Parameter\n",
    "# â†’ Millionen von Gewichten schon in der ersten Schicht!\n",
    "```\n",
    "\n",
    "### ğŸ’¡ Die CNN-LÃ¶sung\n",
    "\n",
    "**CNNs nutzen 3 SchlÃ¼sselkonzepte:**\n",
    "\n",
    "#### 1. ğŸ” **Local Connectivity (Lokale Verbindungen)**\n",
    "- Jedes Neuron schaut nur auf einen kleinen Bildbereich (z.B. 3x3 Pixel)\n",
    "- Reduziert Parameter drastisch\n",
    "\n",
    "#### 2. ğŸ”„ **Weight Sharing (Gewichtsteilen)**  \n",
    "- Derselbe Filter wird Ã¼ber das gesamte Bild angewendet\n",
    "- Ein 3x3 Filter hat nur 9 Parameter (statt Millionen!)\n",
    "\n",
    "#### 3. ğŸ“ **Translation Invariance (PositionsunabhÃ¤ngigkeit)**\n",
    "- Erkennt Objekte unabhÃ¤ngig von ihrer Position im Bild\n",
    "- Katze oben links = Katze unten rechts\n",
    "\n",
    "### ğŸ—ï¸ CNN-Architektur im Detail\n",
    "\n",
    "```\n",
    "Input (28x28x1) \n",
    "    â†“ Conv2D(32 filters, 3x3) + ReLU\n",
    "Feature Maps (26x26x32)\n",
    "    â†“ MaxPooling(2x2) \n",
    "Reduced Maps (13x13x32)\n",
    "    â†“ Conv2D(64 filters, 3x3) + ReLU\n",
    "Feature Maps (11x11x64)\n",
    "    â†“ MaxPooling(2x2)\n",
    "Final Maps (5x5x64)\n",
    "    â†“ Flatten\n",
    "Vector (1600)\n",
    "    â†“ Dense(128) + ReLU\n",
    "    â†“ Dense(10) + Softmax\n",
    "Output (10 classes)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Bildfilter verstehen - Von klassisch zu CNN\n",
    "\n",
    "### ğŸ¯ Lernziel\n",
    "Bevor wir CNNs trainieren, verstehen wir **wie Filter funktionieren** durch klassische Computer Vision Filter.\n",
    "\n",
    "### ğŸ“Š Filter-Konzept\n",
    "Ein **Filter (Kernel)** ist eine kleine Matrix (z.B. 3x3), die Ã¼ber ein Bild \"gefaltet\" wird:\n",
    "\n",
    "```python\n",
    "# Beispiel 3x3 Filter\n",
    "filter_3x3 = [\n",
    "    [a, b, c],\n",
    "    [d, e, f], \n",
    "    [g, h, i]\n",
    "]\n",
    "\n",
    "# Convolution Operation:\n",
    "output_pixel = (pixel_area * filter).sum()\n",
    "```\n",
    "\n",
    "### ğŸ› ï¸ Filter-Typen die wir testen werden:\n",
    "\n",
    "1. **ğŸŒ«ï¸ Blur Filter (Mean)** - Bild unscharf machen\n",
    "2. **âš¡ Edge Detection (Prewitt/Sobel)** - Kanten finden  \n",
    "3. **ğŸ”„ Sharpening (Laplace)** - Bild schÃ¤rfen\n",
    "4. **ğŸ¨ Custom Filters** - Eigene Effekte\n",
    "\n",
    "**ğŸ’¡ Der Clou:** CNNs lernen diese Filter automatisch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š Spezielle Importe fÃ¼r Bildverarbeitung\n",
    "from ipywidgets import interact, interactive, fixed, widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import figure\n",
    "import numpy as np\n",
    "from scipy import misc, signal\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Matplotlib Konfiguration fÃ¼r bessere Plots\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"âœ… Bildverarbeitungs-Libraries importiert!\")\n",
    "\n",
    "# Test: VerfÃ¼gbare Beispielbilder\n",
    "try:\n",
    "    # Scipy Beispielbild\n",
    "    ascent = misc.ascent()\n",
    "    print(f\"ğŸ“¸ Beispielbild geladen: {ascent.shape}\")\n",
    "    \n",
    "    # OpenCV Test\n",
    "    print(f\"ğŸ”§ OpenCV Version: {cv2.__version__}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Fehler beim Laden: {e}\")\n",
    "    print(\"ğŸ’¡ Tipp: Installieren Sie scipy und opencv-python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Ascent image from scipy\n",
    "ascent = misc.ascent()\n",
    "\n",
    "# Moderne Visualisierung\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Original Bild\n",
    "axes[0].imshow(ascent, cmap='gray', interpolation='nearest')\n",
    "axes[0].set_title('ğŸ”ï¸ Original Bild (Ascent)', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Histogram\n",
    "axes[1].hist(ascent.flatten(), bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[1].set_title('ğŸ“Š Pixel-Verteilung', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Pixel-IntensitÃ¤t (0-255)')\n",
    "axes[1].set_ylabel('Anzahl Pixel')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bildinformationen\n",
    "print(f\"ğŸ“ BildgrÃ¶ÃŸe: {ascent.shape}\")\n",
    "print(f\"ğŸ“Š Datentyp: {ascent.dtype}\")\n",
    "print(f\"ğŸ“ˆ Min/Max Werte: {ascent.min()} / {ascent.max()}\")\n",
    "print(f\"ğŸ¯ Durchschnitt: {ascent.mean():.1f}\")\n",
    "\n",
    "# Fun Fact\n",
    "print(f\"\\nğŸ’¡ Fun Fact: Dieses {ascent.shape[0]}x{ascent.shape[1]} Bild hat {ascent.size:,} Pixel!\")\n",
    "print(f\"ğŸ’¾ Speicherbedarf: {ascent.nbytes:,} Bytes ({ascent.nbytes/1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean-Filter\n",
    "\n",
    "Der erste Filter, den wir anwenden werden, heiÃŸt \"Mean-Filter\". Der Mean-Filter ersetzt einen Pixelwert durch den Mittelwert der Werte, die sich in der Nachbarschaft des Pixels $9\\times 9$ befinden. Das heiÃŸt, wir verwenden alle benachbarten Pixel sowie die Werte des zu ersetzenden Pixels, dann berechnen wir den Mittelwert dieser neun Werte und verwenden das Ergebnis als neuen Pixelwert.\n",
    "\n",
    "## ğŸ® Interaktive Filter-Experimente\n",
    "\n",
    "### ğŸ’» Jupyter Widgets fÃ¼r Live-Experimente\n",
    "\n",
    "Wir kÃ¶nnen mit **ipywidgets** interaktiv verschiedene Filter testen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ® Interaktive Filter-Demo\n",
    "\n",
    "def apply_filter_interactive(filter_type='Original', blur_size=3):\n",
    "    \"\"\"\n",
    "    Interaktive Funktion zum Testen verschiedener Filter\n",
    "    \"\"\"\n",
    "    # Filter auswÃ¤hlen\n",
    "    if filter_type == 'Original':\n",
    "        result = ascent\n",
    "    elif filter_type == 'Blur':\n",
    "        kernel_size = max(3, blur_size)\n",
    "        if kernel_size % 2 == 0:  # Kernel muss ungerade sein\n",
    "            kernel_size += 1\n",
    "        blur_kernel = np.ones((kernel_size, kernel_size)) / (kernel_size**2)\n",
    "        result = signal.convolve2d(ascent, blur_kernel, boundary='symm', mode='same')\n",
    "    elif filter_type == 'Edge X':\n",
    "        result = np.abs(signal.convolve2d(ascent, sobel_x, boundary='symm', mode='same'))\n",
    "    elif filter_type == 'Edge Y':\n",
    "        result = np.abs(signal.convolve2d(ascent, sobel_y, boundary='symm', mode='same'))\n",
    "    elif filter_type == 'Sharpen':\n",
    "        result = signal.convolve2d(ascent, sharpen, boundary='symm', mode='same')\n",
    "        result = np.clip(result, 0, 255)  # Werte begrenzen\n",
    "    elif filter_type == 'Laplace':\n",
    "        result = signal.convolve2d(ascent, laplace, boundary='symm', mode='same')\n",
    "        result = np.abs(result)  # Absolutwerte fÃ¼r bessere Sichtbarkeit\n",
    "    \n",
    "    # Visualisierung\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(ascent, cmap='gray')\n",
    "    plt.title('ğŸ”ï¸ Original', fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(result, cmap='gray')\n",
    "    plt.title(f'{filter_type} Filter', fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistiken\n",
    "    print(f\"ğŸ“Š {filter_type} Filter Statistiken:\")\n",
    "    print(f\"   Min: {result.min():.1f}, Max: {result.max():.1f}\")\n",
    "    print(f\"   Mean: {result.mean():.1f}, Std: {result.std():.1f}\")\n",
    "\n",
    "# Widget erstellen\n",
    "filter_widget = interact(\n",
    "    apply_filter_interactive,\n",
    "    filter_type=widgets.Dropdown(\n",
    "        options=['Original', 'Blur', 'Edge X', 'Edge Y', 'Sharpen', 'Laplace'],\n",
    "        value='Original',\n",
    "        description='ğŸ”§ Filter:'\n",
    "    ),\n",
    "    blur_size=widgets.IntSlider(\n",
    "        value=3, min=3, max=15, step=2,\n",
    "        description='Blur Size:'\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"ğŸ® Interaktive Filter-Demo geladen!\")\n",
    "print(\"ğŸ’¡ Tipp: Probieren Sie verschiedene Filter und Blur-GrÃ¶ÃŸen aus!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some 3x3 filter matrices used in computer vision\n",
    "\n",
    "# Mean-filter\n",
    "mean_3x3 = (1/9) * np.ones([3, 3])\n",
    "mean_9x9 = (1/81) * np.ones([9, 9])  # StÃ¤rkerer Blur-Effekt\n",
    "\n",
    "# Approximation of the gradient\n",
    "# Prewitt Filter - findet Kanten in x/y Richtung\n",
    "prewitt_x = np.array([[-1, 0, 1], \n",
    "                      [-1, 0, 1], \n",
    "                      [-1, 0, 1]])\n",
    "\n",
    "prewitt_y = np.array([[-1, -1, -1], \n",
    "                      [ 0,  0,  0], \n",
    "                      [ 1,  1,  1]])\n",
    "\n",
    "# Sobel Filter - bessere Kantendetection (gewichtet)\n",
    "sobel_x = np.array([[-1, 0, 1], \n",
    "                    [-2, 0, 2], \n",
    "                    [-1, 0, 1]])\n",
    "\n",
    "sobel_y = np.array([[-1, -2, -1], \n",
    "                    [ 0,  0,  0], \n",
    "                    [ 1,  2,  1]])\n",
    "\n",
    "# Second-order derivation \n",
    "# Laplace Filter - hebt Details hervor\n",
    "laplace = np.array([[ 0, -1,  0], \n",
    "                    [-1,  4, -1], \n",
    "                    [ 0, -1,  0]])\n",
    "\n",
    "# Enhanced Sharpening\n",
    "sharpen = np.array([[ 0, -1,  0], \n",
    "                    [-1,  5, -1], \n",
    "                    [ 0, -1,  0]])\n",
    "\n",
    "# ğŸ“Š Filter visualisieren\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "filters = [\n",
    "    (mean_3x3, \"ğŸŒ«ï¸ Blur (3x3)\"),\n",
    "    (prewitt_x, \"âš¡ Prewitt X\"),\n",
    "    (prewitt_y, \"âš¡ Prewitt Y\"), \n",
    "    (sobel_x, \"âš¡ Sobel X\"),\n",
    "    (sobel_y, \"âš¡ Sobel Y\"),\n",
    "    (laplace, \"ğŸ” Laplace\"),\n",
    "    (sharpen, \"âœ¨ Sharpen\"),\n",
    "    (mean_9x9[:3,:3], \"ğŸŒ«ï¸ Blur (9x9)\")  # Zeige nur 3x3 Ausschnitt\n",
    "]\n",
    "\n",
    "for i, (filter_matrix, title) in enumerate(filters):\n",
    "    row, col = i // 4, i % 4\n",
    "    im = axes[row, col].imshow(filter_matrix, cmap='RdBu', vmin=-2, vmax=2)\n",
    "    axes[row, col].set_title(title, fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Werte in die Zellen schreiben\n",
    "    for (j, k), val in np.ndenumerate(filter_matrix):\n",
    "        axes[row, col].text(k, j, f'{val:.1f}', ha='center', va='center', \n",
    "                           color='white' if abs(val) > 1 else 'black', fontweight='bold')\n",
    "    \n",
    "    axes[row, col].set_xticks([])\n",
    "    axes[row, col].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Filter erfolgreich definiert!\")\n",
    "print(\"\\nğŸ’¡ Wie Filter funktionieren:\")\n",
    "print(\"1. Filter-Matrix wird Ã¼ber Bild 'gefaltet' (convolved)\")  \n",
    "print(\"2. Jeder Pixel wird durch gewichtete Summe seiner Nachbarn ersetzt\")\n",
    "print(\"3. Verschiedene Filter erkennen verschiedene Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸŒ«ï¸ BLUR FILTER - Mean Filter anwenden\n",
    "\n",
    "# Verschiedene Blur-StÃ¤rken testen\n",
    "blur_light = signal.convolve2d(ascent, mean_3x3, boundary='symm', mode='same')\n",
    "blur_heavy = signal.convolve2d(ascent, mean_9x9, boundary='symm', mode='same')\n",
    "\n",
    "# Moderne Vergleichsvisualisierung\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "images = [\n",
    "    (ascent, \"ğŸ”ï¸ Original\", \"gray\"),\n",
    "    (blur_light, \"ğŸŒ«ï¸ Blur (3x3)\", \"gray\"),  \n",
    "    (blur_heavy, \"ğŸŒ«ï¸ Blur (9x9)\", \"gray\")\n",
    "]\n",
    "\n",
    "for i, (img, title, cmap) in enumerate(images):\n",
    "    axes[i].imshow(img, cmap=cmap, interpolation='nearest')\n",
    "    axes[i].set_title(title, fontsize=14, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "    \n",
    "    # Info-Text hinzufÃ¼gen\n",
    "    axes[i].text(10, 30, f\"Min: {img.min():.0f}\\nMax: {img.max():.0f}\\nMean: {img.mean():.1f}\", \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8),\n",
    "                fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ğŸ“Š Detailvergleich - Bildbereich vergrÃ¶ÃŸern\n",
    "region = slice(200, 300), slice(200, 300)  # 100x100 Pixel Ausschnitt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i, (img, title, cmap) in enumerate(images):\n",
    "    axes[i].imshow(img[region], cmap=cmap, interpolation='nearest')\n",
    "    axes[i].set_title(f\"{title} - Detail\", fontsize=12)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(\"ğŸ” Detailvergleich: Blur-Effekt\", fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“ Mean Filter Analyse:\")\n",
    "print(\"â€¢ 3x3 Filter: Leichter Blur-Effekt, Details bleiben erkennbar\")\n",
    "print(\"â€¢ 9x9 Filter: Starker Blur-Effekt, viele Details gehen verloren\")\n",
    "print(\"â€¢ Anwendung: RauschunterdrÃ¼ckung, kÃ¼nstlerische Effekte\")\n",
    "print(\"\\nğŸ’¡ In CNNs: Das Netzwerk lernt automatisch, welche 'Blur-StÃ¤rke' optimal ist!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie Sie sehen kÃ¶nnen, wird dadurch das Bild unscharf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prewitt-Filter\n",
    "\n",
    "Das Ziel des Prewitt-Filters ist, den Pixel-Wert durch seine Ableitung, d.h. die Ã„nderung im Farbwert, zu ersetzen. Auch hier kommt eine $9\\times 9$-Nachbarschaft zum Einsatz. Da Bilder i.d.R. 2-dimensional sind (x- und y-Achse), kann die Ableitung sowohl der einen als auch der anderen Dimension berechnet werden. Praktisch bedeutet dies, dass es zwei Prewitt-Filter gibt, einen in x- und einen in y-Richtung. AuÃŸerdem korrespondieren grÃ¶ÃŸere Ã„nderungen im Pixelwert (d.h. der Wert der Ableitung ist grÃ¶ÃŸer) mit Kanten im Bild, weshalb die Prewitt-Filter auch als Kantendetektoren bezeichnet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ascent_x_prew = signal.convolve2d(ascent, prewitt_x, boundary='symm', mode='same')\n",
    "ascent_x_prew = np.absolute(ascent_x_prew)\n",
    "fig, ax = plt.subplots(figsize=(figure_inches, figure_inches))\n",
    "ax.set_title('Ergebnis nach Anwendung des Prewitt-Filters in x-Richtung', fontsize = 15)\n",
    "ax.imshow(ascent_x_prew, interpolation='nearest', cmap='gray')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ascent_y_prew = signal.convolve2d(ascent, prewitt_y, boundary='symm', mode='same')\n",
    "ascent_y_prew = np.absolute(ascent_y_prew)\n",
    "fig, ax = plt.subplots(figsize=(figure_inches, figure_inches))\n",
    "ax.set_title('Ergebnis nach Anwendung des Prewitt-Filters in y-Richtung', fontsize = 15)\n",
    "ax.imshow(ascent_y_prew, interpolation='nearest', cmap='gray')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auf den beiden Bildern sieht man, dass die entsprechenden Kanten in entweder x- oder y-Richtung extrahiert wurden. In der praktischen Anwendung, mÃ¶chte man aber meist alle Kanten extrahieren - unabhÃ¤ngig von der Richtung. In diesem Fall werden schlichtweg die beiden Bilder addiert und anschlieÃŸend durch 2 geteilt. Das Teilen durch 2 ist notwendig, damit die Pixelwert im Bereich 0 bis 255 verbleiben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_y_prew = (ascent_y_prew + ascent_x_prew) / 2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(figure_inches, figure_inches))\n",
    "ax.set_title('Ergebnis nach Anwendung des Prewitt-Filters in x- & y-Richtung', fontsize = 15)\n",
    "ax.imshow(x_y_prew, interpolation='nearest', cmap='gray')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 5.1.1:</b> Welcher lineare Prewitt-Filter wird benÃ¶tigt, um horizontale Kanten hervorzuheben? \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gesichtserkennung\n",
    "\n",
    "Im folgenden wird die im Video erwÃ¤hnte Gesichtserkennung skizziert.\n",
    "\n",
    "Zu Beginn erstmal ein Bild mit Gesichtern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "img = Image.open('images/Gesichter.jpg')\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kann mithilfe einer Gesichtserkennung, die ensprechenden Bounding Boxen extrahiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('images/Gesichtserkennung_1_Bounding_Boxes.png')\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und abschlieÃŸend die Landmarks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('images/Gesichtserkennung_2_Landmarks.png')\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kÃ¶nnen die Gesichter extrahiert werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = './images/landmarks/face/'\n",
    "for file in os.listdir(path):\n",
    "    if \".png\" in file:\n",
    "        img = Image.open(os.path.join(path, file))\n",
    "        plt.figure()\n",
    "        plt.imshow(img)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kÃ¶nnen die Landmarken extrahiert werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = './images/landmarks/landmarks/'\n",
    "for file in os.listdir(path):\n",
    "    if \".png\" in file:\n",
    "        img = Image.open(os.path.join(path, file))\n",
    "        plt.figure()\n",
    "        plt.imshow(img)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und zuletzt werden die Gesichter transformiert, sodass alle gleich groÃŸe (96x96 Pixel) sind und mittig liegen. Somit kÃ¶nnen die Landmarken (z.B. Augen) immer an der gleichen Stelle im Bild auftauchen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = './images/landmarks/transformed/'\n",
    "for file in os.listdir(path):\n",
    "    if \".png\" in file:\n",
    "        img = Image.open(os.path.join(path, file))\n",
    "        plt.figure()\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ Von klassischen Filtern zu modernen CNNs\n",
    "\n",
    "### ğŸ’¡ Der Durchbruch: Lernbare Filter\n",
    "\n",
    "Wir haben gesehen, wie **handgeschriebene Filter** funktionieren. CNNs machen dasselbe - nur **automatisch**!\n",
    "\n",
    "### ğŸ—ï¸ Modernes CNN mit TensorFlow/Keras\n",
    "\n",
    "Lassen Sie uns ein einfaches CNN fÃ¼r MNIST-Ziffenerkennung erstellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¤– Modernes CNN mit TensorFlow/Keras\n",
    "\n",
    "# MNIST Daten laden\n",
    "print(\"ğŸ“¥ MNIST Dataset laden...\")\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Daten vorbereiten\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"âœ… Training: {x_train.shape}, Test: {x_test.shape}\")\n",
    "\n",
    "# CNN Modell erstellen\n",
    "model = keras.Sequential([\n",
    "    # 1. Convolutional Block\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), name='conv1'),\n",
    "    layers.MaxPooling2D((2, 2), name='pool1'),\n",
    "    \n",
    "    # 2. Convolutional Block  \n",
    "    layers.Conv2D(64, (3, 3), activation='relu', name='conv2'),\n",
    "    layers.MaxPooling2D((2, 2), name='pool2'),\n",
    "    \n",
    "    # 3. Dense Layers\n",
    "    layers.Flatten(name='flatten'),\n",
    "    layers.Dense(128, activation='relu', name='dense1'),\n",
    "    layers.Dropout(0.2, name='dropout'),\n",
    "    layers.Dense(10, activation='softmax', name='output')\n",
    "])\n",
    "\n",
    "# Modell kompilieren\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Modell-Architektur anzeigen\n",
    "print(\"\\nğŸ—ï¸ CNN Architektur:\")\n",
    "model.summary()\n",
    "\n",
    "# Visualisierung der Architektur\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True, \n",
    "                          to_file='cnn_architecture.png', dpi=150)\n",
    "\n",
    "# Plot anzeigen\n",
    "try:\n",
    "    from IPython.display import Image as IPImage\n",
    "    display(IPImage('cnn_architecture.png'))\n",
    "except:\n",
    "    print(\"ğŸ“Š Architektur in 'cnn_architecture.png' gespeichert\")\n",
    "\n",
    "print(\"\\nğŸ’¡ CNN vs. Fully-Connected Vergleich:\")\n",
    "print(\"ğŸ”¹ CNN Parameter: {:,}\".format(model.count_params()))\n",
    "\n",
    "# Vergleich mit Fully-Connected\n",
    "fc_params = (28*28) * 128 + 128 * 10  # Nur erste und letzte Schicht\n",
    "print(f\"ğŸ”¹ Equivalent FC Parameter: {fc_params:,}\")\n",
    "print(f\"ğŸš€ Parameter-Reduktion: {(1 - model.count_params()/fc_params)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ CNN Training (Quick Demo)\n",
    "\n",
    "print(\"ğŸƒâ€â™‚ï¸ Schnelles Training (1 Epoche fÃ¼r Demo)...\")\n",
    "\n",
    "# Kleine Stichprobe fÃ¼r schnelles Training\n",
    "x_sample = x_train[:1000]\n",
    "y_sample = y_train[:1000]\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    x_sample, y_sample,\n",
    "    epochs=1,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Test auf paar Beispielen\n",
    "test_loss, test_acc = model.evaluate(x_test[:100], y_test[:100], verbose=0)\n",
    "print(f\"\\nğŸ“Š Test Accuracy (100 Samples): {test_acc:.3f}\")\n",
    "\n",
    "print(\"âœ… CNN erfolgreich trainiert!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” Feature Maps Visualisierung - Das Herz von CNNs!\n",
    "\n",
    "def visualize_feature_maps(model, input_image, layer_names=['conv1', 'conv2']):\n",
    "    \"\"\"\n",
    "    Visualisiert Feature Maps von CNN Layern\n",
    "    \"\"\"\n",
    "    # Model fÃ¼r Feature Extraction erstellen\n",
    "    layer_outputs = [model.get_layer(name).output for name in layer_names]\n",
    "    activation_model = keras.Model(inputs=model.input, outputs=layer_outputs)\n",
    "    \n",
    "    # Feature Maps berechnen\n",
    "    activations = activation_model.predict(input_image[np.newaxis, ...])\n",
    "    \n",
    "    for layer_idx, (layer_name, activation) in enumerate(zip(layer_names, activations)):\n",
    "        # Anzahl Feature Maps\n",
    "        n_features = activation.shape[-1]\n",
    "        n_cols = 8\n",
    "        n_rows = n_features // n_cols + (1 if n_features % n_cols else 0)\n",
    "        \n",
    "        # Plot erstellen\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 2*n_rows))\n",
    "        fig.suptitle(f'ğŸ” Feature Maps - {layer_name.upper()} Layer', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for i in range(n_features):\n",
    "            row, col = i // n_cols, i % n_cols\n",
    "            if n_rows == 1:\n",
    "                ax = axes[col] if n_cols > 1 else axes\n",
    "            else:\n",
    "                ax = axes[row, col]\n",
    "                \n",
    "            # Feature Map anzeigen\n",
    "            feature_map = activation[0, :, :, i]\n",
    "            im = ax.imshow(feature_map, cmap='viridis', interpolation='nearest')\n",
    "            ax.set_title(f'Filter {i+1}', fontsize=10)\n",
    "            ax.axis('off')\n",
    "        \n",
    "        # Leere Subplots verstecken\n",
    "        for i in range(n_features, n_rows * n_cols):\n",
    "            row, col = i // n_cols, i % n_cols\n",
    "            if n_rows == 1:\n",
    "                ax = axes[col] if n_cols > 1 else axes\n",
    "            else:\n",
    "                ax = axes[row, col]\n",
    "            ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"ğŸ“Š {layer_name} Layer: {activation.shape} -> {n_features} Feature Maps\")\n",
    "\n",
    "# Beispielbild auswÃ¤hlen\n",
    "test_image = x_test[0]  # Erste Testziffer\n",
    "\n",
    "# Original Bild anzeigen\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(test_image.squeeze(), cmap='gray')\n",
    "plt.title('ğŸ”¢ Original MNIST Ziffer', fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Vorhersage\n",
    "prediction = model.predict(test_image[np.newaxis, ...])\n",
    "predicted_class = np.argmax(prediction)\n",
    "confidence = np.max(prediction) * 100\n",
    "\n",
    "print(f\"ğŸ¯ Vorhersage: Ziffer {predicted_class} (Confidence: {confidence:.1f}%)\")\n",
    "\n",
    "# Feature Maps visualisieren\n",
    "print(\"\\nğŸ” Feature Maps Analyse:\")\n",
    "print(\"Diese zeigen, was das CNN 'sieht' in jedem Layer...\")\n",
    "visualize_feature_maps(model, test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ® Streamlit Integration\n",
    "\n",
    "### ğŸ’» Interaktive CNN Filter App\n",
    "\n",
    "Sie kÃ¶nnen die **Streamlit App** parallel zu diesem Notebook ausfÃ¼hren:\n",
    "\n",
    "```bash\n",
    "# Im Terminal ausfÃ¼hren:\n",
    "cd 06_Computer_Vision_NLP\n",
    "streamlit run 06_01_streamlit_cnn_filter.py\n",
    "```\n",
    "\n",
    "### ğŸŒŸ Features der App:\n",
    "- âœ… **Interaktive Filter-Tests** mit eigenen Bildern\n",
    "- âœ… **Live Parameter-Anpassung** \n",
    "- âœ… **Filter-Kernel Visualisierung**\n",
    "- âœ… **Bildstatistiken** in Echtzeit\n",
    "- âœ… **Download-Funktion** fÃ¼r gefilterte Bilder\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Zusammenfassung & Key Takeaways\n",
    "\n",
    "### âœ… Was Sie gelernt haben:\n",
    "\n",
    "#### 1. ğŸ” **Klassische Bildfilter**\n",
    "- **Mean Filter:** Blur-Effekte durch Mittelwertbildung\n",
    "- **Sobel/Prewitt:** Kanten-Detection durch Gradient-Berechnung  \n",
    "- **Laplace:** Detail-Enhancement durch 2. Ableitung\n",
    "- **Custom Filter:** Eigene 3x3 Kernel erstellen\n",
    "\n",
    "#### 2. ğŸ§  **CNN Grundkonzepte**\n",
    "- **Local Connectivity:** Nur lokale Pixel-Verbindungen\n",
    "- **Weight Sharing:** Gleicher Filter Ã¼ber ganzes Bild\n",
    "- **Translation Invariance:** PositionsunabhÃ¤ngige Erkennung\n",
    "- **Hierarchical Learning:** Von einfach zu komplex\n",
    "\n",
    "#### 3. ğŸš€ **Praktische CNN Implementation**\n",
    "- **TensorFlow/Keras** fÃ¼r moderne CNNs\n",
    "- **Feature Maps** visualisieren und verstehen\n",
    "- **Parameter-Effizienz** vs. Fully-Connected Networks\n",
    "- **MNIST Klassifikation** als Proof-of-Concept\n",
    "\n",
    "### ğŸ’¡ **Verbindung Filter â†” CNN:**\n",
    "\n",
    "```python\n",
    "# Klassischer Filter (handgeschrieben)\n",
    "sobel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])\n",
    "filtered = signal.convolve2d(image, sobel_x)\n",
    "\n",
    "# CNN Filter (automatisch gelernt)\n",
    "conv_layer = layers.Conv2D(32, (3, 3), activation='relu')\n",
    "# Das Netzwerk lernt 32 verschiedene 3x3 Filter automatisch!\n",
    "```\n",
    "\n",
    "### ğŸ¯ **NÃ¤chste Schritte:**\n",
    "\n",
    "In den kommenden Notebooks vertiefen wir:\n",
    "- **06.2:** Computer Vision Anwendungen (Objekterkennung, Segmentierung)\n",
    "- **06.3:** Data Augmentation fÃ¼r bessere Modelle\n",
    "- **06.4:** Transfer Learning mit vortrainierten Modellen\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ† **Challenge fÃ¼r Sie:**\n",
    "\n",
    "1. **ğŸ“Š Experimentieren Sie** mit der Streamlit App - probieren Sie verschiedene Filter!\n",
    "2. **ğŸ”§ Modifizieren Sie** das CNN-Modell (mehr Layer, andere Aktivierungen)\n",
    "3. **ğŸ“¸ Testen Sie** eigene Bilder mit den klassischen Filtern\n",
    "4. **ğŸ¤” Ãœberlegen Sie:** Welche Filter wÃ¼rde ein CNN fÃ¼r Ihr Anwendungsgebiet lernen?\n",
    "\n",
    "**ğŸ’ª Advanced:** Implementieren Sie einen eigenen Filter in der Streamlit App!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python-amalea"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
