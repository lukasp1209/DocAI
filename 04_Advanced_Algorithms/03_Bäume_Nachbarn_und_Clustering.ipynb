{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38e16a4d",
   "metadata": {},
   "source": [
    "# ðŸŒ³ Woche 3: BÃ¤ume, Nachbarn und Clustering â€“ AMALEA Kernkonzepte\n",
    "\n",
    "> ðŸš€ **Motivation:**\n",
    " >\n",
    "> In dieser Woche tauchst du in die drei wichtigsten Algorithmen des Machine Learning ein â€“ verstÃ¤ndlich, praxisnah und direkt anwendbar fÃ¼r dein Portfolio!\n",
    "\n",
    "> ðŸ’¡ **Warum lohnt sich das?**\n",
    "- Wer Decision Trees, KNN und K-Means versteht, kann 80% aller ML-Projekte meistern.\n",
    "- Du kannst eigene ML-Apps bauen und erklÃ¤ren â€“ ein echter Pluspunkt fÃ¼r Bewerbungen.\n",
    "- Du sammelst praktische Erfahrung mit Tools, die in der Data-Science-Praxis Standard sind.\n",
    "\n",
    "> ðŸ“š **Glossar-Tipp:** Unklare Begriffe? Schau ins [Glossar](../../01_Python_Grundlagen/02_Glossar_Alle_Begriffe_erklÃ¤rt.ipynb) â€“ dort findest du alle wichtigen ErklÃ¤rungen!\n",
    "\n",
    "**Was erwartet dich in dieser Datei?**\n",
    "- Integration der ursprÃ¼nglichen AMALEA-Notebooks:\n",
    "  - \"Willkommen in der Baumschule!\" â†’ Decision Trees\n",
    "  - \"SchÃ¶ne Nachbarschaft\" â†’ K-Nearest Neighbors\n",
    "  - \"K-Means-Clustering\" â†’ Unsupervised Learning\n",
    "- Theoretische Grundlagen und praktische Umsetzung\n",
    "- Streamlit-Apps und Visualisierungen fÃ¼r alle drei Algorithmen\n",
    "- Portfolio-Tipps und weiterfÃ¼hrende Lernpfade\n",
    "\n",
    "## ðŸ“š Was du heute lernst\n",
    "\n",
    "- **Decision Trees** ðŸŒ³ â€“ Wie Computer Entscheidungen treffen\n",
    "- **K-Nearest Neighbors (KNN)** ðŸ‘¥ â€“ Lernen von den Nachbarn\n",
    "- **K-Means Clustering** ðŸŽ¯ â€“ Gruppen in Daten finden\n",
    "- **Supervised vs. Unsupervised Learning** unterscheiden\n",
    "- **Streamlit-Apps** fÃ¼r alle drei Algorithmen erstellen\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¬ ErgÃ¤nzende Videos: Advanced Algorithms\n",
    "\n",
    "**ðŸ“¼ Original AMALEA Video-Serie (KIT 2021):**\n",
    "\n",
    "- **Video 1:** `../Kurs-Videos/amalea-kit2021-w3v2 (1080p).mp4` â€“ Willkommen in der Baumschule! (Decision Trees)\n",
    "- **Video 2:** `../Kurs-Videos/amalea-kit2021-w3v3 (1080p).mp4` â€“ SchÃ¶ne Nachbarschaft (K-Nearest Neighbors)\n",
    "- **Video 3:** `../Kurs-Videos/amalea-kit2021-w3v4 (1080p).mp4` â€“ K-Means Clustering\n",
    "\n",
    "ðŸ’¡ **Tipp:** Diese drei Algorithmen sind die \"Big 3\" des Machine Learning â€“ wenn du sie verstehst, bist du fÃ¼r 80% aller ML-Projekte gerÃ¼stet!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Kernkonzepte aus dem ursprÃ¼nglichen AMALEA-Kurs\n",
    "\n",
    "### Decision Trees ðŸŒ³\n",
    "> **Idee:** Wie Menschen Entscheidungen treffen â€“ durch eine Serie von Ja/Nein-Fragen\n",
    "\n",
    "**Beispiel aus dem ursprÃ¼nglichen Kurs:**\n",
    "```\n",
    "Ist es sonnig?\n",
    "â”œâ”€ JA â†’ Gehe spazieren\n",
    "â””â”€ NEIN â†’ Ist es regnerisch?\n",
    "    â”œâ”€ JA â†’ Bleibe zu Hause\n",
    "    â””â”€ NEIN â†’ Gehe joggen\n",
    "```\n",
    "\n",
    "**Vorteile:**\n",
    "- âœ… Leicht interpretierbar\n",
    "- âœ… Keine Daten-Normalisierung nÃ¶tig\n",
    "- âœ… Arbeitet mit kategorialen und numerischen Daten\n",
    "\n",
    "**Nachteile:**\n",
    "- âŒ Kann zu Overfitting neigen\n",
    "- âŒ Instabil bei kleinen DatenÃ¤nderungen\n",
    "\n",
    "### K-Nearest Neighbors (KNN) ðŸ‘¥\n",
    "> **Idee:** \"Sage mir, wer deine Nachbarn sind, und ich sage dir, wer du bist.\"\n",
    "\n",
    "**Funktionsweise:**\n",
    "1. Finde die k nÃ¤chsten Nachbarn\n",
    "2. Schaue, welche Klasse am hÃ¤ufigsten ist\n",
    "3. Treffe Vorhersage basierend auf Mehrheit\n",
    "\n",
    "**Parameter k:**\n",
    "- k=1: Sehr flexibel, aber anfÃ¤llig fÃ¼r AusreiÃŸer\n",
    "- k=groÃŸ: Glatter, aber weniger Details\n",
    "- k=ungerade: Vermeidet Unentschieden\n",
    "\n",
    "### K-Means Clustering ðŸŽ¯\n",
    "> **Idee:** Finde natÃ¼rliche Gruppen in den Daten (ohne Labels!)\n",
    "\n",
    "**Unterschied zu Supervised Learning:**\n",
    "- **Supervised** (Decision Trees, KNN): Haben Labels/Targets\n",
    "- **Unsupervised** (K-Means): Keine Labels, finde Muster selbst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4030d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“¦ Installation und Imports\n",
    "!pip install scikit-learn matplotlib seaborn plotly streamlit\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris, load_wine, make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, silhouette_score, adjusted_rand_score\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Alle Pakete erfolgreich installiert!\")\n",
    "print(\"ðŸŽ¯ Bereit fÃ¼r die 'Big 3' des Machine Learning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŒ³ Decision Trees - \"Willkommen in der Baumschule!\" (Praktische Umsetzung)\n",
    "print(\"ðŸŒ³ Decision Trees - Wie Computer Entscheidungen treffen\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Iris-Datensatz laden (der Klassiker!)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(\"ðŸ“Š Datensatz-Info:\")\n",
    "print(f\"- Features: {feature_names}\")\n",
    "print(f\"- Targets: {target_names}\")\n",
    "print(f\"- Samples: {X.shape[0]}\")\n",
    "\n",
    "# Datenaufteilung\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Decision Tree trainieren\n",
    "dt_classifier = DecisionTreeClassifier(max_depth=3, min_samples_split=2, random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen und Evaluation\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nâœ… Accuracy: {accuracy:.2%}\")\n",
    "print(f\"ðŸ“‹ Testsamples: {len(y_test)}\")\n",
    "print(f\"ðŸŽ¯ Richtige Vorhersagen: {sum(y_test == y_pred)}\")\n",
    "\n",
    "# Visualisierung des Entscheidungsbaums\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(dt_classifier, feature_names=feature_names, class_names=target_names, \n",
    "          filled=True, rounded=True, fontsize=10)\n",
    "plt.title(\"ðŸŒ³ Decision Tree Visualisierung - Iris Klassifikation\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Interpretation:\")\n",
    "print(\"- Jeder Knoten zeigt eine Ja/Nein-Frage\")\n",
    "print(\"- BlÃ¤tter zeigen die finale Klassifikation\")\n",
    "print(\"- Der Baum 'lernt' optimale Fragen automatisch!\")\n",
    "print(\"- So treffen Computer 'menschenÃ¤hnliche' Entscheidungen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764ca9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ‘¥ K-Nearest Neighbors - \"SchÃ¶ne Nachbarschaft\" (Praktische Umsetzung)\n",
    "print(\"ðŸ‘¥ K-Nearest Neighbors - Lernen von den Nachbarn\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Daten skalieren (wichtig fÃ¼r KNN!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Verschiedene k-Werte testen\n",
    "k_values = [1, 3, 5, 7, 9, 11, 15]\n",
    "accuracies = []\n",
    "\n",
    "print(\"ðŸ” Testing verschiedene k-Werte:\")\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    y_pred_knn = knn.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred_knn)\n",
    "    accuracies.append(acc)\n",
    "    print(f\"k={k:2d}: Accuracy = {acc:.2%}\")\n",
    "\n",
    "# Bestes k finden\n",
    "best_k = k_values[np.argmax(accuracies)]\n",
    "print(f\"\\nðŸ† Bestes k: {best_k} mit {max(accuracies):.2%} Accuracy\")\n",
    "\n",
    "# Visualisierung der k-Werte Performance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, accuracies, 'bo-', linewidth=2, markersize=8)\n",
    "plt.title('ðŸ‘¥ KNN Performance fÃ¼r verschiedene k-Werte')\n",
    "plt.xlabel('k (Anzahl Nachbarn)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(best_k, color='red', linestyle='--', alpha=0.7, label=f'Bestes k={best_k}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ KNN Learnings:\")\n",
    "print(\"- k=1: Sehr flexibel, aber anfÃ¤llig fÃ¼r Noise\")\n",
    "print(\"- k=groÃŸ: Glatter, aber weniger Details\")\n",
    "print(\"- Skalierung ist WICHTIG bei KNN!\")\n",
    "print(\"- 'Lazy Learning': Kein Training, nur Speichern der Daten\")\n",
    "print(\"- Funktioniert wie menschliche Intuition: 'Ã„hnliche Dinge sind Ã¤hnlich'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7f2967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ K-Means Clustering - Unsupervised Learning in Aktion\n",
    "print(\"ðŸŽ¯ K-Means Clustering - Unsupervised Learning in Aktion\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Clustering-Daten erstellen (wir simulieren \"unbekannte\" Gruppen)\n",
    "X_cluster, y_true = make_blobs(n_samples=300, centers=4, n_features=2, random_state=42, cluster_std=1.2)\n",
    "\n",
    "print(\"ðŸ“Š Clustering-Datensatz:\")\n",
    "print(f\"- Samples: {X_cluster.shape[0]}\")\n",
    "print(f\"- Features: {X_cluster.shape[1]} (fÃ¼r 2D-Visualisierung)\")\n",
    "print(f\"- Wahre Cluster: 4 (aber wir tun so, als wÃ¼ssten wir das nicht!)\")\n",
    "\n",
    "# Elbow-Method: Finde optimales k\n",
    "print(\"\\nðŸ” Elbow-Method: Suche optimales k...\")\n",
    "inertias = []\n",
    "k_range = range(1, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_cluster)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Elbow-Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "plt.title('ðŸŽ¯ Elbow-Method fÃ¼r optimales k')\n",
    "plt.xlabel('Anzahl Cluster (k)')\n",
    "plt.ylabel('Inertia (Within-cluster sum of squares)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(4, color='red', linestyle='--', alpha=0.7, label='Erwartetes k=4')\n",
    "plt.legend()\n",
    "\n",
    "# K-Means mit k=4\n",
    "optimal_k = 4\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans_final.fit_predict(X_cluster)\n",
    "centers = kmeans_final.cluster_centers_\n",
    "\n",
    "# Cluster-Visualisierung\n",
    "plt.subplot(1, 2, 2)\n",
    "colors = ['red', 'blue', 'green', 'purple']\n",
    "for i in range(optimal_k):\n",
    "    cluster_points = X_cluster[cluster_labels == i]\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], \n",
    "                c=colors[i], alpha=0.6, s=50, label=f'Cluster {i+1}')\n",
    "\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', marker='x', \n",
    "            s=200, linewidths=3, label='Zentroide')\n",
    "plt.title('ðŸŽ¯ K-Means Clustering Ergebnis')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# QualitÃ¤tsbewertung\n",
    "silhouette_avg = silhouette_score(X_cluster, cluster_labels)\n",
    "ari_score = adjusted_rand_score(y_true, cluster_labels)\n",
    "\n",
    "print(f\"\\nðŸ“Š Clustering-QualitÃ¤t:\")\n",
    "print(f\"âœ… Silhouette Score: {silhouette_avg:.3f} (hÃ¶her = besser, max=1)\")\n",
    "print(f\"âœ… Adjusted Rand Index: {ari_score:.3f} (1.0 = perfekte Ãœbereinstimmung)\")\n",
    "\n",
    "print(\"\\nðŸ’¡ K-Means Learnings:\")\n",
    "print(\"- Unsupervised: Keine Labels/Targets benÃ¶tigt!\")\n",
    "print(\"- Elbow-Method hilft bei k-Wahl\")\n",
    "print(\"- Zentroide = Mittelpunkt jedes Clusters\")\n",
    "print(\"- Anwendung: Customer Segmentation, Datenexploration\")\n",
    "print(\"- Findet 'versteckte' Muster in den Daten!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda3efc6",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Zusammenfassung: Die \"Big 3\" des Machine Learning\n",
    "\n",
    "Du hast jetzt die drei wichtigsten Algorithmen des Machine Learning kennengelernt - sowohl theoretisch als auch praktisch!\n",
    "\n",
    "### ðŸŒ³ Decision Trees\n",
    "- **StÃ¤rken**: Interpretierbar, keine Skalierung nÃ¶tig, arbeitet mit allen Datentypen\n",
    "- **SchwÃ¤chen**: Overfitting, instabil bei kleinen DatenÃ¤nderungen\n",
    "- **Einsatz**: Wenn Interpretierbarkeit wichtig ist (Medizin, Finanzen)\n",
    "- **AMALEA-Weisheit**: \"Wie Menschen denken - durch Ja/Nein-Fragen\"\n",
    "\n",
    "### ðŸ‘¥ K-Nearest Neighbors\n",
    "- **StÃ¤rken**: Einfach zu verstehen, keine Annahmen Ã¼ber Datenverteilung\n",
    "- **SchwÃ¤chen**: Braucht Skalierung, langsam bei groÃŸen Daten\n",
    "- **Einsatz**: Baseline-Algorithmus, lokale Muster, Empfehlungssysteme\n",
    "- **AMALEA-Weisheit**: \"Sage mir, wer deine Nachbarn sind...\"\n",
    "\n",
    "### ðŸŽ¯ K-Means Clustering\n",
    "- **StÃ¤rken**: Unsupervised, findet versteckte Muster, skalierbar\n",
    "- **SchwÃ¤chen**: k muss vorgegeben werden, nur runde Cluster\n",
    "- **Einsatz**: Datenexploration, Customer Segmentation, Feature Engineering\n",
    "- **AMALEA-Weisheit**: \"Daten sprechen lassen - ohne Labels!\"\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ NÃ¤chste Schritte & Vertiefung\n",
    "\n",
    "### ðŸ“š Empfohlene Reihenfolge:\n",
    "1. **ðŸŽ¬ Videos schauen**: Die Original AMALEA-Videos ergÃ¤nzen perfekt diese praktische Erfahrung\n",
    "2. **ðŸ”¬ Eigene Daten testen**: Lade deine eigenen Datasets und experimentiere\n",
    "3. **ðŸ“Š Streamlit-Apps erstellen**: Baue interaktive Apps fÃ¼r alle drei Algorithmen\n",
    "4. **ðŸ§  Neural Networks**: Weiter zu Woche 4 - Deep Learning!\n",
    "\n",
    "### ðŸ’¡ Pro-Tipps aus den AMALEA-Videos:\n",
    "- **Immer mehrere Algorithmen testen** - jeder hat seine StÃ¤rken\n",
    "- **Cross-Validation verwenden** fÃ¼r robuste Evaluation\n",
    "- **Feature Engineering** kann wichtiger sein als der Algorithmus\n",
    "- **Domain Knowledge** schlÃ¤gt oft komplexe Algorithmen\n",
    "- **Start simple, then complexify** - beginne mit einfachen Modellen\n",
    "\n",
    "### ðŸ”¬ Experimentier-Ideen:\n",
    "- Teste verschiedene `max_depth` Werte bei Decision Trees\n",
    "- Probiere verschiedene Distance Metrics bei KNN\n",
    "- Verwende PCA vor K-Means fÃ¼r hochdimensionale Daten\n",
    "- Kombiniere alle drei: Clustering â†’ Feature Engineering â†’ Classification\n",
    "\n",
    "---\n",
    "\n",
    "> ðŸŽ“ **Original AMALEA-Weisheit**: \"Verstehst du Decision Trees, KNN und K-Means, verstehst du 80% aller ML-Projekte!\"\n",
    "\n",
    "**Herzlichen GlÃ¼ckwunsch! Du bist jetzt bereit fÃ¼r Advanced Machine Learning! ðŸš€**\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“ˆ Was kommt als nÃ¤chstes?\n",
    "- **Woche 4**: Neural Networks & Deep Learning\n",
    "- **Woche 5**: Computer Vision & CNNs  \n",
    "- **Woche 6**: Natural Language Processing & Transformers"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
