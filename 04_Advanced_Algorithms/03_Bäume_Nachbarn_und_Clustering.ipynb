{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38e16a4d",
   "metadata": {},
   "source": [
    "# üå≥ Woche 3: B√§ume, Nachbarn und Clustering - AMALEA Kernkonzepte\n",
    "\n",
    "**Integration der urspr√ºnglichen AMALEA-Notebooks:**\n",
    "- \"Willkommen in der Baumschule!\" ‚Üí Decision Trees\n",
    "- \"Sch√∂ne Nachbarschaft\" ‚Üí K-Nearest Neighbors  \n",
    "- \"K-Means-Clustering\" ‚Üí Unsupervised Learning\n",
    "\n",
    "## üìö Was du heute lernst\n",
    "\n",
    "- **Decision Trees** üå≥ - Wie Computer Entscheidungen treffen\n",
    "- **K-Nearest Neighbors (KNN)** üë• - Lernen von den Nachbarn\n",
    "- **K-Means Clustering** üéØ - Gruppen in Daten finden\n",
    "- **Supervised vs. Unsupervised Learning** unterscheiden\n",
    "- **Streamlit-Apps** f√ºr alle drei Algorithmen erstellen\n",
    "\n",
    "---\n",
    "\n",
    "## üé¨ Erg√§nzende Videos: Advanced Algorithms\n",
    "\n",
    "**üìº Original AMALEA Video-Serie (KIT 2021):**\n",
    "\n",
    "- **Video 1:** `../Kurs-Videos/amalea-kit2021-w3v2 (1080p).mp4` - Willkommen in der Baumschule! (Decision Trees)\n",
    "- **Video 2:** `../Kurs-Videos/amalea-kit2021-w3v3 (1080p).mp4` - Sch√∂ne Nachbarschaft (K-Nearest Neighbors)  \n",
    "- **Video 3:** `../Kurs-Videos/amalea-kit2021-w3v4 (1080p).mp4` - K-Means Clustering\n",
    "\n",
    "üí° **Tipp:** Diese 3 Algorithmen sind die \"Big 3\" des Machine Learning - verstehst du sie, verstehst du 80% aller ML-Projekte!\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Kernkonzepte aus dem urspr√ºnglichen AMALEA-Kurs\n",
    "\n",
    "### Decision Trees üå≥\n",
    "> **Idee**: Wie Menschen Entscheidungen treffen - durch eine Serie von Ja/Nein-Fragen\n",
    "\n",
    "**Beispiel aus dem urspr√ºnglichen Kurs:**\n",
    "```\n",
    "Ist es sonnig?\n",
    "‚îú‚îÄ JA ‚Üí Gehe spazieren\n",
    "‚îî‚îÄ NEIN ‚Üí Ist es regnerisch?\n",
    "    ‚îú‚îÄ JA ‚Üí Bleibe zu Hause\n",
    "    ‚îî‚îÄ NEIN ‚Üí Gehe joggen\n",
    "```\n",
    "\n",
    "**Vorteile:**\n",
    "- ‚úÖ Leicht interpretierbar\n",
    "- ‚úÖ Keine Daten-Normalisierung n√∂tig\n",
    "- ‚úÖ Arbeitet mit kategorialen und numerischen Daten\n",
    "\n",
    "**Nachteile:**\n",
    "- ‚ùå Kann zu Overfitting neigen\n",
    "- ‚ùå Instabil bei kleinen Daten√§nderungen\n",
    "\n",
    "### K-Nearest Neighbors (KNN) üë•\n",
    "> **Idee**: \"Sage mir, wer deine Nachbarn sind, und ich sage dir, wer du bist\"\n",
    "\n",
    "**Funktionsweise:**\n",
    "1. Finde die k n√§chsten Nachbarn\n",
    "2. Schaue, welche Klasse am h√§ufigsten ist\n",
    "3. Treffe Vorhersage basierend auf Mehrheit\n",
    "\n",
    "**Parameter k:**\n",
    "- k=1: Sehr flexibel, aber anf√§llig f√ºr Noise\n",
    "- k=gro√ü: Glatter, aber weniger Details\n",
    "- k=ungerade: Vermeidet Unentschieden\n",
    "\n",
    "### K-Means Clustering üéØ\n",
    "> **Idee**: Finde nat√ºrliche Gruppen in den Daten (ohne Labels!)\n",
    "\n",
    "**Unterschied zu Supervised Learning:**\n",
    "- **Supervised** (Decision Trees, KNN): Haben Labels/Targets\n",
    "- **Unsupervised** (K-Means): Keine Labels, finde Muster selbst"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
