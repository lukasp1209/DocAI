{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸŒ³ Woche 3: BÃ¤ume, Nachbarn und Clustering - AMALEA Kernkonzepte\n",
    "\n",
    "**Integration der ursprÃ¼nglichen AMALEA-Notebooks:**\n",
    "- \"Willkommen in der Baumschule!\" â†’ Decision Trees\n",
    "- \"SchÃ¶ne Nachbarschaft\" â†’ K-Nearest Neighbors  \n",
    "- \"K-Means-Clustering\" â†’ Unsupervised Learning\n",
    "\n",
    "## ğŸ“š Was du heute lernst\n",
    "\n",
    "- **Decision Trees** ğŸŒ³ - Wie Computer Entscheidungen treffen\n",
    "- **K-Nearest Neighbors (KNN)** ğŸ‘¥ - Lernen von den Nachbarn\n",
    "- **K-Means Clustering** ğŸ¯ - Gruppen in Daten finden\n",
    "- **Supervised vs. Unsupervised Learning** unterscheiden\n",
    "- **Streamlit-Apps** fÃ¼r alle drei Algorithmen erstellen\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¬ ErgÃ¤nzende Videos: Advanced Algorithms\n",
    "\n",
    "**ğŸ“¼ Original AMALEA Video-Serie (KIT 2021):**\n",
    "\n",
    "- **Video 1:** `../Kurs-Videos/amalea-kit2021-w3v2 (1080p).mp4` - Willkommen in der Baumschule! (Decision Trees)\n",
    "- **Video 2:** `../Kurs-Videos/amalea-kit2021-w3v3 (1080p).mp4` - SchÃ¶ne Nachbarschaft (K-Nearest Neighbors)  \n",
    "- **Video 3:** `../Kurs-Videos/amalea-kit2021-w3v4 (1080p).mp4` - K-Means Clustering\n",
    "\n",
    "ğŸ’¡ **Tipp:** Diese 3 Algorithmen sind die \"Big 3\" des Machine Learning - verstehst du sie, verstehst du 80% aller ML-Projekte!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¦ Installation und Imports\n",
    "!pip install scikit-learn matplotlib seaborn plotly streamlit\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris, load_wine, make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, silhouette_score, adjusted_rand_score\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Alle Pakete erfolgreich installiert!\")\n",
    "print(\"ğŸ¯ Bereit fÃ¼r die 'Big 3' des Machine Learning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸŒ³ Decision Trees - \"Willkommen in der Baumschule!\"\n",
    "print(\"ğŸŒ³ Decision Trees - Wie Computer Entscheidungen treffen\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Iris-Datensatz laden\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(\"ğŸ“Š Datensatz-Info:\")\n",
    "print(f\"- Features: {feature_names}\")\n",
    "print(f\"- Targets: {target_names}\")\n",
    "print(f\"- Samples: {X.shape[0]}\")\n",
    "\n",
    "# Datenaufteilung\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Decision Tree trainieren\n",
    "dt_classifier = DecisionTreeClassifier(max_depth=3, min_samples_split=2, random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen und Evaluation\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nâœ… Accuracy: {accuracy:.2%}\")\n",
    "print(f\"ğŸ“‹ Testsamples: {len(y_test)}\")\n",
    "print(f\"ğŸ¯ Richtige Vorhersagen: {sum(y_test == y_pred)}\")\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(dt_classifier, feature_names=feature_names, class_names=target_names, filled=True, rounded=True, fontsize=10)\n",
    "plt.title(\"ğŸŒ³ Decision Tree Visualisierung - Iris Klassifikation\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Interpretation:\")\n",
    "print(\"- Jeder Knoten zeigt eine Ja/Nein-Frage\")\n",
    "print(\"- BlÃ¤tter zeigen die finale Klassifikation\")\n",
    "print(\"- Der Baum 'lernt' optimale Fragen automatisch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ‘¥ K-Nearest Neighbors - \"SchÃ¶ne Nachbarschaft\"\n",
    "print(\"ğŸ‘¥ K-Nearest Neighbors - Lernen von den Nachbarn\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Daten skalieren (wichtig fÃ¼r KNN!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Verschiedene k-Werte testen\n",
    "k_values = [1, 3, 5, 7, 9, 11, 15]\n",
    "accuracies = []\n",
    "\n",
    "print(\"ğŸ” Testing verschiedene k-Werte:\")\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    y_pred_knn = knn.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred_knn)\n",
    "    accuracies.append(acc)\n",
    "    print(f\"k={k:2d}: Accuracy = {acc:.2%}\")\n",
    "\n",
    "# Bestes k finden\n",
    "best_k = k_values[np.argmax(accuracies)]\n",
    "print(f\"\\nğŸ† Bestes k: {best_k} mit {max(accuracies):.2%} Accuracy\")\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, accuracies, 'bo-', linewidth=2, markersize=8)\n",
    "plt.title('ğŸ‘¥ KNN Performance fÃ¼r verschiedene k-Werte')\n",
    "plt.xlabel('k (Anzahl Nachbarn)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(best_k, color='red', linestyle='--', alpha=0.7, label=f'Bestes k={best_k}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nï¿½ï¿½ KNN Learnings:\")\n",
    "print(\"- k=1: Sehr flexibel, aber anfÃ¤llig fÃ¼r Noise\")\n",
    "print(\"- k=groÃŸ: Glatter, aber weniger Details\")\n",
    "print(\"- Skalierung ist WICHTIG bei KNN!\")\n",
    "print(\"- 'Lazy Learning': Kein Training, nur Speichern der Daten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ K-Means Clustering - Unsupervised Learning\n",
    "print(\"ï¿½ï¿½ K-Means Clustering - Unsupervised Learning in Aktion\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Clustering-Daten erstellen\n",
    "X_cluster, y_true = make_blobs(n_samples=300, centers=4, n_features=2, random_state=42, cluster_std=1.2)\n",
    "\n",
    "print(\"ğŸ“Š Clustering-Datensatz:\")\n",
    "print(f\"- Samples: {X_cluster.shape[0]}\")\n",
    "print(f\"- Features: {X_cluster.shape[1]} (fÃ¼r 2D-Visualisierung)\")\n",
    "print(f\"- Wahre Cluster: 4 (aber wir tun so, als wÃ¼ssten wir das nicht!)\")\n",
    "\n",
    "# Elbow-Method\n",
    "print(\"\\nğŸ” Elbow-Method: Suche optimales k...\")\n",
    "inertias = []\n",
    "k_range = range(1, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_cluster)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "plt.title('ğŸ¯ Elbow-Method fÃ¼r optimales k')\n",
    "plt.xlabel('Anzahl Cluster (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(4, color='red', linestyle='--', alpha=0.7, label='Erwartetes k=4')\n",
    "plt.legend()\n",
    "\n",
    "# K-Means mit k=4\n",
    "optimal_k = 4\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans_final.fit_predict(X_cluster)\n",
    "centers = kmeans_final.cluster_centers_\n",
    "\n",
    "# Cluster-Visualisierung\n",
    "plt.subplot(1, 2, 2)\n",
    "colors = ['red', 'blue', 'green', 'purple']\n",
    "for i in range(optimal_k):\n",
    "    cluster_points = X_cluster[cluster_labels == i]\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i], alpha=0.6, s=50, label=f'Cluster {i+1}')\n",
    "\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', marker='x', s=200, linewidths=3, label='Zentroide')\n",
    "plt.title('ğŸ¯ K-Means Clustering Ergebnis')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# QualitÃ¤tsbewertung\n",
    "silhouette_avg = silhouette_score(X_cluster, cluster_labels)\n",
    "ari_score = adjusted_rand_score(y_true, cluster_labels)\n",
    "\n",
    "print(f\"\\nğŸ“Š Clustering-QualitÃ¤t:\")\n",
    "print(f\"âœ… Silhouette Score: {silhouette_avg:.3f} (hÃ¶her = besser)\")\n",
    "print(f\"âœ… Adjusted Rand Index: {ari_score:.3f} (1.0 = perfekt)\")\n",
    "\n",
    "print(\"\\nğŸ’¡ K-Means Learnings:\")\n",
    "print(\"- Unsupervised: Keine Labels/Targets benÃ¶tigt!\")\n",
    "print(\"- Elbow-Method hilft bei k-Wahl\")\n",
    "print(\"- Zentroide = Mittelpunkt jedes Clusters\")\n",
    "print(\"- Anwendung: Customer Segmentation, Datenexploration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Zusammenfassung: Die \"Big 3\" des Machine Learning\n",
    "\n",
    "Du hast jetzt die drei wichtigsten Algorithmen des Machine Learning kennengelernt!\n",
    "\n",
    "### ğŸŒ³ Decision Trees\n",
    "- **StÃ¤rken**: Interpretierbar, keine Skalierung nÃ¶tig\n",
    "- **SchwÃ¤chen**: Overfitting, instabil\n",
    "- **Einsatz**: Wenn Interpretierbarkeit wichtig ist\n",
    "\n",
    "### ğŸ‘¥ K-Nearest Neighbors\n",
    "- **StÃ¤rken**: Einfach zu verstehen, no assumptions\n",
    "- **SchwÃ¤chen**: Braucht Skalierung, langsam bei groÃŸen Daten\n",
    "- **Einsatz**: Baseline-Algorithmus, lokale Muster\n",
    "\n",
    "### ğŸ¯ K-Means Clustering\n",
    "- **StÃ¤rken**: Unsupervised, findet versteckte Muster\n",
    "- **SchwÃ¤chen**: k muss vorgegeben werden, nur runde Cluster\n",
    "- **Einsatz**: Datenexploration, Customer Segmentation\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ NÃ¤chste Schritte\n",
    "\n",
    "1. **ğŸ¬ Videos schauen**: Die Original AMALEA-Videos ergÃ¤nzen perfekt diese praktische Erfahrung\n",
    "2. **ğŸ”¬ Eigene Daten testen**: Lade deine eigenen Datasets und experimentiere\n",
    "3. **ğŸ“š Weiterlesen**: Neural Networks & Deep Learning in Woche 4!\n",
    "\n",
    "### ğŸ’¡ Pro-Tipps aus den AMALEA-Videos:\n",
    "- **Immer mehrere Algorithmen testen** - jeder hat seine StÃ¤rken\n",
    "- **Cross-Validation verwenden** fÃ¼r robuste Evaluation\n",
    "- **Feature Engineering** kann wichtiger sein als der Algorithmus\n",
    "- **Domain Knowledge** schlÃ¤gt oft komplexe Algorithmen\n",
    "\n",
    "---\n",
    "\n",
    "> ğŸ“ **Original AMALEA-Weisheit**: \"Verstehst du Decision Trees, KNN und K-Means, verstehst du 80% aller ML-Projekte!\"\n",
    "\n",
    "**Herzlichen GlÃ¼ckwunsch! Du bist jetzt bereit fÃ¼r Advanced Machine Learning! ğŸš€**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
