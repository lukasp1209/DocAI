"""
üß† Neural Network Playground - AMALEA 2025
Interactive Demo integrating all original AMALEA Neural Network concepts

Integration der urspr√ºnglichen AMALEA-Notebooks:
- "Jetzt geht's in die Tiefe" ‚Üí Neural Network Grundlagen
- "Wir trainieren nur bergab" ‚Üí Backpropagation & Optimierung
- "Regression II" ‚Üí Neural Networks f√ºr Regression
- "Classification Softmax" ‚Üí Neural Networks f√ºr Klassifikation
"""

import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from sklearn.datasets import make_regression, make_classification, load_iris, make_circles
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPRegressor, MLPClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, mean_squared_error, confusion_matrix, classification_report
import seaborn as sns

# üéØ Streamlit App Configuration
st.set_page_config(
    page_title="üß† Neural Network Playground - AMALEA 2025",
    page_icon="üß†",
    layout="wide",
    initial_sidebar_state="expanded"
)

# üìä Title and Introduction
st.title("üß† Neural Network Playground")
st.markdown("**Interaktive Demo der AMALEA Neural Network Konzepte**")

# Sidebar f√ºr Navigation
st.sidebar.title("üéõÔ∏è Navigation")
app_mode = st.sidebar.selectbox(
    "W√§hle einen Bereich:",
    ["üß† Einfachstes Neuron", "üìà Aktivierungsfunktionen", "üéØ Regression Demo", 
     "üè∑Ô∏è Klassifikation Demo", "üç¶ Softmax Explorer", "üéÆ Interaktiver Playground"]
)

# ============================================================================
# üß† BEREICH 1: Einfachstes Neuron (aus AMALEA "Jetzt geht's in die Tiefe")
# ============================================================================

if app_mode == "üß† Einfachstes Neuron":
    st.header("üß† Einfachstes Neuron verstehen")
    st.markdown("**Aus dem urspr√ºnglichen AMALEA-Kurs: 'Jetzt geht's in die Tiefe'**")
    
    # Interaktive Parameter
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üéõÔ∏è Neuron-Parameter")
        w = st.slider("Gewicht (w)", -5.0, 5.0, -1.0, 0.1)
        b = st.slider("Bias (b)", -5.0, 5.0, 1.0, 0.1)
        
        # Formel anzeigen
        st.latex(f"f(x) = w \\cdot x + b = {w} \\cdot x + {b}")
        
        # AMALEA Original Info
        st.info("üí° **AMALEA-Original**: Mit w=-1, b=1 ergibt f(5) = -4")
        
    with col2:
        st.subheader("üìä Neuron-Verhalten")
        x_values = np.linspace(-10, 10, 100)
        y_values = w * x_values + b
        
        fig = go.Figure()
        fig.add_trace(go.Scatter(x=x_values, y=y_values, mode='lines', name='Neuron Output'))
        fig.update_layout(
            title="Einfachstes Neuron: f(x) = w*x + b",
            xaxis_title="Input (x)",
            yaxis_title="Output f(x)",
            height=400
        )
        st.plotly_chart(fig, use_container_width=True)
    
    # Beispielberechnung
    st.subheader("üßÆ Beispiel-Berechnung")
    test_input = st.number_input("Teste Input-Wert:", value=5.0)
    result = w * test_input + b
    st.success(f"f({test_input}) = {w} √ó {test_input} + {b} = **{result:.2f}**")

# ============================================================================
# üìà BEREICH 2: Aktivierungsfunktionen (AMALEA Deep Learning Grundlagen)
# ============================================================================

elif app_mode == "üìà Aktivierungsfunktionen":
    st.header("üìà Aktivierungsfunktionen verstehen")
    st.markdown("**Herzst√ºck des Deep Learning - aus den urspr√ºnglichen AMALEA-Notebooks**")
    
    # Funktionen definieren
    def relu(x):
        return np.maximum(0, x)
    
    def sigmoid(x):
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Clip f√ºr numerische Stabilit√§t
    
    def tanh_func(x):
        return np.tanh(x)
    
    def leaky_relu(x, alpha=0.01):
        return np.where(x > 0, x, alpha * x)
    
    # Interaktive Auswahl
    x = np.linspace(-5, 5, 1000)
    
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.subheader("üéõÔ∏è Funktions-Parameter")
        
        show_relu = st.checkbox("ReLU", value=True)
        show_sigmoid = st.checkbox("Sigmoid", value=True)
        show_tanh = st.checkbox("Tanh", value=True)
        show_leaky = st.checkbox("Leaky ReLU", value=False)
        
        if show_leaky:
            alpha = st.slider("Leaky ReLU Œ±", 0.01, 0.3, 0.01, 0.01)
        else:
            alpha = 0.01
    
    with col2:
        st.subheader("üìä Aktivierungsfunktionen")
        
        fig = go.Figure()
        
        if show_relu:
            fig.add_trace(go.Scatter(x=x, y=relu(x), mode='lines', name='ReLU: max(0,x)', line=dict(color='blue')))
        if show_sigmoid:
            fig.add_trace(go.Scatter(x=x, y=sigmoid(x), mode='lines', name='Sigmoid: 1/(1+e^(-x))', line=dict(color='red')))
        if show_tanh:
            fig.add_trace(go.Scatter(x=x, y=tanh_func(x), mode='lines', name='Tanh: tanh(x)', line=dict(color='green')))
        if show_leaky:
            fig.add_trace(go.Scatter(x=x, y=leaky_relu(x, alpha), mode='lines', name=f'Leaky ReLU (Œ±={alpha})', line=dict(color='orange')))
        
        fig.update_layout(
            title="Aktivierungsfunktionen im Vergleich",
            xaxis_title="Input (x)",
            yaxis_title="Output f(x)",
            height=500,
            hovermode='x unified'
        )
        st.plotly_chart(fig, use_container_width=True)
    
    # Verwendungszwecke
    st.subheader("üéØ Wann welche Funktion verwenden?")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        **üîµ ReLU**
        - ‚úÖ Standard f√ºr Hidden Layers
        - ‚úÖ Schnell zu berechnen
        - ‚ùå "Dying ReLU" Problem
        """)
    
    with col2:
        st.markdown("""
        **üî¥ Sigmoid**
        - ‚úÖ Bin√§re Klassifikation (Output)
        - ‚úÖ Werte zwischen 0 und 1
        - ‚ùå Vanishing Gradient Problem
        """)
    
    with col3:
        st.markdown("""
        **üü¢ Tanh**
        - ‚úÖ Werte zwischen -1 und 1
        - ‚úÖ Zero-centered
        - ‚ùå Auch Vanishing Gradient
        """)

# ============================================================================
# üéØ BEREICH 3: Regression Demo (aus AMALEA "Regression II")
# ============================================================================

elif app_mode == "üéØ Regression Demo":
    st.header("üéØ Neural Network Regression")
    st.markdown("**Aus dem urspr√ºnglichen AMALEA-Kurs: 'Regression II - K√ºnstliche Gehirne'**")
    
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.subheader("üéõÔ∏è Daten-Parameter")
        n_samples = st.slider("Anzahl Samples", 50, 500, 200)
        noise_level = st.slider("Noise Level", 0.0, 50.0, 10.0)
        
        st.subheader("üß† Netzwerk-Parameter")
        hidden_layers = st.selectbox("Hidden Layer", [(10,), (20,), (10, 5), (20, 10)])
        activation = st.selectbox("Aktivierung", ['relu', 'tanh', 'logistic'])
        learning_rate = st.selectbox("Learning Rate", [0.001, 0.01, 0.1])
        
        train_button = st.button("üöÄ Trainiere Neural Network!")
    
    with col2:
        if train_button:
            # Daten erstellen
            X, y = make_regression(n_samples=n_samples, n_features=1, noise=noise_level, random_state=42)
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            
            # Skalierung
            scaler_X = StandardScaler()
            scaler_y = StandardScaler()
            X_train_scaled = scaler_X.fit_transform(X_train)
            X_test_scaled = scaler_X.transform(X_test)
            y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()
            
            # Neural Network trainieren
            with st.spinner("Training Neural Network..."):
                nn = MLPRegressor(
                    hidden_layer_sizes=hidden_layers,
                    activation=activation,
                    learning_rate_init=learning_rate,
                    max_iter=1000,
                    random_state=42
                )
                nn.fit(X_train_scaled, y_train_scaled)
                
                # Vorhersagen
                y_pred_scaled = nn.predict(X_test_scaled)
                y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()
            
            # Visualisierung
            fig = go.Figure()
            
            # Trainingsdaten
            fig.add_trace(go.Scatter(
                x=X_train.ravel(), y=y_train, mode='markers', 
                name='Training Data', opacity=0.6, marker=dict(color='blue')
            ))
            
            # Testdaten
            fig.add_trace(go.Scatter(
                x=X_test.ravel(), y=y_test, mode='markers', 
                name='Test Data (True)', marker=dict(color='red')
            ))
            
            # Vorhersagen
            fig.add_trace(go.Scatter(
                x=X_test.ravel(), y=y_pred, mode='markers', 
                name='Predictions', marker=dict(color='green', symbol='x', size=10)
            ))
            
            fig.update_layout(
                title="Neural Network Regression Results",
                xaxis_title="X",
                yaxis_title="y",
                height=500
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # Metriken
            mse = mean_squared_error(y_test, y_pred)
            st.success(f"üéØ Mean Squared Error: **{mse:.2f}**")

# ============================================================================
# üè∑Ô∏è BEREICH 4: Klassifikation Demo (aus AMALEA "Classification Softmax")
# ============================================================================

elif app_mode == "üè∑Ô∏è Klassifikation Demo":
    st.header("üè∑Ô∏è Neural Network Klassifikation")
    st.markdown("**Aus dem urspr√ºnglichen AMALEA-Kurs: 'Classification Softmax-Eis'**")
    
    # Dataset Auswahl
    dataset_choice = st.selectbox(
        "üìä Dataset w√§hlen:",
        ["Iris (AMALEA-Klassiker)", "K√ºnstliche Daten", "Kreise (Non-linear)"]
    )
    
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.subheader("üß† Netzwerk-Konfiguration")
        
        if dataset_choice == "Iris (AMALEA-Klassiker)":
            # Iris Dataset
            iris = load_iris()
            X, y = iris.data, iris.target
            feature_names = iris.feature_names
            target_names = iris.target_names
            
            # Feature Auswahl
            feature_1 = st.selectbox("Feature 1", feature_names, index=0)
            feature_2 = st.selectbox("Feature 2", feature_names, index=1)
            
            X_viz = X[:, [feature_names.index(feature_1), feature_names.index(feature_2)]]
            
        elif dataset_choice == "K√ºnstliche Daten":
            n_samples = st.slider("Anzahl Samples", 100, 1000, 300)
            n_classes = st.slider("Anzahl Klassen", 2, 4, 3)
            X_viz, y = make_classification(
                n_samples=n_samples, n_features=2, n_redundant=0, 
                n_informative=2, n_clusters_per_class=1, n_classes=n_classes,
                random_state=42
            )
            target_names = [f"Klasse {i}" for i in range(n_classes)]
            
        else:  # Kreise
            n_samples = st.slider("Anzahl Samples", 100, 1000, 300)
            noise = st.slider("Noise", 0.0, 0.3, 0.1)
            X_viz, y = make_circles(n_samples=n_samples, noise=noise, random_state=42)
            target_names = ["Innen", "Au√üen"]
        
        # Netzwerk Parameter
        hidden_layers = st.selectbox("Hidden Layers", [(10,), (20,), (10, 5), (50, 20)])
        activation = st.selectbox("Aktivierung", ['relu', 'tanh', 'logistic'])
        solver = st.selectbox("Optimizer", ['adam', 'sgd', 'lbfgs'])
        
        train_classification = st.button("üöÄ Trainiere Klassifikator!")
    
    with col2:
        if train_classification:
            # Daten aufteilen
            X_train, X_test, y_train, y_test = train_test_split(
                X_viz, y, test_size=0.2, stratify=y, random_state=42
            )
            
            # Skalierung
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # Neural Network trainieren
            with st.spinner("Training Classifier..."):
                nn_clf = MLPClassifier(
                    hidden_layer_sizes=hidden_layers,
                    activation=activation,
                    solver=solver,
                    max_iter=1000,
                    random_state=42
                )
                nn_clf.fit(X_train_scaled, y_train)
                y_pred = nn_clf.predict(X_test_scaled)
            
            # Entscheidungsgrenze visualisieren
            h = 0.02
            x_min, x_max = X_viz[:, 0].min() - 1, X_viz[:, 0].max() + 1
            y_min, y_max = X_viz[:, 1].min() - 1, X_viz[:, 1].max() + 1
            xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                               np.arange(y_min, y_max, h))
            
            mesh_points = np.c_[xx.ravel(), yy.ravel()]
            mesh_points_scaled = scaler.transform(mesh_points)
            Z = nn_clf.predict(mesh_points_scaled)
            Z = Z.reshape(xx.shape)
            
            # Plot erstellen
            fig = go.Figure()
            
            # Entscheidungsgrenze
            fig.add_trace(go.Contour(
                x=np.arange(x_min, x_max, h),
                y=np.arange(y_min, y_max, h),
                z=Z,
                showscale=False,
                opacity=0.3,
                colorscale='Viridis'
            ))
            
            # Datenpunkte
            colors = ['red', 'blue', 'green', 'orange']
            for i, target_name in enumerate(target_names):
                mask = y == i
                fig.add_trace(go.Scatter(
                    x=X_viz[mask, 0], y=X_viz[mask, 1],
                    mode='markers', name=target_name,
                    marker=dict(color=colors[i % len(colors)], size=8)
                ))
            
            fig.update_layout(
                title="Neural Network Classification + Decision Boundary",
                xaxis_title="Feature 1",
                yaxis_title="Feature 2",
                height=500
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # Accuracy
            accuracy = accuracy_score(y_test, y_pred)
            st.success(f"üéØ Test Accuracy: **{accuracy:.2%}**")
            
            # Confusion Matrix
            cm = confusion_matrix(y_test, y_pred)
            fig_cm = px.imshow(cm, text_auto=True, aspect="auto",
                             x=target_names[:len(np.unique(y))], 
                             y=target_names[:len(np.unique(y))],
                             title="Confusion Matrix")
            st.plotly_chart(fig_cm, use_container_width=True)

# ============================================================================
# üç¶ BEREICH 5: Softmax Explorer
# ============================================================================

elif app_mode == "üç¶ Softmax Explorer":
    st.header("üç¶ Softmax Explorer")
    st.markdown("**Verstehe 'Softmax-Eis f√ºr einen one-hot day' interaktiv**")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üéõÔ∏è Logits eingeben")
        
        # Anzahl Klassen
        n_classes = st.slider("Anzahl Klassen", 2, 5, 3)
        
        # Logits eingeben
        logits = []
        for i in range(n_classes):
            logit = st.slider(f"Logit Klasse {i}", -5.0, 5.0, 0.0, 0.1, key=f"logit_{i}")
            logits.append(logit)
        
        # Temperature f√ºr Softmax
        temperature = st.slider("Temperature", 0.1, 5.0, 1.0, 0.1)
        
        # Softmax berechnen
        logits_array = np.array(logits) / temperature
        exp_logits = np.exp(logits_array - np.max(logits_array))  # Numerische Stabilit√§t
        softmax_probs = exp_logits / np.sum(exp_logits)
        
        # Predicted Class
        predicted_class = np.argmax(softmax_probs)
        confidence = softmax_probs[predicted_class]
        
        st.subheader("üìä Ergebnisse")
        st.write(f"**Vorhergesagte Klasse:** {predicted_class}")
        st.write(f"**Konfidenz:** {confidence:.1%}")
        st.write(f"**Summe:** {np.sum(softmax_probs):.6f}")
    
    with col2:
        st.subheader("üìà Softmax Visualisierung")
        
        # Bar Chart
        class_names = [f"Klasse {i}" for i in range(n_classes)]
        
        fig = go.Figure(data=[
            go.Bar(
                x=class_names,
                y=softmax_probs,
                text=[f"{p:.3f}" for p in softmax_probs],
                textposition='auto',
            )
        ])
        
        fig.update_layout(
            title="Softmax Wahrscheinlichkeiten",
            yaxis_title="Wahrscheinlichkeit",
            height=400
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Temperature Effect Demo
        st.subheader("üå°Ô∏è Temperature Effekt")
        
        temperatures = [0.5, 1.0, 2.0, 5.0]
        temp_probs = []
        
        for temp in temperatures:
            scaled_logits = np.array(logits) / temp
            exp_scaled = np.exp(scaled_logits - np.max(scaled_logits))
            temp_prob = exp_scaled / np.sum(exp_scaled)
            temp_probs.append(temp_prob)
        
        # Heatmap f√ºr Temperature Effekt
        temp_df = pd.DataFrame(temp_probs, 
                              columns=class_names,
                              index=[f"T={t}" for t in temperatures])
        
        fig_heatmap = px.imshow(temp_df, 
                               text_auto='.3f',
                               aspect="auto",
                               title="Temperature Effekt auf Softmax")
        st.plotly_chart(fig_heatmap, use_container_width=True)
    
    # One-Hot Encoding Demo
    st.subheader("üî• One-Hot Encoding")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**Ground Truth (One-Hot):**")
        true_class = st.selectbox("Wahre Klasse", range(n_classes))
        one_hot = np.zeros(n_classes)
        one_hot[true_class] = 1
        st.write(one_hot)
    
    with col2:
        st.write("**Cross-Entropy Loss:**")
        # Clip f√ºr numerische Stabilit√§t
        clipped_probs = np.clip(softmax_probs, 1e-15, 1 - 1e-15)
        cross_entropy = -np.sum(one_hot * np.log(clipped_probs))
        st.write(f"{cross_entropy:.4f}")
        
        if cross_entropy < 0.1:
            st.success("Excellent prediction! üéØ")
        elif cross_entropy < 1.0:
            st.info("Good prediction üëç")
        else:
            st.warning("Poor prediction üòû")

# ============================================================================
# üéÆ BEREICH 6: Interaktiver Playground
# ============================================================================

else:  # Interaktiver Playground
    st.header("üéÆ Neural Network Playground")
    st.markdown("**Experimentiere mit verschiedenen Architekturen!**")
    
    # Zwei-Spalten Layout
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.subheader("üèóÔ∏è Netzwerk-Architektur")
        
        # Layer Konfiguration
        n_layers = st.slider("Anzahl Hidden Layers", 1, 5, 2)
        
        layers = []
        for i in range(n_layers):
            neurons = st.slider(f"Layer {i+1} Neuronen", 1, 100, 10*(i+1))
            layers.append(neurons)
        
        # Aktivierungsfunktion
        activation = st.selectbox("Aktivierungsfunktion", ['relu', 'tanh', 'logistic'])
        
        # Trainings-Parameter
        st.subheader("üéØ Training")
        learning_rate = st.select_slider("Learning Rate", [0.001, 0.01, 0.1, 1.0])
        max_iter = st.slider("Max Iterations", 100, 2000, 500)
        
        # Problem Type
        problem_type = st.radio("Problem Type", ["Regression", "Classification"])
        
        st.info(f"üß† **Architektur**: {layers}\nüìä **Aktivierung**: {activation}")
    
    with col2:
        st.subheader("üéØ Live Training")
        
        if st.button("üöÄ Starte Training!"):
            if problem_type == "Regression":
                # Regression Problem
                X, y = make_regression(n_samples=300, n_features=1, noise=15, random_state=42)
                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
                
                scaler_X = StandardScaler()
                scaler_y = StandardScaler()
                X_train_scaled = scaler_X.fit_transform(X_train)
                X_test_scaled = scaler_X.transform(X_test)
                y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()
                
                # Model
                model = MLPRegressor(
                    hidden_layer_sizes=tuple(layers),
                    activation=activation,
                    learning_rate_init=learning_rate,
                    max_iter=max_iter,
                    random_state=42
                )
                
                # Training mit Progress
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                model.fit(X_train_scaled, y_train_scaled)
                progress_bar.progress(100)
                status_text.text("Training completed!")
                
                # Vorhersagen
                y_pred_scaled = model.predict(X_test_scaled)
                y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()
                
                # Plot
                fig = go.Figure()
                fig.add_trace(go.Scatter(x=X_test.ravel(), y=y_test, mode='markers', name='True'))
                fig.add_trace(go.Scatter(x=X_test.ravel(), y=y_pred, mode='markers', name='Predicted'))
                fig.update_layout(title="Regression Results", height=400)
                st.plotly_chart(fig, use_container_width=True)
                
                mse = mean_squared_error(y_test, y_pred)
                st.metric("MSE", f"{mse:.2f}")
                
            else:
                # Classification Problem
                X, y = make_classification(n_samples=300, n_features=2, n_redundant=0, 
                                         n_informative=2, n_clusters_per_class=1, random_state=42)
                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
                
                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_test_scaled = scaler.transform(X_test)
                
                # Model
                model = MLPClassifier(
                    hidden_layer_sizes=tuple(layers),
                    activation=activation,
                    learning_rate_init=learning_rate,
                    max_iter=max_iter,
                    random_state=42
                )
                
                # Training
                progress_bar = st.progress(0)
                model.fit(X_train_scaled, y_train)
                progress_bar.progress(100)
                
                y_pred = model.predict(X_test_scaled)
                
                # Plot
                fig = go.Figure()
                colors = ['red', 'blue']
                for i in [0, 1]:
                    mask = y_test == i
                    fig.add_trace(go.Scatter(
                        x=X_test[mask, 0], y=X_test[mask, 1],
                        mode='markers', name=f'Class {i} (True)',
                        marker=dict(color=colors[i])
                    ))
                    
                    mask_pred = y_pred == i
                    fig.add_trace(go.Scatter(
                        x=X_test[mask_pred, 0], y=X_test[mask_pred, 1],
                        mode='markers', name=f'Class {i} (Pred)',
                        marker=dict(color=colors[i], symbol='x', size=10)
                    ))
                
                fig.update_layout(title="Classification Results", height=400)
                st.plotly_chart(fig, use_container_width=True)
                
                accuracy = accuracy_score(y_test, y_pred)
                st.metric("Accuracy", f"{accuracy:.2%}")

# ============================================================================
# Footer
# ============================================================================

st.markdown("---")
st.markdown("**üéì AMALEA 2025 - Neural Networks modernisiert mit Streamlit**")
st.markdown("Alle urspr√ºnglichen AMALEA-Konzepte interaktiv erlebbar!")

# Zus√§tzliche Info in Sidebar
st.sidebar.markdown("---")
st.sidebar.subheader("üìö AMALEA Integration")
st.sidebar.markdown("""
**Urspr√ºngliche Notebooks:**
- üß† "Jetzt geht's in die Tiefe"
- üèîÔ∏è "Wir trainieren nur bergab"  
- üìä "Regression II"
- üç¶ "Classification Softmax"

**Modernisierte Features:**
- ‚úÖ Interaktive Parameter
- ‚úÖ Live-Visualisierung
- ‚úÖ Streamlit-Integration
- ‚úÖ Beginner-freundlich
""")

st.sidebar.markdown("---")
st.sidebar.info("üí° **Tipp**: Experimentiere mit verschiedenen Parametern!")
