{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9befe4d2",
   "metadata": {},
   "source": [
    "# ğŸ¤– Woche 2: Machine Learning in Streamlit - AMALEA modernisiert\n",
    "\n",
    "**Aufbauend auf dem ursprÃ¼nglichen AMALEA-Kurs: \"Maschinelles Lernen und seine Anwendungen\"**\n",
    "\n",
    "## ğŸ“š Was du heute lernst\n",
    "\n",
    "- **ML-Grundlagen** aus dem ursprÃ¼nglichen AMALEA-Kurs verstehen\n",
    "- **Deskriptive vs. PrÃ¤diktive Statistik** (AMALEA-Konzept)\n",
    "- **Training/Test/Validation** Datenaufteilung (AMALEA-Standard)\n",
    "- **ML in Streamlit** integrieren\n",
    "- **Iris-Datensatz** klassifizieren (AMALEA-Klassiker)\n",
    "- **Interaktive ML-Apps** erstellen\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  ML-Grundlagen aus dem ursprÃ¼nglichen AMALEA-Kurs\n",
    "\n",
    "### Definition aus dem ursprÃ¼nglichen Kurs:\n",
    "\n",
    "> *\"Machine Learning at its most basic is the practice of using algorithms to parse data, learn from it, and then make a determination or prediction about something in the world.\"* -- Nvidia\n",
    "\n",
    "### Wichtige Konzepte:\n",
    "\n",
    "**ğŸ“Š Deskriptive Statistik** (Vergangenheit verstehen)\n",
    "- Daten aus vergangenen Ereignissen analysieren\n",
    "- Grundlegende Techniken: Anzahl, Summe, Durchschnitt, Min/Max\n",
    "- **Ziel**: Was ist passiert?\n",
    "\n",
    "**ğŸ”® PrÃ¤diktive Statistik** (Zukunft vorhersagen)\n",
    "- Basierend auf historischen Daten Vorhersagen treffen\n",
    "- Verwendet Modelle und Algorithmen\n",
    "- **Ziel**: Was wird passieren?\n",
    "\n",
    "**ğŸ¤– Machine Learning**\n",
    "- Automatisierter Ansatz fÃ¼r prÃ¤diktive Statistik\n",
    "- Lernt Muster aus Daten statt explizite Regeln\n",
    "- **Ziel**: Bessere Vorhersagen als regelbasierte Systeme\n",
    "\n",
    "### Datenaufteilung (AMALEA-Standard):\n",
    "\n",
    "| Datensatz | Zweck | Anteil |\n",
    "|-----------|-------|--------|\n",
    "| **Training** | Modell-Parameter optimieren | ~60-80% |\n",
    "| **Validation** | Hyperparameter tuning | ~10-20% |\n",
    "| **Test** | Finale Bewertung (nur einmal!) | ~10-20% |\n",
    "\n",
    "> **Wichtig**: Teste nie mit den Trainingsdaten! Das wÃ¤re Betrug!\n",
    "\n",
    "---\n",
    "\n",
    "## Lernziele dieser Woche\n",
    "- Einfache ML-Modelle in Streamlit integrieren\n",
    "- Benutzer-Eingaben fÃ¼r Vorhersagen verwenden\n",
    "- Model Training und Evaluation in der App\n",
    "- Interaktive ML-Demos erstellen\n",
    "\n",
    "## Von Datenanalyse zu Vorhersagen\n",
    "Letzte Woche haben wir gelernt, Daten zu visualisieren. Diese Woche machen wir den nÃ¤chsten Schritt: Wir lassen die App Vorhersagen treffen!\n",
    "\n",
    "### ğŸ§  Was ist Machine Learning?\n",
    "**Machine Learning (ML)** = Computer lernen Muster aus Daten und machen Vorhersagen\n",
    "\n",
    "**Einfaches Beispiel:**\n",
    "- **Daten:** GrÃ¶ÃŸe und Gewicht von 1000 Menschen\n",
    "- **Muster:** GrÃ¶ÃŸere Menschen wiegen meist mehr\n",
    "- **Vorhersage:** Bei neuer Person mit GrÃ¶ÃŸe 180cm â†’ schÃ¤tze Gewicht\n",
    "\n",
    "**Haupttypen:**\n",
    "- **Klassifikation:** Kategorie vorhersagen (z.B. Spam/Nicht-Spam)\n",
    "- **Regression:** Zahlenwert vorhersagen (z.B. Hauspreise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a3f5de",
   "metadata": {},
   "source": [
    "## ğŸ“¦ ZusÃ¤tzliche Pakete fÃ¼r ML\n",
    "\n",
    "**Scikit-learn** = Die wichtigste ML-Bibliothek fÃ¼r Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403ef39a",
   "metadata": {},
   "source": [
    "## ğŸ¬ Video-Serie: Original AMALEA Machine Learning Grundlagen\n",
    "\n",
    "**ğŸ“¼ Diese Video-Serie stammt aus dem Original AMALEA-Kurs (KIT 2021) und ist GOLDSTANDARD fÃ¼r ML-Grundlagen!**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¥ Video 1: Maschinelles Lernen und seine Anwendungen\n",
    "**ğŸ“ Datei:** `../Kurs-Videos/amalea-kit2021-w2v1 (1080p).mp4`  \n",
    "**â±ï¸ Dauer:** ~25 Minuten  \n",
    "**ğŸ“š Inhalte:**\n",
    "- Was ist Machine Learning?\n",
    "- Supervised vs. Unsupervised Learning\n",
    "- Regression vs. Classification\n",
    "- Real-World Anwendungen und Beispiele\n",
    "\n",
    "**ğŸ’¡ Warum du dieses Video schauen solltest:** Perfekte EinfÃ¼hrung in ML-Konzepte, die auch 2025 noch 100% relevant sind!\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¥ Video 2: 100% Genauigkeit - Das muss doch gut sein, oder?\n",
    "**ğŸ“ Datei:** `../Kurs-Videos/amalea-kit2021-w2v2 (1080p).mp4`  \n",
    "**â±ï¸ Dauer:** ~20 Minuten  \n",
    "**ğŸ“š Inhalte:**\n",
    "- Overfitting vs. Underfitting\n",
    "- Training vs. Validation vs. Test Sets\n",
    "- Warum 100% Accuracy oft schlecht ist\n",
    "- Bias-Variance Tradeoff\n",
    "\n",
    "**ğŸ¯ Learning Goal:** Verstehe, warum perfekte Modelle oft die schlechtesten sind!\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¥ Video 3: Oh sorry, das war ein Falsch-Positiv\n",
    "**ğŸ“ Datei:** `../Kurs-Videos/amalea-kit2021-w2v3 (1080p).mp4`  \n",
    "**â±ï¸ Dauer:** ~30 Minuten  \n",
    "**ğŸ“š Inhalte:**\n",
    "- Confusion Matrix verstehen\n",
    "- Precision, Recall, F1-Score\n",
    "- False Positives vs. False Negatives\n",
    "- ROC Curves und AUC\n",
    "- Wann welche Metrik verwenden?\n",
    "\n",
    "**âš ï¸ Praxis-Relevanz:** Diese Konzepte sind ENTSCHEIDEND fÃ¼r echte ML-Projekte!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Empfohlene Reihenfolge:\n",
    "\n",
    "1. **ğŸ“¹ Erst alle 3 Videos schauen** (ca. 1,5 Stunden)\n",
    "2. **ğŸ’» Dann dieses Notebook durcharbeiten** \n",
    "3. **ğŸ”„ Videos bei Bedarf nochmal anschauen**\n",
    "\n",
    "---\n",
    "\n",
    "> ğŸ† **Pro-Tipp:** Diese Original AMALEA Videos sind so gut, dass sie auch an der Stanford University verwendet werden kÃ¶nnten! ğŸ˜‰\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3e674",
   "metadata": {},
   "source": [
    "## ğŸ¬ Original AMALEA Videos: Machine Learning Fundamentals\n",
    "\n",
    "**Diese drei Videos aus dem ursprÃ¼nglichen AMALEA-Kurs sind GOLD! ğŸ’° Schaut sie euch unbedingt an:**\n",
    "\n",
    "### ğŸ“¹ **Video 1: \"Maschinelles Lernen und seine Anwendungen\"**\n",
    "- **Datei:** `../Kurs-Videos/amalea-kit2021-w2v1 (1080p).mp4`\n",
    "- **Dauer:** ~20 Minuten\n",
    "- **Inhalt:** Was ist ML? Supervised vs. Unsupervised Learning, Anwendungsbeispiele\n",
    "- **Warum wichtig:** Perfekte EinfÃ¼hrung in ML-Konzepte\n",
    "\n",
    "### ğŸ“¹ **Video 2: \"100% Genauigkeit, das muss doch gut sein, oder?\"**\n",
    "- **Datei:** `../Kurs-Videos/amalea-kit2021-w2v2 (1080p).mp4`\n",
    "- **Dauer:** ~15 Minuten  \n",
    "- **Inhalt:** Overfitting, Underfitting, Bias-Variance Trade-off\n",
    "- **Warum wichtig:** Versteht, warum perfekte Genauigkeit oft schlecht ist!\n",
    "\n",
    "### ğŸ“¹ **Video 3: \"Oh sorry, das war ein Falsch-Positiv\"**\n",
    "- **Datei:** `../Kurs-Videos/amalea-kit2021-w2v3 (1080p).mp4`\n",
    "- **Dauer:** ~18 Minuten\n",
    "- **Inhalt:** Precision, Recall, F1-Score, Confusion Matrix\n",
    "- **Warum wichtig:** ML-Evaluation wie ein Profi!\n",
    "\n",
    "> **ğŸ’¡ Lerntipp:** Schaut die Videos â†’ dann macht die praktischen Ãœbungen unten. Die Theorie ist zeitlos und perfekt erklÃ¤rt!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installiere ML-Bibliotheken\n",
    "!pip install scikit-learn joblib\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import plotly.express as px\n",
    "import joblib\n",
    "\n",
    "print(\"âœ… ML-Pakete erfolgreich installiert!\")\n",
    "\n",
    "# ğŸ¯ ML-Grundlagen aus dem ursprÃ¼nglichen AMALEA-Kurs\n",
    "# Basiert auf \"Maschinelles Lernen und seine Anwendungen\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"ğŸ¤– Machine Learning Grundlagen - AMALEA modernisiert\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1ï¸âƒ£ Iris-Datensatz laden (AMALEA-Klassiker)\n",
    "print(\"1ï¸âƒ£ Iris-Datensatz laden (bekannt aus dem ursprÃ¼nglichen AMALEA-Kurs):\")\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['species'] = iris.target_names[iris.target]\n",
    "\n",
    "print(f\"ğŸ“Š Datensatz-Info:\")\n",
    "print(f\"- Samples (Zeilen): {df.shape[0]}\")\n",
    "print(f\"- Features (Spalten): {df.shape[1]-1}\")  # -1 fÃ¼r target\n",
    "print(f\"- Klassen: {list(iris.target_names)}\")\n",
    "print(f\"\\nğŸ“‹ Erste 5 Zeilen:\")\n",
    "print(df.head())\n",
    "\n",
    "# 2ï¸âƒ£ Features vs. Target (AMALEA-Konzept)\n",
    "print(f\"\\n2ï¸âƒ£ Features vs. Target identifizieren (AMALEA-Konzept):\")\n",
    "X = iris.data  # Features (Input)\n",
    "y = iris.target  # Target (Output)\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(f\"Features (Input): {feature_names}\")\n",
    "print(f\"Target (Output): Species {target_names}\")\n",
    "\n",
    "# 3ï¸âƒ£ Datenaufteilung (AMALEA-Standard)\n",
    "print(f\"\\n3ï¸âƒ£ Datenaufteilung nach AMALEA-Standard:\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Training: {X_train.shape[0]} Samples ({X_train.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"Test: {X_test.shape[0]} Samples ({X_test.shape[0]/len(X)*100:.0f}%)\")\n",
    "\n",
    "# 4ï¸âƒ£ ML-Algorithmus (Random Forest - war im ursprÃ¼nglichen AMALEA erwÃ¤hnt)\n",
    "print(f\"\\n4ï¸âƒ£ Random Forest Algorithmus (aus AMALEA-Kurs):\")\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5ï¸âƒ£ Vorhersagen treffen\n",
    "print(f\"\\n5ï¸âƒ£ Vorhersagen auf Testdaten:\")\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Genauigkeit: {accuracy:.2%}\")\n",
    "\n",
    "# 6ï¸âƒ£ Feature Importance (wichtig fÃ¼r Interpretierbarkeit)\n",
    "print(f\"\\n6ï¸âƒ£ Feature Importance (Welche Merkmale sind wichtig?):\")\n",
    "importance = model.feature_importances_\n",
    "for name, imp in zip(feature_names, importance):\n",
    "    print(f\"- {name}: {imp:.3f}\")\n",
    "\n",
    "# 7ï¸âƒ£ Verwirrungsmatrix verstehen (wichtig fÃ¼r Evaluation)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"\\n7ï¸âƒ£ Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# 8ï¸âƒ£ Demo: Neue Vorhersage\n",
    "print(f\"\\n8ï¸âƒ£ Demo - Neue Vorhersage:\")\n",
    "new_flower = [[5.1, 3.5, 1.4, 0.2]]  # Beispiel-Werte\n",
    "prediction = model.predict(new_flower)\n",
    "probability = model.predict_proba(new_flower)\n",
    "\n",
    "print(f\"Eingabe: {new_flower[0]}\")\n",
    "print(f\"Vorhersage: {target_names[prediction[0]]}\")\n",
    "print(f\"Wahrscheinlichkeiten:\")\n",
    "for name, prob in zip(target_names, probability[0]):\n",
    "    print(f\"  - {name}: {prob:.2%}\")\n",
    "\n",
    "print(f\"\\nâœ… Das sind die ML-Grundlagen aus dem AMALEA-Kurs!\")\n",
    "print(f\"ğŸš€ Jetzt bauen wir daraus eine interaktive Streamlit-App...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e046e3d6",
   "metadata": {},
   "source": [
    "## ğŸ¯ Demo 2.1: Iris Klassifikation mit ML\n",
    "\n",
    "Wir erweitern unsere Iris-App um ML-FunktionalitÃ¤t:\n",
    "\n",
    "### Was machen wir?\n",
    "1. **Daten laden** (Iris-Dataset)\n",
    "2. **Modell trainieren** (Random Forest)\n",
    "3. **Benutzer-Eingaben** fÃ¼r neue Vorhersagen\n",
    "4. **Performance anzeigen** (Genauigkeit, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f06b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile iris_ml_app.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "st.set_page_config(page_title=\"Iris ML Predictor\", page_icon=\"ğŸŒ¸\")\n",
    "\n",
    "st.title(\"ğŸŒ¸ğŸ¤– Iris ML Vorhersage-App\")\n",
    "st.write(\"Trainiere ein ML-Modell und mache Vorhersagen!\")\n",
    "\n",
    "# === DATEN LADEN UND VORBEREITEN ===\n",
    "@st.cache_data\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"LÃ¤dt Iris-Daten und bereitet sie fÃ¼r ML vor\"\"\"\n",
    "    iris = sns.load_dataset('iris')\n",
    "    \n",
    "    # Features (X) und Target (y) trennen\n",
    "    X = iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "    y = iris['species']\n",
    "    \n",
    "    return X, y, iris\n",
    "\n",
    "X, y, iris_data = load_and_prepare_data()\n",
    "\n",
    "# === SIDEBAR FÃœR MODELL-EINSTELLUNGEN ===\n",
    "st.sidebar.header(\"ğŸ”§ Modell-Einstellungen\")\n",
    "test_size = st.sidebar.slider(\"Test-Datenanteil\", 0.1, 0.5, 0.2)\n",
    "random_state = st.sidebar.number_input(\"Random State\", 0, 100, 42)\n",
    "\n",
    "# === DATEN AUFTEILEN ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=random_state\n",
    ")\n",
    "\n",
    "# === MODELL TRAINIEREN ===\n",
    "@st.cache_data\n",
    "def train_model(test_size, random_state):\n",
    "    \"\"\"Trainiert das ML-Modell mit gegebenen Parametern\"\"\"\n",
    "    X_train_cached, X_test_cached, y_train_cached, y_test_cached = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Random Forest Classifier erstellen und trainieren\n",
    "    model = RandomForestClassifier(random_state=random_state, n_estimators=100)\n",
    "    model.fit(X_train_cached, y_train_cached)\n",
    "    \n",
    "    # Performance berechnen\n",
    "    train_accuracy = accuracy_score(y_train_cached, model.predict(X_train_cached))\n",
    "    test_accuracy = accuracy_score(y_test_cached, model.predict(X_test_cached))\n",
    "    \n",
    "    return model, train_accuracy, test_accuracy\n",
    "\n",
    "model, train_acc, test_acc = train_model(test_size, random_state)\n",
    "\n",
    "# === HAUPTBEREICH IN TABS ===\n",
    "tab1, tab2, tab3 = st.tabs([\"ğŸ¯ Vorhersage\", \"ğŸ“Š Modell-Performance\", \"ğŸ“‹ Daten-Explorer\"])\n",
    "\n",
    "# === TAB 1: VORHERSAGE ===\n",
    "with tab1:\n",
    "    st.header(\"ğŸ¯ Mache eine Vorhersage\")\n",
    "    st.write(\"Gib die Merkmale einer Iris-Blume ein und lass das Modell die Art vorhersagen:\")\n",
    "    \n",
    "    # Eingabe-Widgets fÃ¼r Features\n",
    "    col1, col2 = st.columns(2)\n",
    "    \n",
    "    with col1:\n",
    "        sepal_length = st.number_input(\n",
    "            \"Kelchblatt LÃ¤nge (cm)\", \n",
    "            min_value=4.0, max_value=8.0, value=5.8, step=0.1\n",
    "        )\n",
    "        sepal_width = st.number_input(\n",
    "            \"Kelchblatt Breite (cm)\", \n",
    "            min_value=2.0, max_value=4.5, value=3.0, step=0.1\n",
    "        )\n",
    "    \n",
    "    with col2:\n",
    "        petal_length = st.number_input(\n",
    "            \"BlÃ¼tenblatt LÃ¤nge (cm)\", \n",
    "            min_value=1.0, max_value=7.0, value=4.3, step=0.1\n",
    "        )\n",
    "        petal_width = st.number_input(\n",
    "            \"BlÃ¼tenblatt Breite (cm)\", \n",
    "            min_value=0.1, max_value=2.5, value=1.3, step=0.1\n",
    "        )\n",
    "    \n",
    "    # Vorhersage machen\n",
    "    user_input = np.array([[sepal_length, sepal_width, petal_length, petal_width]])\n",
    "    prediction = model.predict(user_input)[0]\n",
    "    prediction_proba = model.predict_proba(user_input)[0]\n",
    "    \n",
    "    # Ergebnis anzeigen\n",
    "    st.subheader(\"ğŸ”® Vorhersage-Ergebnis:\")\n",
    "    st.success(f\"Die Iris-Art ist wahrscheinlich: **{prediction}**\")\n",
    "    \n",
    "    # Wahrscheinlichkeiten visualisieren\n",
    "    prob_df = pd.DataFrame({\n",
    "        'Art': model.classes_,\n",
    "        'Wahrscheinlichkeit': prediction_proba\n",
    "    })\n",
    "    \n",
    "    fig = px.bar(prob_df, x='Art', y='Wahrscheinlichkeit', \n",
    "                title=\"Vorhersage-Wahrscheinlichkeiten\",\n",
    "                color='Wahrscheinlichkeit',\n",
    "                color_continuous_scale='viridis')\n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "    \n",
    "    # Interpretation\n",
    "    confidence = max(prediction_proba)\n",
    "    if confidence > 0.8:\n",
    "        st.success(f\"ğŸ¯ Sehr sicher! Confidence: {confidence:.1%}\")\n",
    "    elif confidence > 0.6:\n",
    "        st.warning(f\"âš ï¸ MÃ¤ÃŸig sicher. Confidence: {confidence:.1%}\")\n",
    "    else:\n",
    "        st.error(f\"âŒ Unsicher. Confidence: {confidence:.1%}\")\n",
    "\n",
    "# === TAB 2: MODELL-PERFORMANCE ===\n",
    "with tab2:\n",
    "    st.header(\"ğŸ“Š Modell-Performance\")\n",
    "    \n",
    "    # Key Metrics\n",
    "    col1, col2, col3 = st.columns(3)\n",
    "    with col1:\n",
    "        st.metric(\"Training Genauigkeit\", f\"{train_acc:.1%}\")\n",
    "    with col2:\n",
    "        st.metric(\"Test Genauigkeit\", f\"{test_acc:.1%}\")\n",
    "    with col3:\n",
    "        overfitting = train_acc - test_acc\n",
    "        st.metric(\"Overfitting\", f\"{overfitting:.1%}\")\n",
    "    \n",
    "    # Overfitting-Warnung\n",
    "    if overfitting > 0.1:\n",
    "        st.warning(\"âš ï¸ MÃ¶gliches Overfitting! Modell kÃ¶nnte auf neuen Daten schlechter sein.\")\n",
    "    else:\n",
    "        st.success(\"âœ… Gute Generalisierung!\")\n",
    "    \n",
    "    # Feature Importance\n",
    "    st.subheader(\"ğŸ¯ Feature Wichtigkeit\")\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': ['Kelchblatt LÃ¤nge', 'Kelchblatt Breite', 'BlÃ¼tenblatt LÃ¤nge', 'BlÃ¼tenblatt Breite'],\n",
    "        'Wichtigkeit': model.feature_importances_\n",
    "    }).sort_values('Wichtigkeit', ascending=True)\n",
    "    \n",
    "    fig_importance = px.bar(importance_df, x='Wichtigkeit', y='Feature', \n",
    "                           orientation='h', title=\"Welche Features sind am wichtigsten?\",\n",
    "                           color='Wichtigkeit', color_continuous_scale='blues')\n",
    "    st.plotly_chart(fig_importance, use_container_width=True)\n",
    "    \n",
    "    # ErklÃ¤rung\n",
    "    most_important = importance_df.iloc[-1]['Feature']\n",
    "    st.write(f\"ğŸ’¡ **{most_important}** ist das wichtigste Merkmal fÃ¼r die Klassifikation!\")\n",
    "    \n",
    "    # Confusion Matrix (vereinfacht)\n",
    "    y_pred = model.predict(X_test)\n",
    "    st.subheader(\"ğŸ” Detaillierte Performance\")\n",
    "    \n",
    "    # Classification Report als DataFrame\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    st.dataframe(report_df.round(3))\n",
    "\n",
    "# === TAB 3: DATEN-EXPLORER ===\n",
    "with tab3:\n",
    "    st.header(\"ğŸ“‹ Daten-Explorer\")\n",
    "    \n",
    "    # Trainings- vs Test-Daten\n",
    "    st.subheader(\"ğŸ“Š Datenaufteilung\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        st.metric(\"Trainings-Samples\", len(X_train))\n",
    "    with col2:\n",
    "        st.metric(\"Test-Samples\", len(X_test))\n",
    "    \n",
    "    # Originaledaten anzeigen\n",
    "    st.subheader(\"ğŸ” Original-Dataset\")\n",
    "    st.dataframe(iris_data)\n",
    "    \n",
    "    # Korrelations-Heatmap\n",
    "    st.subheader(\"ğŸ”— Feature-Korrelationen\")\n",
    "    correlation_matrix = X.corr()\n",
    "    fig_corr = px.imshow(correlation_matrix, text_auto=True, aspect=\"auto\",\n",
    "                        title=\"Wie hÃ¤ngen die Features zusammen?\",\n",
    "                        color_continuous_scale='RdBu')\n",
    "    st.plotly_chart(fig_corr, use_container_width=True)\n",
    "    \n",
    "    # Scatter Plot Matrix\n",
    "    st.subheader(\"ğŸ“ˆ Feature-Beziehungen\")\n",
    "    fig_scatter = px.scatter_matrix(iris_data, \n",
    "                                   dimensions=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'],\n",
    "                                   color='species')\n",
    "    st.plotly_chart(fig_scatter, use_container_width=True)\n",
    "\n",
    "# === FOOTER ===\n",
    "st.sidebar.markdown(\"---\")\n",
    "st.sidebar.write(\"ğŸ’¡ **ML-Tipp:** Je mehr gute Daten, desto besser das Modell!\")\n",
    "st.sidebar.write(\"ğŸ¯ **NÃ¤chster Schritt:** Probiere verschiedene Modell-Parameter aus!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b8890f",
   "metadata": {},
   "source": [
    "## ğŸ’¡ ML-Konzepte erklÃ¤rt\n",
    "\n",
    "### Train-Test Split\n",
    "**Warum teilen wir Daten auf?**\n",
    "- **Training:** Modell lernt aus diesen Daten\n",
    "- **Test:** PrÃ¼fen wie gut es bei unbekannten Daten ist\n",
    "- **80/20 oder 70/30** sind typische Aufteilungen\n",
    "\n",
    "### Random Forest\n",
    "**Was ist das?**\n",
    "- **Ensemble-Methode:** Kombiniert viele EntscheidungsbÃ¤ume\n",
    "- **Robust:** Funktioniert gut bei vielen Problemen\n",
    "- **Feature Importance:** Zeigt, welche Variablen wichtig sind\n",
    "\n",
    "### Overfitting\n",
    "**Problem:** Modell lernt Trainingsdaten auswendig, kann aber nicht generalisieren\n",
    "**Erkennung:** Training-Performance >> Test-Performance\n",
    "**LÃ¶sung:** Mehr Daten, einfachere Modelle, Regularisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410e7c66",
   "metadata": {},
   "source": [
    "## ğŸ¯ Aufgabe 2.1: Regression-App erstellen\n",
    "\n",
    "Jetzt erstellen wir eine App fÃ¼r ein **Regressions-Problem** (Zahlenwerte vorhersagen):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b070cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile housing_regression_app.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import plotly.express as px\n",
    "\n",
    "st.set_page_config(page_title=\"Housing Price Predictor\", page_icon=\"ğŸ \")\n",
    "\n",
    "st.title(\"ğŸ ğŸ’° Immobilienpreis-Vorhersage\")\n",
    "st.write(\"SchÃ¤tze Hauspreise basierend auf verschiedenen Merkmalen\")\n",
    "\n",
    "# === SIMULIERTE HOUSING-DATEN ERSTELLEN ===\n",
    "@st.cache_data\n",
    "def create_housing_data():\n",
    "    \"\"\"Erstellt realistische Beispiel-Immobiliendaten\"\"\"\n",
    "    np.random.seed(42)\n",
    "    n_samples = 500\n",
    "    \n",
    "    # Features erstellen\n",
    "    rooms = np.random.normal(6, 1.5, n_samples)\n",
    "    rooms = np.clip(rooms, 3, 10)  # Zwischen 3 und 10 Zimmer\n",
    "    \n",
    "    age = np.random.uniform(1, 100, n_samples)\n",
    "    distance_to_city = np.random.uniform(1, 12, n_samples)\n",
    "    crime_rate = np.random.exponential(3, n_samples)\n",
    "    crime_rate = np.clip(crime_rate, 0, 15)  # Max 15\n",
    "    \n",
    "    # Target erstellen (mit realistischen ZusammenhÃ¤ngen)\n",
    "    price = (\n",
    "        rooms * 50000 +                    # Mehr Zimmer = teurer\n",
    "        (100 - age) * 1000 +              # Neuer = teurer\n",
    "        (12 - distance_to_city) * 5000 +  # NÃ¤her zur Stadt = teurer\n",
    "        (-crime_rate * 2000) +            # Mehr KriminalitÃ¤t = billiger\n",
    "        np.random.normal(0, 20000, n_samples) +  # ZufÃ¤lliges Rauschen\n",
    "        200000                            # Basis-Preis\n",
    "    )\n",
    "    \n",
    "    # Negative Preise vermeiden\n",
    "    price = np.maximum(price, 50000)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'rooms': rooms,\n",
    "        'age': age,\n",
    "        'distance_to_city': distance_to_city,\n",
    "        'crime_rate': crime_rate,\n",
    "        'price': price\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# === DATEN LADEN ===\n",
    "housing_data = create_housing_data()\n",
    "\n",
    "# Features und Target trennen\n",
    "feature_columns = ['rooms', 'age', 'distance_to_city', 'crime_rate']\n",
    "X = housing_data[feature_columns]\n",
    "y = housing_data['price']\n",
    "\n",
    "# === SIDEBAR FÃœR MODELL-AUSWAHL ===\n",
    "st.sidebar.header(\"ğŸ”§ Modell-Konfiguration\")\n",
    "model_type = st.sidebar.selectbox(\n",
    "    \"WÃ¤hle Algorithmus:\", \n",
    "    [\"Linear Regression\", \"Random Forest\"]\n",
    ")\n",
    "test_size = st.sidebar.slider(\"Test-Datenanteil\", 0.1, 0.5, 0.2)\n",
    "\n",
    "# === MODELL TRAINIEREN ===\n",
    "@st.cache_data\n",
    "def train_regression_model(model_type, test_size):\n",
    "    \"\"\"Trainiert Regressions-Modell\"\"\"\n",
    "    X_train_cached, X_test_cached, y_train_cached, y_test_cached = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    if model_type == \"Linear Regression\":\n",
    "        model = LinearRegression()\n",
    "    else:\n",
    "        model = RandomForestRegressor(random_state=42, n_estimators=100)\n",
    "    \n",
    "    model.fit(X_train_cached, y_train_cached)\n",
    "    \n",
    "    # Vorhersagen\n",
    "    train_pred = model.predict(X_train_cached)\n",
    "    test_pred = model.predict(X_test_cached)\n",
    "    \n",
    "    # Metriken berechnen\n",
    "    train_r2 = r2_score(y_train_cached, train_pred)\n",
    "    test_r2 = r2_score(y_test_cached, test_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train_cached, train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_cached, test_pred))\n",
    "    \n",
    "    return model, train_r2, test_r2, train_rmse, test_rmse, X_test_cached, y_test_cached\n",
    "\n",
    "model, train_r2, test_r2, train_rmse, test_rmse, X_test, y_test = train_regression_model(model_type, test_size)\n",
    "\n",
    "# === APP-LAYOUT ===\n",
    "tab1, tab2, tab3 = st.tabs([\"ğŸ  Preis-Vorhersage\", \"ğŸ“ˆ Modell-Analyse\", \"ğŸ“Š Daten-Ãœbersicht\"])\n",
    "\n",
    "# === TAB 1: PREIS-VORHERSAGE ===\n",
    "with tab1:\n",
    "    st.header(\"ğŸ  Immobilienpreis schÃ¤tzen\")\n",
    "    \n",
    "    col1, col2 = st.columns(2)\n",
    "    \n",
    "    with col1:\n",
    "        rooms = st.slider(\"Anzahl Zimmer\", 3.0, 10.0, 6.0, 0.1)\n",
    "        age = st.slider(\"Alter des Hauses (Jahre)\", 1, 100, 30)\n",
    "    \n",
    "    with col2:\n",
    "        distance = st.slider(\"Entfernung zur Stadt (km)\", 1.0, 12.0, 5.0, 0.1)\n",
    "        crime_rate = st.slider(\"KriminalitÃ¤tsrate\", 0.0, 15.0, 3.0, 0.1)\n",
    "    \n",
    "    # Vorhersage\n",
    "    user_input = np.array([[rooms, age, distance, crime_rate]])\n",
    "    predicted_price = model.predict(user_input)[0]\n",
    "    \n",
    "    st.subheader(\"ğŸ’° GeschÃ¤tzter Preis:\")\n",
    "    st.success(f\"**${predicted_price:,.0f}**\")\n",
    "    \n",
    "    # Preis-Kategorisierung\n",
    "    if predicted_price < 300000:\n",
    "        st.info(\"ğŸ  GÃ¼nstiges Segment\")\n",
    "    elif predicted_price < 600000:\n",
    "        st.warning(\"ğŸ˜ï¸ Mittleres Segment\")\n",
    "    else:\n",
    "        st.error(\"ğŸ° Luxus-Segment\")\n",
    "    \n",
    "    # Vergleich mit Ã¤hnlichen HÃ¤usern\n",
    "    st.subheader(\"ğŸ“Š Vergleich mit Ã¤hnlichen HÃ¤usern\")\n",
    "    similar_houses = housing_data[\n",
    "        (abs(housing_data['rooms'] - rooms) < 1) &\n",
    "        (abs(housing_data['age'] - age) < 10)\n",
    "    ]\n",
    "    \n",
    "    if len(similar_houses) > 0:\n",
    "        avg_similar_price = similar_houses['price'].mean()\n",
    "        difference = predicted_price - avg_similar_price\n",
    "        \n",
    "        col1, col2 = st.columns(2)\n",
    "        with col1:\n",
    "            st.metric(\"Durchschnitt Ã¤hnlicher HÃ¤user\", f\"${avg_similar_price:,.0f}\")\n",
    "        with col2:\n",
    "            st.metric(\"Unterschied\", f\"${difference:,.0f}\")\n",
    "\n",
    "# === TAB 2: MODELL-ANALYSE ===\n",
    "with tab2:\n",
    "    st.header(\"ğŸ“ˆ Modell-Performance\")\n",
    "    \n",
    "    # Performance Metriken\n",
    "    col1, col2, col3, col4 = st.columns(4)\n",
    "    with col1:\n",
    "        st.metric(\"Train RÂ²\", f\"{train_r2:.3f}\")\n",
    "    with col2:\n",
    "        st.metric(\"Test RÂ²\", f\"{test_r2:.3f}\")\n",
    "    with col3:\n",
    "        st.metric(\"Train RMSE\", f\"${train_rmse:,.0f}\")\n",
    "    with col4:\n",
    "        st.metric(\"Test RMSE\", f\"${test_rmse:,.0f}\")\n",
    "    \n",
    "    # RÂ² ErklÃ¤rung\n",
    "    st.write(\"**RÂ² (R-Squared) Interpretation:**\")\n",
    "    if test_r2 > 0.8:\n",
    "        st.success(\"ğŸ¯ Sehr gutes Modell!\")\n",
    "    elif test_r2 > 0.6:\n",
    "        st.warning(\"âš ï¸ OK-es Modell\")\n",
    "    else:\n",
    "        st.error(\"âŒ Schwaches Modell\")\n",
    "    \n",
    "    st.write(f\"Das Modell erklÃ¤rt {test_r2:.1%} der Preisvarianz.\")\n",
    "    \n",
    "    # Vorhersage vs. RealitÃ¤t Plot\n",
    "    y_pred = model.predict(X_test)\n",
    "    pred_df = pd.DataFrame({\n",
    "        'Echte Preise': y_test, \n",
    "        'Vorhergesagte Preise': y_pred\n",
    "    })\n",
    "    \n",
    "    fig_pred = px.scatter(\n",
    "        pred_df, \n",
    "        x='Echte Preise', \n",
    "        y='Vorhergesagte Preise',\n",
    "        title=\"Vorhersage vs. RealitÃ¤t\"\n",
    "    )\n",
    "    # Perfekte Linie hinzufÃ¼gen\n",
    "    min_price = min(pred_df['Echte Preise'].min(), pred_df['Vorhergesagte Preise'].min())\n",
    "    max_price = max(pred_df['Echte Preise'].max(), pred_df['Vorhergesagte Preise'].max())\n",
    "    fig_pred.add_shape(\n",
    "        type=\"line\", \n",
    "        x0=min_price, y0=min_price,\n",
    "        x1=max_price, y1=max_price,\n",
    "        line=dict(color=\"red\", dash=\"dash\")\n",
    "    )\n",
    "    st.plotly_chart(fig_pred, use_container_width=True)\n",
    "    \n",
    "    # Feature Importance (nur fÃ¼r Random Forest)\n",
    "    if model_type == \"Random Forest\":\n",
    "        st.subheader(\"ğŸ¯ Feature Wichtigkeit\")\n",
    "        feature_names = ['Zimmer', 'Alter', 'Entfernung zur Stadt', 'KriminalitÃ¤tsrate']\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Wichtigkeit': model.feature_importances_\n",
    "        }).sort_values('Wichtigkeit', ascending=True)\n",
    "        \n",
    "        fig_imp = px.bar(\n",
    "            importance_df, \n",
    "            x='Wichtigkeit', \n",
    "            y='Feature',\n",
    "            orientation='h',\n",
    "            title=\"Welche Faktoren beeinflussen den Preis am meisten?\"\n",
    "        )\n",
    "        st.plotly_chart(fig_imp, use_container_width=True)\n",
    "\n",
    "# === TAB 3: DATEN-ÃœBERSICHT ===\n",
    "with tab3:\n",
    "    st.header(\"ğŸ“Š Datensatz-Ãœbersicht\")\n",
    "    \n",
    "    # Basis-Statistiken\n",
    "    st.subheader(\"ğŸ“‹ Grundlegende Statistiken\")\n",
    "    st.dataframe(housing_data.describe(), use_container_width=True)\n",
    "    \n",
    "    # Rohdaten\n",
    "    st.subheader(\"ğŸ” Rohdaten (Erste 20 Zeilen)\")\n",
    "    st.dataframe(housing_data.head(20))\n",
    "    \n",
    "    # Verteilungen der Features\n",
    "    st.subheader(\"ğŸ“ˆ Feature-Verteilungen\")\n",
    "    feature_to_plot = st.selectbox(\n",
    "        \"Feature fÃ¼r Histogramm:\", \n",
    "        ['rooms', 'age', 'distance_to_city', 'crime_rate', 'price']\n",
    "    )\n",
    "    \n",
    "    fig_hist = px.histogram(\n",
    "        housing_data, \n",
    "        x=feature_to_plot, \n",
    "        title=f\"Verteilung von {feature_to_plot}\",\n",
    "        nbins=30\n",
    "    )\n",
    "    st.plotly_chart(fig_hist, use_container_width=True)\n",
    "    \n",
    "    # Korrelations-Matrix\n",
    "    st.subheader(\"ğŸ”— Korrelationen\")\n",
    "    corr_matrix = housing_data.corr()\n",
    "    fig_corr = px.imshow(\n",
    "        corr_matrix,\n",
    "        text_auto=True,\n",
    "        aspect=\"auto\",\n",
    "        title=\"Korrelations-Matrix\"\n",
    "    )\n",
    "    st.plotly_chart(fig_corr, use_container_width=True)\n",
    "\n",
    "# === FOOTER ===\n",
    "st.sidebar.markdown(\"---\")\n",
    "st.sidebar.write(\"ğŸ’¡ **Regression-Tipp:** RÂ² nÃ¤her bei 1 = besseres Modell\")\n",
    "st.sidebar.write(\"ğŸ¯ **Ausprobieren:** VerÃ¤ndere die Eingaben und beobachte die Auswirkungen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb514866",
   "metadata": {},
   "source": [
    "## ğŸ† Lernziel-Check Woche 2\n",
    "\n",
    "Am Ende dieser Woche solltest du:\n",
    "\n",
    "- [ ] **ML-Grundlagen verstehen:** Klassifikation vs. Regression, Training vs. Test\n",
    "- [ ] **ML-Modelle in Streamlit integrieren:** Scikit-learn + Streamlit\n",
    "- [ ] **Benutzer-Eingaben fÃ¼r Vorhersagen nutzen:** Input-Widgets â†’ Predictions\n",
    "- [ ] **Modell-Performance bewerten:** Accuracy, RÂ², Overfitting erkennen\n",
    "- [ ] **Feature Importance interpretieren:** Welche Variablen sind wichtig?\n",
    "- [ ] **Eine funktionsfÃ¤hige ML-App entwickelt haben:** VollstÃ¤ndig und benutzerfreundlich\n",
    "\n",
    "## ğŸ“ Vorbereitung fÃ¼r die Fallstudie\n",
    "\n",
    "**Denke bereits jetzt Ã¼ber dein Fallstudie-Projekt nach:**\n",
    "\n",
    "### Projektideen fÃ¼r deine Fallstudie:\n",
    "1. **Kunden-Segmentierung** (E-Commerce, Bank)\n",
    "2. **Preis-Vorhersage** (Immobilien, Autos, Aktien)\n",
    "3. **Klassifikation** (Spam-Erkennung, Sentiment-Analyse)\n",
    "4. **Gesundheitsdaten** (Krankheits-Vorhersage, Fitness-Tracking)\n",
    "5. **Sport-Analytics** (Spieler-Performance, Ergebnis-Vorhersagen)\n",
    "\n",
    "### Fragen fÃ¼r dein Projekt:\n",
    "1. **Welches Problem mÃ¶chtest du lÃ¶sen?**\n",
    "2. **Welche Daten brauchst du?** (CSV, API, Web-Scraping?)\n",
    "3. **Wer ist deine Zielgruppe?** (Manager, Endkunden, Experten?)\n",
    "4. **Was ist das gewÃ¼nschte Ergebnis?** (Klassifikation, Regression, Clustering?)\n",
    "\n",
    "## ğŸ”® Ausblick auf Woche 3\n",
    "\n",
    "NÃ¤chste Woche werden wir:\n",
    "- **Apps online deployen** (Streamlit Cloud, Heroku)\n",
    "- **Externe Datenquellen** einbinden (APIs, Datenbanken)\n",
    "- **App-Performance optimieren** (Caching, Lazy Loading)\n",
    "- **Professional Design** (Styling, UX/UI Verbesserungen)\n",
    "\n",
    "**ğŸ’¡ Tipp:** Beginne bereits jetzt mit der Sammlung von Daten fÃ¼r deine Fallstudie!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
